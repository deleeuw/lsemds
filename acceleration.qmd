```{r echo = FALSE}
source("loadMe.R")
```

# Acceleration of Convergence {#chacceleration}

## Simple Acceleration {#accelsimple}

A simple and inexpensive way to accelerate smacof iterations was proposed by    
@deleeuw_heiser_C_80. 

On the other hand, if we choose $X=2\Gamma(Y)-Y$ then again $X\not= Y$, but
$\eta^2(X-\Gamma(Y))=\eta^2(Y-\Gamma(Y))$. Thus
\begin{equation}
\sigma(X)\leq 1+\eta^2(X-\Gamma(Y))-\eta^2(\Gamma(Y))=
1+\eta^2(Y-\Gamma(Y))-\eta^2(\Gamma(Y))=\sigma(Y).
(\#eq:upbmajupb)
\end{equation}
Let's define the two update rules $\text{up}_A(X):=\Gamma(X)$ and $\text{up}_B(X)=2\Gamma(X)-X$.

```{r updtstrategy, echo = FALSE}
library(polynom)
f<-polynomial(c(1,1,-.6,-.3,.01))
plot(f,xlim=c(-4,2), lwd = 4, col = "RED")
g <- deriv(f)
h <- min(solve(g))
fh <- predict(f, h)
gg <- deriv(g)
y <- -3
gp <- predict(g, y)
fp <- predict(f, y)
gy <- polynomial (c(fp - gp * y + 4 * y ^ 2, gp - 8 * y, 4))
fy <- as.function(gy)
curve(fy, from=-4, to = 2, lwd = 3, col = "BLUE", add = TRUE)
abline(v=-3, lwd = 2)
ymin <- solve(deriv(gy))
abline(h=fp, lwd = 2)
yext <- 2 * ymin - y
abline(v=yext, lwd = 2)
abline(v=ymin, lwd = 2)
```

This is illustrated in figure .... We want to locate a local minimum of $f$, in red, 
in the interval $(-4,2)$. In this case we happen to know that $f$ is a quartic polynomial, with minimum `r fh` at `r h`.  In the interval we are looking at we have $f''(x)\leq 8$. Suppose our initial guess for the location of the minimum is $x=-3$, the first vertical line from the left, with $f(-3)$ equal to `r fp`. The upper bound on the second derivative allows us to construct a quadratic majorizer $g$, in blue, touching $f$ at $-3$. Update rule $\text{up}_A$ tells us to go to the minimum of $g$, which is at `r ymin`, the second vertical line. Here $g$ is equal to
`r predict(gy, ymin)` and $f$ is `r predict(f, ymin)`.

Rule $\text{up}_B$ "overrelaxes" and goes all the way to `r yext`, the third vertical line from the left, where $g$ is equal to both $g(-3)$ and $f(-3)$, and where $f$ is `r predict(f, yext)`, indeed much closer to the minimum. Examples such as this make $\text{up}_B$ look good.

De Leeuw and Heiser give a rather informal theoretical justification of $\text{up}_B$ as well. Suppose the sequence $X^+=\Gamma(X)$ generated by $\text{up}_A$ has slow linear convergence with ACR $1-\epsilon$, where $\epsilon$ is positive and small. Then choosing the  $\text{up}_B$ will change the ACR of $1-\epsilon$ to  $2(1-\epsilon)-1=1-2\epsilon\approx(1-\epsilon)^2$, and will approximately halve the number of iterations to convergence. This argument is supported by numerical experiments which seem to show that indeed about half the number of iterations are needed. It seems that $\text{up}_B$ will get you something for almost nothing, and thus it has been implemented in various versions of the smacof programs as the default update. Unfortunately this may mean that many users have obtained, and presumably reported, MDS results that are incorrect.

What is ignored in @deleeuw_heiser_C_80 is that majorization only guarantees that the sequence of loss function values converges for both update methods. The general convergence theory discussed earlier in this chapter shows that for both $\text{up}_A$ and $\text{up}_B$ the sequence $\{X^{(k)}\}$ has at least one accumulation point, and that the accumulation points of the sequence $\{X^{(k)}\}$ are fixed points of the update rule, which means for both $\text{up}_A$ and $\text{up}_B$ that at accumulation points $X$ we have $X=\Gamma(X)$.  But it does **not** say that $\{X^{(k)}\}$ converges.

The argument also ignores that at any $X$ the derivative of $\text{up}_A$ has a zero eigenvalue, with eigenvector $X$. For $\text{up}_B$ the eigenvector $X$ has eigenvalue equal to $-1$, which is the largest one in modulus near any local minimum. And so ...

Suppose we have a configuration of the form $\alpha X$ with $X=\Gamma(X)$.
Then $\text{up}_B(\alpha X)=2\Gamma(\alpha X)-\alpha X=(2-\alpha)X$ and
$\text{up}_B((2-\alpha)X)=\alpha X$. Thus starting with $X^{(1)}=\alpha X$ 
$\text{up}_B$ generates a sequence with even members $(2-\alpha)X$ and odd members
$\alpha X$. Thus there are two convergent subsequences with accumulation points
$\alpha X$ and $(2-\alpha)X$. And never the twain shall meet.    

As far as stress is concerned, note that if $X=\Gamma(X)$ then $\sigma(\alpha X)=\sigma((2-\alpha)X)$. Thus the stress values never change, and consequently form a convergent sequence. 

We also see that $\text{up}^{(2)}_B(\alpha X):=\text{up}_B(\text{up}_B(\alpha X))=\alpha X$, which means that $\alpha X$ is a fixed point of $\text{up}_B^{(2)}$ for any fixed point $X$ of $\text{up}_A$ and any $\alpha$.

Another way to express the difference between the two update rule is that $\text{up}_A$
is *self-scaling*, i.e. $\Gamma(\alpha X)=\Gamma(X)$, while $\text{up}_B$
is not. Self-scaling implies $\mathcal{D}\Gamma(X)(X)=0$, while for $\text{up}_B$
$\mathcal{D}(2\Gamma(X)-X)(X)=-X$.
 
Let's now look at a real example. We use the Ekman color similarity data again, this time transformed by $\delta_{ij}=(1-s_{ij})^3$, The analysis is in two dimensions, with no weights.  We run four analyses, by crossing update rules $\text{up}_A$ and $\text{up}_B$ with stopping criteria  $\sigma(X^{(k)})-\sigma(X^{(k+1)})<\epsilon$ and $\max_{i,s}|x^{(k)}_{is}-x^{(k+1)}_{is}|<\epsilon$. Let's call these stopping criteria *stop_s* and *stop_x*. In all cases we allow a maximum of 1000 iterations and we set $\epsilon$ to 1e-10.

```{r ekstrategydata, echo = FALSE}
data(ekman, package = "smacof")
delta <- as.matrix ((1-ekman) ^ 3)
w <- 1 - diag(14)
m <- 91
delta <- delta / sqrt(m /sum (w * delta ^ 2))
v <- smacofVmatR(w)
```
```{r updsinglestep, echo = FALSE}
h10 <- smacofRelaxR(w, delta, 2, strategy = 1, xstop = FALSE)
h11 <- smacofRelaxR(w, delta, 2, strategy = 1, xstop = TRUE)
h20 <- smacofRelaxR(w, delta, 2, strategy = 2, xstop = FALSE)
h21 <- smacofRelaxR(w, delta, 2, strategy = 2, xstop = TRUE)
```

The results are in table ... The first subtable gives the number of iterations, the second the final stress value.
We see that generally stop_x requires more iterations than stop_s, because it is a stricter criterion. If we use
stop_x then $\text{up}_B$ does not converge at all. Both with stop_s and stop_x $\text{up}_B$ gves a higher stress value than $\text{up}_A$. And yes, with stop_s (which is the default stop criterion in the smacof programs so far)
$\text{up}_B$ use fewer iterations than $\text{up}_A$.

```{r echo = FALSE, cache = TRUE, results = "asis"}
ha <- data.frame("stop_f"= c(h10$itel, h20$itel), "stop_x" = c(h11$itel, h21$itel), 
                row.names=c("rule A", "rule B"))
hb <- data.frame("stop_f"= c(h10$s, h20$s), "stop_x" = c(h11$s, h21$s), 
                row.names=c("rule A", "rule B"))
kable(list(ha,hb))
```

```{r echo = FALSE, cache = TRUE}
mga <- formatC(max(abs(smacofGradientR(h11$x, h11$b, v))), width = 15, digits = 10, format = "f")
mgb <- formatC(max(abs(smacofGradientR(h21$x, h21$b, v))), width = 15, digits = 10, format = "f")
```

To verify that something is seriously wrong with running $\text{up}_B$, we compute the maximum absolute value of the gradient at convergence for both rules and stop_s. For $\text{up}_A$ it is 
`r mga` and for $\text{up}_B$ it is `r mgb`. 
Once again, with $\text{up}_B$ both loss function and configuration converge to an incorrect value. 

This can also be illustrated
graphically. We see from table ... that $\text{up}_B$ with stop_x ends after 1000 iteration. We perform an extra iteration, number 1001, and see how the configuration changes. In figure ... iteration 1000 is in black, iteration 1001 in red with slightly bigger characters. Except for a scaling factor the two configurations are the same.
Elementwise dividing the $\text{up}_B$ by the $\text{up}_A$ final configuration gives a shrinkage factor $\alpha$ of `r mean(h21$x/h11$x)`. This shrinkage factor can also be computed from the final stress values. Using $\rho(X)=\eta^2(X)$ and $\sigma(X)=1-\eta^2(X)$ we find $\sigma(\alpha X)-\sigma(X)=(\alpha-1)\eta^2(X)$, and thus
\begin{equation}
\alpha=1\pm\sqrt{\frac{\sigma(\alpha X)-\sigma(X)}{1-\sigma(X))}}.
(\#eq:minshrink)
\end{equation}

There are two values $\alpha$ and $2-\alpha$, equal to `r 1-sqrt((h21$s-h11$s) / (m - h11$s))` and
`r 1+sqrt((h21$s-h11$s) / (m - h11$s))`, because the sequence has two accumulation points.


```{r echo = FALSE}
par(pty="s")
x <- h21$x
d <- as.matrix(dist(x))
s <- smacofLossR (d, w, delta)
b <- smacofBmatR (d, w, delta)
vinv <- ginv (smacofVmatR (w))
z<- 2 * smacofGuttmanR (x, b, vinv) - x
plot(x, type = "n")
text(x, labels(delta)[[1]])
text(z, labels(delta)[[1]], col = "RED", cex = 1.5)
```

Things do not look good for $\text{up}_B$ but simple remedies are available. The first one is renormalization. After
the iterations, with say stop_s, have converged, we scale the configuration such that $\rho(X)=\eta^2(X)$ and recompute stress. This corrects both stress and the confguration to the correct outcome. Another way to normalize is to
do another single $\text{up}_A$ step after convergence of $\text{up}_B$. This has the same effect. We tried $\text{up}_B$
with both renormalization approaches and both stop_s and stop_b.  The number of $\text{up}_B$ iterations
is still the same as in table ... because we just compute something additional at the end. All stress values for the four combinations are now the correct `r h10$s`. It seems that using $\text{up}_B$ with stop_s and renormalization at the end gives us the best of both worlds. It accelerates convergence and it gives the correct loss function values.
                    
```{r echo = FALSE, cache = TRUE}
h20r <- smacofRelaxR(w, delta, 2, strategy = 2, xstop = FALSE, renormalize = 1)
h21r <- smacofRelaxR(w, delta, 2, strategy = 2, xstop = TRUE, renormalize = 1)
h20s <- smacofRelaxR(w, delta, 2, strategy = 2, xstop = FALSE, renormalize = 2)
h21s <- smacofRelaxR(w, delta, 2, strategy = 2, xstop = TRUE, renormalize = 2)
```



```{r echo = FALSE, cache = TRUE}
h202 <- smacofRelaxR(w, delta, 2, strategy = 3, xstop = TRUE, renormalize = 1)
h212 <- smacofRelaxR(w, delta, 2, strategy = 4, xstop = TRUE, renormalize = 1)
```

Of course $\text{up}_B$ with stop_x still does not converge, and probably the best way to deal with that unfortunate fact is to avoid the combination alltogether. 

We can still use stop_x and get acceleration by define a single interation as $\text{up}_{AB}(x):=\text{up}_A(\text{up}_B(X))$. 
For comparison purposes we also run $\text{up}_{AA}(x):=\text{up}_A(\text{up}_A(X))$. Both converge to the correct values, $\text{up}_{AA}$ in `r h202$itel` and $\text{up}_{AB}(x)$ in `r h212$itel` iterations. 

Again $\text{up}_{AB}$ is an attractive strategy. It works with both
stop_s and stop_x and it accelerates. Less so than $\text{up}_B$, however. If the ACR of $\text{up}_A$ is $1-\epsilon$, then, by the same reasoning as before, the ACR of $\text{up}_{AB}$ is $(1-\epsilon)^\frac32$.

# Relaxed

## Modification

@deleeuw_heiser_C_80 first suggested the "relaxed" update
\begin{equation}
\Psi(X):=2\Phi(X)-X.
(\#eq:relax)
\end{equation}
The reason for recommending \@ref(eq:relax) is two-fold. First, the smacof inequality \@ref(eq:smacofinequality) says
\begin{equation}
\sigma(X)\leq 1+\eta^2(X-\Phi(Y))-\eta^2(\Phi(Y)).
(\#eq:smaineq)
\end{equation}
If $X=\alpha\Phi(Y)+(1-\alpha)Y$ then this becomes
\begin{equation}
\sigma(\alpha\Phi(Y)+(1-\alpha)Y)\leq 1+(1-\alpha)^2\eta^2(Y-\Phi(Y))-\eta^2(\Phi(Y))
\end{equation}
If $(1-\alpha)^2\leq 1$ then 
\begin{equation}
1+(1-\alpha)^2\eta^2(Y-\Phi(Y))-\eta^2(\Phi(Y))\leq 1+\eta^2(Y-\Phi(Y))-\eta^2(\Phi(Y))=\sigma(Y)
\end{equation}
Thus updating with $X^{(k+1)}=\alpha\Phi(X^{(k)})+(1-\alpha)X^{(k)}$ is a stricly
monotone algorithm as long as $0\leq\alpha\leq 2$.

But if $\alpha=2$ an $Y=\lambda X$ !!

The second reason for choosing the relaxed update \@ref(eq:relax) given by @deleeuw_heiser_C_80
is that its asymptotic convergence rate is 
\begin{equation}
\max_s|2\lambda_s-1|=\max(2\lambda_{\text{max}}-1,1-2\lambda_{\text{min}}).
\end{equation}
@deleeuw_heiser_C_80 then somewhat carelessly assume that this is equal to
$2\lambda_{\text{max}}-1$ and argue that if $\lambda_{\text{max}}=1-\epsilon$
with $\epsilon$ small, as it usually is in MDS,  then 
\begin{equation}
2\lambda_{\text{max}}-1=1-2\epsilon\approx(1-\epsilon)^2=\lambda_{\text{max}}^2,
\end{equation}
so that the relaxed update requires approximately half the number of iterations of the
basic update. Despite the somewhat sloppy reasoning, the approximate halving of the number of iterations is often observed in practice. 

## Function Values

It turns out (@groenen_glunt_hayden_96, @deleeuw_R_06b), however, that applying the relaxed update
has some unintended consequences, which basically imply that it should never
be used without additional precautions. Let's take a look at the
Ekman results.
```{r compute5, echo = FALSE, cache = FALSE}
h5 <- smacofAccelerate(ekman, opt = 5, halt = 0, epsf = 1e-15, verbose = 1)
```
In iteration `r h5$itel`, the final iteration, stress is `r prettyNum(h5$s, digits = 15, format = "f")`. The "change" $\eta(X^{(k)}-X^{(k+1)})$ is `r prettyNum(h5$chng, digits = 15, format = "f")` and the estimate
of the asymptotic convergence ratio, the "change" divided by the "change" of the 
previous iteration, is `r prettyNum(h5$labd, digits = 15, format = "f")`.

The loss function values converge and the number of iterations is reduced from 57 to 23.
But we see that $\eta(X^{(k+1)}-X^{(k)})$ does not converge to zero, and that $\sigma_k$ converges to a value which does not correspond to a local minimum of $\sigma$.



If we check the conditions of theorem 3.1 in @meyer_76 we see that, although the 
algorithmic map is closed and the iterates are in a compact set, $\Psi$
is not strictly monotone at some non-fixed points. The problem was first discussed
in @groenen_glunt_hayden_96. Suppose $X$ is a fixed point and  $\tau\not= 1$. Then
$\tau\overline{X}$ is not a fixed point of $\Psi$, because 
$\Psi(\tau\overline{X})=(2-\tau)\overline{X}$. And
\begin{equation}
\sigma(\tau\overline{X})=1-\tau\rho(\overline{X})+\frac12\tau^2\eta^2(\overline{X})=
1-\frac12\tau(2-\tau)\rho(\overline{X})=\sigma((2-\tau)\overline{X})
(\#eq:sigmatau)
\end{equation}
Thus the algorithm has convergent subsequences which may not converge to a fixed
point of $\Psi$ (and thus of $\Phi$). And indeed, the computational results show that the method produces a sequence $X^{(k)}$ with two subseqences. If $\overline{X}$ is a fixed point of $\Phi$ then there is a $\tau>0$ such that
the subsequence with $k$ even converges to $\tau\overline{X}$
while the subsequence with $k$ odd converges to $(2-\tau)\overline{X}$.

This suggests a simple fix. After convergence of the funcion values we make
a final update using $\Phi$ instead of $\Psi$. Computationally this is simple to do. If the final iteration updates $X^{(k)}$ to $X^{(k+1)}=\Psi(X^{(k)})$ then
set the final solution to the average $\frac12(X^{(k)}+X^{(k+1)})$. Making this 
adjustment at the end of the Ekman sequence gives us a final stress equal to `r prettyNum(h4$s, digits = 15, format = "f")`.

## Asymptotic Rate of Convergence

The eigenvalues of the Jacobian are 
```{r eval5f, echo = FALSE, eval = FALSE}
jacob4f <- smacofRelaxJacobianFormula(h4$x, h4$delta, h4$wgth, opt = 4)
smacofMatrixPrint(Re(eigen(jacob4f)$values))
```

```{r eval5n, echo = FALSE, eval = FALSE}
jacob4n<- smacofRelaxJacobianNumerical(h4$x, h4$delta, h4$wgth, opt = 4)
smacofMatrixPrint(Re(eigen(jacob4n)$values))
```

# Doubling

## Modification

The analysis in the previous section suggest the update function $\Psi^2$, i.e.
$$
\Xi(X)=\Psi(\Psi(X)).
$$

## Function Values

The algorithm generates the same sequence of function values 
and configurations as the relaxed algoroithm $\Psi$. The only difference is that we
test for convergence, and compute CHNG and EARC, every other
iteration.

With $\Psi^2$ the algorithm is everywhere strictly monotonic and does
converge to a fixed point. But not all problems associated with $\Psi$ 
have disappeared.
If $X$ is a stationary point of $\sigma$, and thus a fixed 
point of $\Phi$, $\Psi$, and $\Psi^2$, then $\tau X$ is a 
fixed point of $\Psi^2$ for all $\tau>0$. Thus we cannot
exclude the possibility that the sequence converges to
a fixed point proportional to $X$, but not equal to $X$.

Here are the results for the Ekman data if we use $\Psi^2$.
```{r compute6, echo = FALSE, cache = TRUE}
h6 <- smacofAccelerate(ekman, opt = 6, halt = 0, epsf = 1e-15, verbose = 0)
```
In iteration `r h6$itel`, the final iteration, stress is `r prettyNum(h6$s, digits = 15, format = "f")`. The CHNG is `r prettyNum(h6$chng, digits = 15, format = "f")` and the EARC is `r prettyNum(h6$labd, digits = 15, format = "f")`. 

Again we need some adjustment. A final update using $\Phi$ will do the trick.
After this adjustment stress is `r prettyNum(h6$s, digits = 15, format = "f")`


## Asymptotic Rate of Convergence

$$
\mathcal{D}\Psi^2_X(H)=\mathcal{D}\Psi_{\Psi(X)}(\mathcal{D}\Psi_X(H))
$$

The asymptotic convergence rate is 
$$
\max_s (2\lambda_s-1)^2=\max\{ (2\lambda_\text{max}-1)^2, (2\lambda_\text{min}-1)^2\}
$$.


```{r eval6, echo = FALSE, eval = FALSE}
jacob5 <- numHess(h6$x, h5$delta, h5$wgth, opt = 4)
smacofMatrixPrint(Re(eigen(jacob5)$values))
```

# Dilation

## Modification

@deleeuw_R_06b discusses some other ways to fix the relaxed update problem. 
The first one, borrowed from @groenen_glunt_hayden_96, defines
Defines
$$
\Pi(X):=\frac{\rho(X)}{\eta^2(X)}X
$$
and 
$$
\Xi(X):=\Pi(\Psi(X))
$$
## Function Values

Here are the results for the Ekman data if we use dilation.
```{r compute7, echo = FALSE, cache = TRUE}
h7 <- smacofAccelerate(ekman, opt = 7, halt = 0, epsf = 1e-15, verbose = 0)
```
In iteration `r h7$itel`, the final iteration, stress is `r prettyNum(h7$s, digits = 15, format = "f")`. The CHNG is `r prettyNum(h7$chng, digits = 15, format = "f")` and the EARC is `r prettyNum(h7$labd, digits = 15, format = "f")`. 


## Asymptotic Rate of Convergence

First, differentiate $\Pi$ of ... Using the product and quotient rules for differentiation we find
$$
\mathcal{D}\Pi_X(H)=\frac{\rho(X)}{\eta^2(X)}H+\frac{\eta^2(X)\mathcal{D}\rho_X(H)-\rho(X)\mathcal{D}\eta^2_X(H)}{\eta^4(X)}X
$$
Using
$$
\mathcal{D}\rho_X(H)=\text{tr}\ H'B(X)X
$$
$$
\mathcal{D}\eta^2_X(H)=2\text{tr}\ H'VX
$$
this becomes
$$
\mathcal{D}\Pi_X(H)=\frac{\rho(X)}{\eta^2(X)}H+\text{tr}\ H'\left\{\frac{\eta^2(X) B(X)X-2\rho(X)VX}{\eta^4(X)}\right\}X
$$


The chain rule says
$$
\mathcal{D}\Xi_X(H)=\mathcal{D}\Psi_X(H)-\frac{\text{tr}\ X'V\mathcal{D}\Psi_X(H)}{\text{tr}\ X'VX}X
$$
Since $\mathcal{D}\Psi_X(X)=-X$ we have
$$
\mathcal{D}\Xi_X(X)=-X+\frac{\text{tr}\ X'VX}{\text{tr}\ X'VX}X=0
$$
Thus the offending eigenvector $X$ of $\mathcal{D}\Psi$ is eliminated.

More generally, if $\mathcal{D}\Psi_X(H)=\lambda H$ with $H\not= X$ then
$\mathcal{D}\Xi_X(H-X)=\lambda (H-X),$
and thus $\mathcal{D}\Xi$ has the same eigenvalues as $\mathcal{D}\Psi$.


# Stabilizing

## Modification

Another strategy 

\begin{equation}
\Xi(X):=\Phi(\Psi(X))
\end{equation}

$$
\mathcal{D}\Xi_X(H)=\mathcal{D}\Phi_{\Psi(X)}(\mathcal{D}\Psi_X(H))=2\mathcal{D}\Phi_{\Psi(X)}(\mathcal{D}\Phi_X(H))-\mathcal{D}\Phi_{\Psi(X)}(H)
$$

\begin{equation}
\max_s|\lambda_s(2\lambda_s-1)|
\end{equation}

## Function Values

Here are the results for the Ekman data if we use stabilization.
```{r compute8, echo = FALSE, cache = TRUE}
h8 <- smacofAccelerate(ekman, opt = 8, halt = 0, epsf = 1e-15, verbose = 0)
```
In iteration `r h8$itel`, the final iteration, stress is `r prettyNum(h8$s, digits = 15, format = "f")`. The CHNG is `r prettyNum(h8$chng, digits = 15, format = "f")` and the EARC is `r prettyNum(h8$labd, digits = 15, format = "f")`. 

## Asymptotic Rate of Convergence









## One-Parameter Methods

In psychometrics, and perhaps in multivariate analysis, @ramsay_75 was the first to apply a general acceleration methods to sequences in $\mathbb{R}^n$ of the form $x^{(k+1)}=f(x^{(k)})$. 

@deleeuw_R_06b

## SQUAREM

## Vector Extrapolation Methods

@deleeuw_R_08h

@deleeuw_R_08i

@sidi_17


