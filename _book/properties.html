<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Properties of Stress – Least Squares Euclidean Multidimensional Scaling</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./spaces.html" rel="next">
<link href="./intro.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
</head><body class="nav-sidebar floating">.greybox {
  padding: 1em;
  background: white;
  color: black;
  border: 2px solid orange;
  border-radius: 10px;
}
.center {
  text-align: center;
}

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>





<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./properties.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Properties of Stress</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Least Squares Euclidean Multidimensional Scaling</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Note</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notation and Reserved Symbols</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./properties.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Properties of Stress</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./spaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Stress Spaces</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Multidimensional Scaling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./minimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Minimization of Basic Stress</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Acceleration of Convergence</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nonmetric.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Nonmetric MDS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./interval.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Interval MDS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./polynomial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Polynomial MDS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ordinal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Ordinal MDS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./splinical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Splinical MDS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unidimensional.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Unidimensional Scaling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./full.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Full-dimensional Scaling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unfolding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Unfolding</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./constrained.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Constrained Multidimensional Scaling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./individual.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Individual Differences</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymmetry.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Asymmetry in MDS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nominal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Nominal MDS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nonordinal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Nonmonotonic MDS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nonpairs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Compound Objects</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sstress.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">sstress</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rstress.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">fstress and rstress</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./altls.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Alternative Least Squares Loss</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./inverse.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Inverse Multidimensional Scaling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Stability of MDS Solutions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./global.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">In Search of Global Minima</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Software</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./backmatter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">(APPENDIX) Appendices</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#propnotation" id="toc-propnotation" class="nav-link active" data-scroll-target="#propnotation"><span class="header-section-number">2.1</span> Notation</a>
  <ul class="collapse">
  <li><a href="#propexpand" id="toc-propexpand" class="nav-link" data-scroll-target="#propexpand"><span class="header-section-number">2.1.1</span> Expanding</a></li>
  <li><a href="#propmatrix" id="toc-propmatrix" class="nav-link" data-scroll-target="#propmatrix"><span class="header-section-number">2.1.2</span> Matrix Expressions</a></li>
  <li><a href="#propcoefspace" id="toc-propcoefspace" class="nav-link" data-scroll-target="#propcoefspace"><span class="header-section-number">2.1.3</span> Coefficient Space</a></li>
  <li><a href="#our-friends-cs-and-amgm" id="toc-our-friends-cs-and-amgm" class="nav-link" data-scroll-target="#our-friends-cs-and-amgm"><span class="header-section-number">2.1.4</span> Our Friends CS and AM/GM</a></li>
  </ul></li>
  <li><a href="#propglobal" id="toc-propglobal" class="nav-link" data-scroll-target="#propglobal"><span class="header-section-number">2.2</span> Global Properties</a>
  <ul class="collapse">
  <li><a href="#propbounded" id="toc-propbounded" class="nav-link" data-scroll-target="#propbounded"><span class="header-section-number">2.2.1</span> Boundedness</a></li>
  <li><a href="#propinvariance" id="toc-propinvariance" class="nav-link" data-scroll-target="#propinvariance"><span class="header-section-number">2.2.2</span> Invariance</a></li>
  <li><a href="#propcontinuity" id="toc-propcontinuity" class="nav-link" data-scroll-target="#propcontinuity"><span class="header-section-number">2.2.3</span> Continuity</a></li>
  <li><a href="#propcoercive" id="toc-propcoercive" class="nav-link" data-scroll-target="#propcoercive"><span class="header-section-number">2.2.4</span> Coercivity</a></li>
  </ul></li>
  <li><a href="#propdiff" id="toc-propdiff" class="nav-link" data-scroll-target="#propdiff"><span class="header-section-number">2.3</span> Differentiability</a>
  <ul class="collapse">
  <li><a href="#partial-derivatives" id="toc-partial-derivatives" class="nav-link" data-scroll-target="#partial-derivatives"><span class="header-section-number">2.3.1</span> Partial Derivatives</a></li>
  <li><a href="#propspecexp" id="toc-propspecexp" class="nav-link" data-scroll-target="#propspecexp"><span class="header-section-number">2.3.2</span> Special Expansions</a></li>
  </ul></li>
  <li><a href="#propconvex" id="toc-propconvex" class="nav-link" data-scroll-target="#propconvex"><span class="header-section-number">2.4</span> Convexity</a>
  <ul class="collapse">
  <li><a href="#distances-1" id="toc-distances-1" class="nav-link" data-scroll-target="#distances-1"><span class="header-section-number">2.4.1</span> Distances</a></li>
  <li><a href="#subdifdef" id="toc-subdifdef" class="nav-link" data-scroll-target="#subdifdef"><span class="header-section-number">2.4.2</span> Subdifferentials</a></li>
  <li><a href="#propdc" id="toc-propdc" class="nav-link" data-scroll-target="#propdc"><span class="header-section-number">2.4.3</span> DC Functions</a></li>
  <li><a href="#propnegdis" id="toc-propnegdis" class="nav-link" data-scroll-target="#propnegdis"><span class="header-section-number">2.4.4</span> Negative Dissimilarities</a></li>
  </ul></li>
  <li><a href="#propstationary" id="toc-propstationary" class="nav-link" data-scroll-target="#propstationary"><span class="header-section-number">2.5</span> Stationary Points</a>
  <ul class="collapse">
  <li><a href="#local-maxima" id="toc-local-maxima" class="nav-link" data-scroll-target="#local-maxima"><span class="header-section-number">2.5.1</span> Local Maxima</a></li>
  <li><a href="#proplocmin" id="toc-proplocmin" class="nav-link" data-scroll-target="#proplocmin"><span class="header-section-number">2.5.2</span> Local Minima</a></li>
  <li><a href="#propsaddle" id="toc-propsaddle" class="nav-link" data-scroll-target="#propsaddle"><span class="header-section-number">2.5.3</span> Saddle Points</a></li>
  <li><a href="#an-example" id="toc-an-example" class="nav-link" data-scroll-target="#an-example"><span class="header-section-number">2.5.4</span> An Example</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="propchapter" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Properties of Stress</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="propnotation" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="propnotation"><span class="header-section-number">2.1</span> Notation</h2>
<p>The notation used in the <span class="math inline">\(\textrm{smacof}\)</span> approach to MDS first appeared in <span class="citation" data-cites="deleeuw_C_77">De Leeuw (<a href="references.html#ref-deleeuw_C_77" role="doc-biblioref">1977</a>)</span>, and was subsequently used in several of the later key smacof references, such as <span class="citation" data-cites="deleeuw_heiser_C_82">De Leeuw and Heiser (<a href="references.html#ref-deleeuw_heiser_C_82" role="doc-biblioref">1982</a>)</span>, <span class="citation" data-cites="deleeuw_A_88b">De Leeuw (<a href="references.html#ref-deleeuw_A_88b" role="doc-biblioref">1988</a>)</span>, chapter 8 of <span class="citation" data-cites="borg_groenen_05">Borg and Groenen (<a href="references.html#ref-borg_groenen_05" role="doc-biblioref">2005</a>)</span>, and <span class="citation" data-cites="deleeuw_mair_A_09c">De Leeuw and Mair (<a href="references.html#ref-deleeuw_mair_A_09c" role="doc-biblioref">2009</a>)</span>. We follow it in this book.</p>
<section id="propexpand" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="propexpand"><span class="header-section-number">2.1.1</span> Expanding</h3>
<p>We expand stress by writing out the squares of the residuals and then summing. Define</p>
<p><span class="math display">\[\begin{align}
\eta_\delta^2&amp;:=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\delta_{ij}^2,(\#eq:comps1)\\
\rho(X)&amp;:=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\delta_{ij}d_{ij}(X),(\#eq:comps2)\\
\eta^2(X)&amp;:=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}d_{ij}^2(X).(\#eq:comps3)
\end{align}\]</span></p>
<p>More precisely, using conditional summation,</p>
<p><span class="math display">\[\begin{align}
\rho(X)&amp;:=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}\left\{w_{ij}\delta_{ij}d_{ij}(X)\mid w_{ij}\delta_{ij}&gt;0\right\},(\#eq:compszero1)\\
\eta^2(X)&amp;:=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}\left\{w_{ij}d_{ij}^2(X)\mid w_{ij}&gt;0\right\}(\#eq:compszero2).
\end{align}\]</span></p>
<p>Remember that we have normalized by <span class="math inline">\(\eta_\delta^2=1\)</span>. With our newly defined functions <span class="math inline">\(\rho\)</span> and <span class="math inline">\(\eta^2\)</span> we can write stress as</p>
<p><span class="math display">\[\begin{equation}
\sigma(X)=\frac12(1+\eta^2(X))-\rho(X).
(\#eq:expand)
\end{equation}\]</span></p>
<p>The CS inequality implies that for all <span class="math inline">\(X\)</span></p>
<p><span class="math display">\[\begin{equation}
\rho(X)=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\delta_{ij}d_{ij}(X)\leq\eta_\delta\eta(X)=\eta(X),
(\#eq:propcsrhoeta)
\end{equation}\]</span></p>
<p>and thus, from @ref(eq:expand),</p>
<p><span class="math display">\[\begin{align}
&amp;\frac12(1-\eta(X))^2\leq\sigma(X)\leq\frac12(1+\eta^2(X)),(\#eq:propcssigeta1)\\
&amp;\frac12(1-\rho(X))^2\leq\sigma(X)\leq\frac12(1-2\rho(X)).(\#eq:propcssigeta2)
\end{align}\]</span></p>
</section>
<section id="propmatrix" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="propmatrix"><span class="header-section-number">2.1.2</span> Matrix Expressions</h3>
<p>Using matrix notation allows us to arrive at compact expressions, which suggest various mathematical and computational shortcuts. In order to use matrix notation for distances we mainly rely on the difference matrices <span class="math inline">\(A_{ij}\)</span>, which we now define.</p>
<ul>
<li><p>A <em>unit vector</em> <span class="math inline">\(e_i\)</span> is a vector with element <span class="math inline">\(i\)</span> equal to <span class="math inline">\(+1\)</span> and all other elements equal to 0. A <em>unit matrix</em> <span class="math inline">\(E_{ij}\)</span> is a matrix of the form <span class="math inline">\(e_i^{\ }e_j'\)</span>,</p></li>
<li><p>A <em>diff matrix</em> <span class="math inline">\(A_{ij}\)</span> is a matrix of the form <span class="math inline">\((e_i-e_j)(e_i-e_j)'\)</span>.</p></li>
</ul>
<p>The element in row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span> of a matrix <span class="math inline">\(X\)</span> is normally referred to as <span class="math inline">\(x_{ij}\)</span>. But in some cases, to prevent confusion, we use the notation <span class="math inline">\(\{X\}_{ij}\)</span>. Thus, for example, <span class="math inline">\(\{e_i\}_j=\delta^{ij}\)</span>, where <span class="math inline">\(\delta^{ij}\)</span> is <em>Kronecker’s delta</em> (zero when <span class="math inline">\(i=j\)</span> and one otherwise).</p>
<p>The diff matrices <span class="math inline">\(A_{ij}\)</span> with <span class="math inline">\(i\not= j\)</span> have only four non-zero elements</p>
<p><span class="math display">\[\begin{align}
\begin{split}
\{A_{ij}\}_{ii}&amp;=\{A_{ij}\}_{jj}=+1,\\
\{A_{ij}\}_{ij}&amp;=\{A_{ij}\}_{ji}=-1,
\end{split}
(\#eq:apaele)
\end{align}\]</span></p>
<p>and all other elements of <span class="math inline">\(A_{ij}\)</span> are zero. Thus <span class="math inline">\(A_{ij}=A_{ji}\)</span> and <span class="math inline">\(A_{ii}=0\)</span>. Diff matrices are symmetric, and positive semidefinite. They are also <em>doubly-centered</em>, which means that their rows and columns add up to zero. If <span class="math inline">\(i\not j\)</span> they are of rank one and have one eigenvalue equal to two, which means <span class="math inline">\(A_{ij}^s=2^{s-1}A_{ij}\)</span>. Also</p>
<p><span class="math display">\[\begin{equation}
\mathop{\sum\sum}_{1\leq i&lt;j\leq n} A_{ij}=nI-ee'=nJ,
(\#eq:asum)
\end{equation}\]</span></p>
<p>with <span class="math inline">\(J\)</span> the centering matrix.</p>
<p>We begin our matrix expressions with <span class="math inline">\(d_{ij}^2(X)=\text{tr}\ X'A_{ij}X\)</span>. Define</p>
<p><span class="math display">\[\begin{equation}
V:=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}A_{ij},
(\#eq:vdef)
\end{equation}\]</span></p>
<p>so that</p>
<p><span class="math display">\[\begin{equation}
\eta^2(X)=\text{tr}\ X'VX.
(\#eq:etav)
\end{equation}\]</span></p>
<p>The matrix <span class="math inline">\(V\)</span> has off-diagonal elements equal to <span class="math inline">\(-w_{ij}\)</span> and diagonal elements <span class="math inline">\(v_{ii}=\sum_{j\not= i} w_{ij}\)</span> It is symmetric, positive semi-definite, and doubly-centered. Thus it is singular, because <span class="math inline">\(Ve=0\)</span>.</p>
<p>To analyze the singularity of <span class="math inline">\(V\)</span> in more detail we observe that <span class="math inline">\(z'Vz=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}(z_i-z_j)^2\)</span>. This is zero if and only if all <span class="math inline">\(w_{ij}(z_i-z_j)^2\)</span> are zero. If we permute the elements of <span class="math inline">\(z\)</span> such that <span class="math inline">\(z_1\leq\cdots\leq z_n\)</span> then the matrix with elements <span class="math inline">\((z_i-z_j)^2\)</span> can be partitioned such that the diagonal blocks, corresponding with tie-blocks in <span class="math inline">\(z\)</span>, are zero and the off-diagonal blocks are strictly positive. Thus <span class="math inline">\(z'Vz=0\)</span> if and only if the corresponding off-diagonal blocks of <span class="math inline">\(W\)</span> are zero. In other words, we can find a <span class="math inline">\(z\)</span> such that <span class="math inline">\(z'Vz=0\)</span> if and only if <span class="math inline">\(W\)</span> is the direct sum of a number of smaller matrices. If this is not the case we call <span class="math inline">\(W\)</span> <em>irreducible</em>, and <span class="math inline">\(z'Vz&gt;0\)</span> for all <span class="math inline">\(z\not= e\)</span>, so that the rank of <span class="math inline">\(V\)</span> is <span class="math inline">\(n-1\)</span>.</p>
<p>If <span class="math inline">\(W\)</span> is reducible the MDS problem separates into a number of smaller independent MDS problems. We will assume in the sequel, without any real loss of generality, that this does not occur, and that consequently <span class="math inline">\(W\)</span> is irreducible.</p>
<p>Because of the singularity of the matrices involved we sometimes have to work with generalized inverses. We limit ourselves to the Moore-Penrose (MP) inverse, which can be defined in terms of the singular value decomposition. If the singular value decomposition is <span class="math inline">\(X=K\Lambda L'\)</span> with <span class="math inline">\(K'K=L'L=I_r\)</span> and <span class="math inline">\(\Lambda\)</span> a positive definite diagonal matrix of order <span class="math inline">\(r=\text{rank}(X)\)</span>, then the MP inverse of <span class="math inline">\(X\)</span> is <span class="math inline">\(X^+=L\Lambda^{-1}K'\)</span>.</p>
<p>Because of irreducibility the MP inverse of <span class="math inline">\(V\)</span> is <span class="math display">\[\begin{equation}
V^+=(V+\frac{ee'}{n})^{-1}-\frac{ee'}{n}.
(\#eq:mpv)
\end{equation}\]</span> If all weights are equal, say to <span class="math inline">\(w\)</span>, then <span class="math inline">\(V=nwJ\)</span> and <span class="math inline">\(V^+=\frac{1}{nw}J\)</span>, with <span class="math inline">\(J\)</span> the centering matrix <span class="math inline">\(I-\frac{1}{n}ee'\)</span>.</p>
<p>Finding an expression for <span class="math inline">\(\rho(X)\)</span> from @ref(eq:comps2) in matrix form is a bit more complicated. Define <span class="math display">\[\begin{equation}
r_{ij}(X):=\begin{cases}0&amp;\text{ if }d_{ij}(X)=0,\\
\frac{\delta_{ij}}{d_{ij}(X)}&amp;\text{ if }d_{ij}(X)&gt;0,
\end{cases}
(\#eq:rdef)
\end{equation}\]</span> and <span class="math display">\[\begin{equation}
B(X):=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}r_{ij}(X)A_{ij}.
(\#eq:bdef)
\end{equation}\]</span> Then we have <span class="math display">\[\begin{equation}
\rho(X)=\text{tr}\ X'B(X)X.
(\#eq:rhob)
\end{equation}\]</span> Just like <span class="math inline">\(V\)</span>, the matrix-valued function <span class="math inline">\(B\)</span> is symmetric, positive-semidefinite, and doubly-centered. If all dissimilarities and distances are positive then irreducibility of <span class="math inline">\(W\)</span> implies that the rank of <span class="math inline">\(B(X)\)</span> is equal to <span class="math inline">\(n-1\)</span>. Note that if <span class="math inline">\(\delta_{ij}=d_{ij}(X)&gt;0\)</span> for all <span class="math inline">\(i,j\)</span> (perfect fit), then the <span class="math inline">\(r_{ij}\)</span> from @ref(eq:rdef) are all equal to one, and <span class="math inline">\(B(X)=V\)</span>.</p>
<p>In @ref(eq:rdef) we have set <span class="math inline">\(r_{ij}(X)=0\)</span> if <span class="math inline">\(d_{ij}(X)=0\)</span>. This is arbitrary. Since <span class="math inline">\(b_{ij}(X)=r_{ij}(X)\)</span> if <span class="math inline">\(d_{ij}(X)=0\)</span> we get a different matrix <span class="math inline">\(B(X)\)</span> if we choose to set, say, <span class="math inline">\(r_{ij}(X)=1\)</span> or <span class="math inline">\(r_{ij}(X)=\delta_{ij}\)</span> whenever <span class="math inline">\(d_{ij}(X)=0\)</span>. But <span class="math display">\[\begin{equation}
B(X)X=\mathop{\sum\sum}_{1 \leq i&lt;j\leq n}w_{ij}r_{ij}(X)(e_i-e_j)(x_i-x_j)'
(\#eq:propbinvar)
\end{equation}\]</span> remains the same, no matter how we choose <span class="math inline">\(r_{ij}(X)\)</span> for the <span class="math inline">\(i&lt;j\)</span> with <span class="math inline">\(d_{ij}(X)=0\)</span>. And, consequently, <span class="math inline">\(\rho(X)=\text{tr}\ X'B(X)X\)</span> remains the same as well.</p>
<p>We now see, from equation @ref(eq:expand), that <span class="math display">\[\begin{equation}
\sigma(X)=1-\ \text{tr}\ X'B(X)X+\frac12\text{tr}\ X'VX.
(\#eq:propmatexp)
\end{equation}\]</span></p>
</section>
<section id="propcoefspace" class="level3" data-number="2.1.3">
<h3 data-number="2.1.3" class="anchored" data-anchor-id="propcoefspace"><span class="header-section-number">2.1.3</span> Coefficient Space</h3>
<p>Observe that we distinguish configuration space, which is the linear space <span class="math inline">\(\mathbb{R}^{n\times p}\)</span> of <span class="math inline">\(n\times p\)</span> matrices, from the linear space <span class="math inline">\(\mathbb{R}^{np}\)</span> of <span class="math inline">\(np\)</span> element vectors. The two spaces are isomorphic, and connected by the <em>vec operator</em> and its inverse.</p>
<p>Some quick definitions. If <span class="math inline">\(Y\in\mathbb{R}^{n\times p}\)</span> is a configuration, then <span class="math inline">\(\text{vec}(Y)\)</span> is an <span class="math inline">\(np\)</span>-element vector obtained by stacking the columns of <span class="math inline">\(Y\)</span> on top of each other. Thus element <span class="math inline">\((i,s)\)</span> of <span class="math inline">\(Y\)</span> becomes element <span class="math inline">\(i+(s-1)*n\)</span> of <span class="math inline">\(\text{vec}(Y)\)</span>. If <span class="math inline">\(Z=X+Y\)</span> in <span class="math inline">\(\mathbb{R}^{n\times p}\)</span> then <span class="math inline">\(\text{vec}(Z)=\text{vec}(X)+\text{vec}(Y)\)</span> in <span class="math inline">\(\mathbb{R}^{np}\)</span>, and if <span class="math inline">\(Z=\alpha Y\)</span> for some real number <span class="math inline">\(\alpha\)</span> then also <span class="math inline">\(\text{vec}(Z)=\alpha\text{vec}(Y)\)</span>. Thus <span class="math inline">\(\text{vec}\)</span> is an isomorphism, and so is its inverse <span class="math inline">\(\text{vec}*{-1}\)</span>, which transforms an <span class="math inline">\(np\)</span>-element vector into an <span class="math inline">\(n\times p\)</span> matrix. In R we <span class="math inline">\(\text{vec}\)</span> a matrix by the as.vector function, which removes the dim attribute from the matrix, and we <span class="math inline">\(\text{vec}^{-1}\)</span> a vector by the matrix function, which adds the dim attribute to the vector.</p>
<p>But that is not all. Euclidean spaces are equipped with an inner product and a corresponding metric. The spaces <span class="math inline">\(\mathbb{R}^{n\times p}\)</span> and <span class="math inline">\(\mathbb{R}^{np}\)</span> are also isometric inner product spaces. If <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are in <span class="math inline">\(\mathbb{R}^{np}\)</span> then their inner product is <span class="math display">\[
\langle x, y\rangle_{np}:=x'y=\sum_{k=1}^{np}x_ky_k,
\]</span> If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are in <span class="math inline">\(\mathbb{R}^{n\times p}\)</span> their inner product is <span class="math display">\[
\langle X,Y\rangle_{n\times p}:=\text{tr}\ X'Y=\sum_{i=1}^{n}\sum_{s=1}^px_{is}y_{is},
\]</span> their lengths are <span class="math inline">\(\|X\|=\sqrt{\text{tr}\ X'X}\)</span> and <span class="math inline">\(\|Y\|=\sqrt{\text{tr}\ Y'Y}\)</span>, and their distance is <span class="math inline">\(\|X-Y\|\)</span>. Now <span class="math inline">\(\langle x,y\rangle=\langle\text{vec}(X),\text{vec}(Y)\rangle\)</span> and $|x-y|=|(X)-(Y)|.</p>
<p>Some formulas in MDS are more easily expressed in <span class="math inline">\(\mathbb{R}^{np}\)</span> (see, for example, section @ref(propdiff)), but most of the time we prefer to work in the more intuitive space <span class="math inline">\(\mathbb{R}^{n\times p}\)</span> of configurations (which is after all where our representations and pictures live).</p>
<p>Suppose <span class="math inline">\(Y_1,\cdots,Y_r\)</span> are <span class="math inline">\(r\)</span> linearly independent matrices in configuration space <span class="math inline">\(\mathbb{R}^{n\times p}\)</span>. We write <span class="math inline">\(\mathcal{Y}\)</span> for the <span class="math inline">\(r\)</span>-dimensional subspace spanned by the basis <span class="math inline">\(Y_1,\cdots,Y_r\)</span>. Of course if <span class="math inline">\(r=np\)</span> then <span class="math inline">\(\mathcal{Y}=\mathbb{R}^{n\times p}\)</span>.</p>
<p>If <span class="math inline">\(X\in\mathcal{Y}\)</span> then there is a <span class="math inline">\(\theta\)</span> in <em>coefficient space</em> <span class="math inline">\(\mathbb{R}^r\)</span> such that <span class="math inline">\(X=\sum_{s=1}^r\theta_s Y_s\)</span>. We now parametrize basic MDS using the new variables <span class="math inline">\(\theta\)</span>. Define</p>
<p><span class="math display">\[\begin{equation}
\tilde d_{ij}^2(\theta):=\text{tr}\ X'A_{ij}X=\theta'\tilde{A}_{ij}\theta,
(\#eq:confpar)
\end{equation}\]</span></p>
<p>with</p>
<p><span class="math display">\[\begin{equation}
\{\tilde A_{ij}\}_{st}:=\text{tr}\ Y_s'A_{ij}Y_t.
(\#eq:conftildea)
\end{equation}\]</span></p>
<p>Now</p>
<p><span class="math display">\[\begin{equation}
\tilde B(\theta):=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\frac{\delta_{ij}}{\tilde d_{ij}(\theta)}\tilde A_{ij},
(\#eq:conftildeb)
\end{equation}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{equation}
\tilde V:=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\tilde A_{ij},
(\#eq:conftildev)
\end{equation}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{equation}
\tilde\sigma(\theta):=1-2\tilde\rho(\theta)+\tilde\eta^2(\theta)=1-2\ \theta'\tilde B(\theta)\theta+\theta'\tilde V\theta.
(\#eq:conftildesigma)
\end{equation}\]</span></p>
<p>For the elements of <span class="math inline">\(\tilde B\)</span> and <span class="math inline">\(\tilde V\)</span> we see</p>
<p><span class="math display">\[\begin{align}
\tilde b_{st}(\theta)&amp;=\text{tr}\ Y_s'B(X)Y_t,\\
\tilde v_{st}&amp;=\text{tr}\ Y_s'VY_t.
\end{align}\]</span></p>
<p>Minimizing <span class="math inline">\(\sigma\)</span> over <span class="math inline">\(X\in\mathcal{Y}\)</span> is now equivalent to minimizing <span class="math inline">\(\tilde\sigma\)</span> over <span class="math inline">\(\theta\in\mathbb{R}^r\)</span>.</p>
<p>If <span class="math inline">\(\mathcal{Y}=\mathbb{R}^{n\times p}\)</span> then, in a sense, this is just notational sleight of hand. Consider, for example, using the basis where the <span class="math inline">\(Y_s\)</span> are the <span class="math inline">\(np\)</span> matrices <span class="math inline">\(e_i^{\ }e_q'\)</span>. Then</p>
<p><span class="math display">\[\begin{equation}
\{\tilde A_{ij}\}_{kq,lv}:=
\delta^{qv}\{A_{ij\}_{kl}}
(\#eq:conftildecanon)
\end{equation}\]</span></p>
<p>Using Kronecker products this can be written as <span class="math inline">\(\tilde A_{ij}=I_p\otimes A_{ij}\)</span>, the direct sum of <span class="math inline">\(p\)</span> copies of <span class="math inline">\(A_{ij}\)</span>. Obviously if <span class="math inline">\(\theta=\text{vec}(Y)\)</span> then <span class="math inline">\(d_{ij}^2(Y)=\theta'\tilde A_{ij}\theta\)</span>. Also <span class="math inline">\(\tilde B(\theta)=I_p\otimes B(Y)\)</span> and <span class="math inline">\(\tilde V=I_p\otimes V\)</span>. The only thing that changes by moving from configuration space to coefficient space, using the canonical basis of <span class="math inline">\(\mathbb{R}^{n\times p}\)</span>, is that the configuration gets strung out to a vector, and the matrices <span class="math inline">\(A_{ij}\)</span> get blown up to <span class="math inline">\(p\)</span> copies of themselves.</p>
<p>But nevertheless it is clear that coefficient space allows us to use different bases as well, and allows us to use bases for proper subspaces of dimension <span class="math inline">\(r&lt;np\)</span>. This can be the <span class="math inline">\(p(n-1)\)</span>-dimensional space of centered configurations, or the <span class="math inline">\(np-\frac12p(p+1)\)</span>- dimensional subspace of lower diagonal centered configurations. These configurations can be used to eliminate translational and rotational indeterminacy from basic MDS.</p>
<p>But the basis can also define a subspace of configurations with, for example, a rectangular lattice pattern, with the edges of the rectangle parallel to the horizontal and vertical axes (<span class="citation" data-cites="borg_leutner_83">Borg and Leutner (<a href="references.html#ref-borg_leutner_83" role="doc-biblioref">1983</a>)</span>) or, for that matter, configurations <span class="math inline">\(X\)</span> constrained to satisfy any number of (consistent) linear equality constraints. If <span class="math inline">\(r&lt;np-\frac12p(p+1)\)</span> then these applications are properly discussed as constrained multidimensional scaling or CMDS. A discussion of various forms of CMDS is in chapter @ref(cmds).</p>
</section>
<section id="our-friends-cs-and-amgm" class="level3" data-number="2.1.4">
<h3 data-number="2.1.4" class="anchored" data-anchor-id="our-friends-cs-and-amgm"><span class="header-section-number">2.1.4</span> Our Friends CS and AM/GM</h3>
<p>Perhaps the most frequently used mathematical results in this book are two elementary inequalities: the Cauchy-Schwartz and the Aritmetic-Geometric Mean inequalities. They are so important that we give them their own section in the book, their own acronyms CS and AM/GM, and we include their statements and even proofs.</p>
<div id="csineq" class="theorem">
<p>If <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are vectors in a Euclidean space <span class="math inline">\(X\)</span> then <span class="math inline">\(\langle x,y\rangle\leq\|x\|\|y\|\)</span>, with equality if and only if there is a real <span class="math inline">\(\alpha\)</span> such that <span class="math inline">\(x=\alpha y\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If <span class="math inline">\(x=0\)</span> and/or <span class="math inline">\(y=0\)</span> then obviously the result is trivially true. If <span class="math inline">\(x\not 0\)</span> and <span class="math inline">\(y\not= 0\)</span> then consider <span class="math inline">\(f(\alpha):=\|x-\alpha y\|^2=\|x\|^2+\alpha^2\|y\|^2-2\alpha\langle x,y\rangle\)</span>. Now <span class="math display">\[\begin{equation}
\min_\alpha f(\alpha)=\|x\|^2-\frac{\langle x,y\rangle^2}{\|y\|^2}\geq 0.
(\#eq:csproof)
\end{equation}\]</span> It follows that <span class="math inline">\(\langle x,y\rangle^2\leq\|x\|^2\|y\|^2\)</span>, which shows <span class="math inline">\(-\|x\|\|y\|\langle x,y\rangle\leq\|x\|\|y\|\)</span>. We have <span class="math inline">\(\min_\alpha f(\alpha)=0\)</span> if and only if <span class="math inline">\(x=\alpha y\)</span> for some <span class="math inline">\(\alpha\)</span>.</p>
</div>
<div id="amgm" class="theorem">
<p>If <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are two non-negative numbers, then <span class="math inline">\(\sqrt{xy}\leq\frac12(x+y)\)</span> with equality if and only if <span class="math inline">\(x=y\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Follows directly from <span class="math inline">\((\sqrt(x)-\sqrt(y))^2=x+y-2\sqrt{xy}\geq 0\)</span>.</p>
</div>
<p>This can also be written as</p>
<div id="amgm2" class="corollary">
<p>If <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are two non-negative numbers, then <span class="math inline">\(xy\leq\frac12(x^2+y^2)\)</span> with equality if and only if <span class="math inline">\(x=y\)</span>.</p>
</div>
<p>Combining CS and AM/GM gives</p>
<div id="amgmcs" class="corollary">
<p>If <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are vectors in a Euclidean space <span class="math inline">\(X\)</span> then <span class="math inline">\(\langle x,y\rangle\leq\frac12(\|x\|^2+\|y\|^2)\)</span>.</p>
</div>
</section>
</section>
<section id="propglobal" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="propglobal"><span class="header-section-number">2.2</span> Global Properties</h2>
<section id="propbounded" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="propbounded"><span class="header-section-number">2.2.1</span> Boundedness</h3>
<div id="stressbounded" class="theorem">
<p><span class="math inline">\(\sigma\)</span> is bounded below by zero and unbounded above.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Stress is a sum of squares, and thus it is non-negative, i.e.&nbsp;bounded below by zero. Because <span class="math inline">\(\sigma(\alpha X)=1-\alpha\rho(X)+\frac12\alpha^2\eta^2(X)\)</span> we see that for each <span class="math inline">\(X\not= 0\)</span> and for each <span class="math inline">\(K&lt;+\infty\)</span> there is an <span class="math inline">\(\alpha\)</span> such that <span class="math inline">\(\sigma(\alpha X)&gt;K\)</span>.</p>
</div>
</section>
<section id="propinvariance" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="propinvariance"><span class="header-section-number">2.2.2</span> Invariance</h3>
<div id="propinvar" class="theorem">
<p>We have the following invariances.</p>
<ul>
<li>Rotational Invariance: <span class="math inline">\(\sigma(XK)=\sigma(X)\)</span> for all <span class="math inline">\(K\)</span> with <span class="math inline">\(K'K=KK'=I\)</span>.</li>
<li>Translational Invariance: <span class="math inline">\(\sigma(X+eu')=\sigma(X)\)</span> for all <span class="math inline">\(u\in\mathbb{R}^p\)</span>.</li>
<li>Reflectional Invariance: <span class="math inline">\(\sigma(XK)=\sigma(X)\)</span> for all diagonal <span class="math inline">\(K\)</span> with <span class="math inline">\(k_{ss}=\pm 1\)</span>.</li>
<li>Evenness: <span class="math inline">\(\sigma(-X)=\sigma(X)\)</span>.</li>
</ul>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Stress only depends on the distances between the points in the configuration, and thus it is invariant under rigid geometrical transformations (rotations, reflections, and translations). Note that reflectional and evenness are actually special cases of rotational invariance.</p>
</div>
<p>It follows directly that the minimizer of stress, if it exists, cannot possibly be unique. Whatever the value at a minimum, it is shared by all rigid transformations of the configuration.</p>
<p>It also follows from translational invariance that we can minimize stress over the <span class="math inline">\(p(n-1)\)</span> dimensional subspace of <span class="math inline">\(\mathbb{R}^{n\times p}\)</span> of all <span class="math inline">\(n\times p\)</span> matrices which are centered, i.e.&nbsp;have <span class="math inline">\(e'X=0\)</span>. Rotational invariance implies we can also require without loss of generality that <span class="math inline">\(X\)</span> is orthogonal, i.e.&nbsp;that <span class="math inline">\(X'X\)</span> is diagonal. This studied in more detail in section #ref(propconfspace).</p>
</section>
<section id="propcontinuity" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="propcontinuity"><span class="header-section-number">2.2.3</span> Continuity</h3>
<p>A real-valued function <span class="math inline">\(f\)</span> on an open subset <span class="math inline">\(X\)</span> of a Euclidean space is <em>Lipschitz</em> or <em>Lipschitz continuous</em> if there is a <span class="math inline">\(K\geq 0\)</span> such that <span class="math inline">\(|f(x)-f(y)|\leq K\|x-y\|\)</span> for all <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> in <span class="math inline">\(X\)</span>. The smallest <span class="math inline">\(K\)</span> for which this inequality holds is called the <em>Lipschitz constant</em> of <span class="math inline">\(f\)</span>. Lipschitz functions are uniformly continuous, and thus continuous. Lipschitz functions are almost everywhere differentiable, and where the derivative exists there is an <span class="math inline">\(L\geq 0\)</span> such that <span class="math inline">\(\|df(x)\|\leq L\)</span>. Thus differentiable functions with an unbounded derivative are not Lipschitz.</p>
<p>A function <span class="math inline">\(f\)</span> is <em>locally Lipschitz</em> on <span class="math inline">\(X\)</span> if for each <span class="math inline">\(x\in X\)</span> there is a open neighborhood <span class="math inline">\(\mathcal{N}(x)\)</span> such that <span class="math inline">\(f\)</span> is Lipschitz on <span class="math inline">\(\mathcal{N}(x)\)</span>. A locally Lipschitz function is continuous and almost everywhere differentiable. Continuously differentiable functions and convex functions are all locally Lipschitz.</p>
<div id="proplip" class="theorem">
<p>On <span class="math inline">\(\mathbb{R}^{n\times p}\)</span></p>
<ul>
<li><span class="math inline">\(d_{ij}\)</span> is Lipschitz continuous with Lipschitz constant <span class="math inline">\(\sqrt{2}\)</span>.</li>
<li><span class="math inline">\(d_{ij}^2\)</span> is locally Lipschitz, but not globally Lipschitz.</li>
</ul>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>To show that <span class="math inline">\(d_{ij}\)</span> is Lipschitz we use the reverse triangle inequality <span class="math inline">\(|\|x\|-\|y\||\leq\|x - y\|\)</span>. It gives</p>
<p><span class="math display">\[\begin{equation}
|d_{ij}(X)-d_{ij}(Y)|=
|\|X'(e_i-e_j)\|-\|Y'(e_i-e_j)\||
\leq\|(X-Y)'(e_i-e_j)\|\leq\sqrt{2}\ \|X-Y\|.
(\#eq:revtrian)
\end{equation}\]</span></p>
<p>To show this Lipschitz bound is sharp use <span class="math inline">\(Y=0\)</span> and <span class="math inline">\(X=\begin{bmatrix}\hfill x\\-x\end{bmatrix}\)</span> with <span class="math inline">\(\|x\|=1\)</span>. Then <span class="math inline">\(|d(X)-d(Y)|=2\)</span> and <span class="math inline">\(\|X-Y\|=\sqrt{2}\)</span>.</p>
<p>Because <span class="math inline">\(d_{ij}^2\)</span> is continuously differentiable it is locally Lipschitz, and because its derivative is unbounded it is not globally Lipschitz.</p>
</div>
<div id="proplosscont" class="corollary">
<p>On <span class="math inline">\(\mathbb{R}^{n\times p}\)</span></p>
<ul>
<li><span class="math inline">\(\rho\)</span> is Lipschitz continuous with Lipschitz constant <span class="math inline">\(\sqrt{2}\ \mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\delta_{ij}\)</span>.</li>
<li><span class="math inline">\(\eta^2\)</span> and <span class="math inline">\(\sigma\)</span> are both locally Lipschitz, but not globally Lipschitz.</li>
</ul>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>This follows directly from theorem @ref(thm:proplip).</p>
</div>
</section>
<section id="propcoercive" class="level3" data-number="2.2.4">
<h3 data-number="2.2.4" class="anchored" data-anchor-id="propcoercive"><span class="header-section-number">2.2.4</span> Coercivity</h3>
<p>Stress is not a quadratic function, and not even a convex function, of the configuration. But it is like a bowl shaped around the origin, with some bumps and creases, in a way we are going to make more precise. First a definition: A real-valued function <span class="math inline">\(f\)</span> is <em>coercive</em> if for every sequence <span class="math inline">\(\{x_k\}\)</span> with <span class="math inline">\(\lim_{k\rightarrow\infty}\|x_k\|=\infty\)</span> we also have <span class="math inline">\(\lim_{k\rightarrow\infty}f(x_k)=+\infty\)</span>.</p>
<div id="propcoerc" class="theorem">
<p><span class="math inline">\(\sigma\)</span> is coercive.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>From @ref(eq:propcssigeta1) we have <span class="math inline">\(\sigma(X)\geq\frac12(1-\eta(X))^2\)</span>. Now <span class="math inline">\(\eta\)</span> is clearly coercive, and thus <span class="math inline">\(\sigma\)</span> is coercive.</p>
</div>
<p>It follows from coercivity that all level sets of stress <span class="math inline">\(\mathcal{L}_s:=\{X\mid \sigma(X)=s\}\)</span> are compact, and that there is at least one configuration for which the global minimum of stress is attained (<span class="citation" data-cites="ortega_rheinboldt_70">Ortega and Rheinboldt (<a href="references.html#ref-ortega_rheinboldt_70" role="doc-biblioref">1970</a>)</span>, section 4.3).</p>
<p>The following theorem provides even more bowl-shapedness.</p>
<div id="rayquad" class="theorem">
<p>If <span class="math inline">\(X\not=0\)</span> then on the ray <span class="math inline">\(\{Y\mid Y=\alpha X\text{ with }\alpha\geq 0\}\)</span> stress is an unbounded convex quadratic in <span class="math inline">\(\alpha\)</span>. The minimum of this quadratic is at <span class="math inline">\(\alpha=\rho(X)/\eta^2(X)\)</span> and it is equal to <span class="math inline">\(1-\frac12\rho^2(X)/\eta^2(X)\)</span>. There is a local maximum at the boundary <span class="math inline">\(\alpha=0\)</span>, equal to 1.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We have <span class="math inline">\(\sigma(\alpha X)=1-\alpha\rho(X)+\frac12\alpha^2\eta^2(X)\)</span>. The statements in the theorem follow easily from this.</p>
</div>
</section>
</section>
<section id="propdiff" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="propdiff"><span class="header-section-number">2.3</span> Differentiability</h2>
<p>The fact that <span class="math inline">\(d_{ij}\)</span> can be zero for some configurations creates problems with the differentiability of stress. These problems have been largely ignored in the MDS literature, and there are indeed reasons why they are not of great <strong>practical</strong> importance (see section @ref(proplocmin) of this chapter), at least not in basic MDS. But for reasons of completeness, and for later generalizations of basic MDS, we discuss zero distances and the resulting problems with differentiability in some detail.</p>
<p>Historically the complications caused by <span class="math inline">\(d_{ij}(X)=0\)</span> were one of the reasons why I switched from differentiability to convexity in <span class="citation" data-cites="deleeuw_C_77">De Leeuw (<a href="references.html#ref-deleeuw_C_77" role="doc-biblioref">1977</a>)</span> and from derivatives to directional derivatives in <span class="citation" data-cites="deleeuw_A_84f">De Leeuw (<a href="references.html#ref-deleeuw_A_84f" role="doc-biblioref">1984</a>)</span>. It turned out that at least some of the important characteristics of the smacof algorithm, and several important aspects of stress surfaces, were better described by inequalities than by equations.</p>
<p>### Directional Derivatives</p>
<p>Because we are dealing with minimization of stress, which is not everywhere differentiable, we use one-sided directional derivatives. Our notation largely follows <span class="citation" data-cites="delfour_12">Delfour (<a href="references.html#ref-delfour_12" role="doc-biblioref">2012</a>)</span>.</p>
<p>The first three directional derivatives at <span class="math inline">\(X\)</span> in the direction <span class="math inline">\(Y\)</span> are defined recursively by <span class="math display">\[\begin{align}
d_+\sigma(X;Y)&amp;:=\lim_{\epsilon\downarrow 0}\frac{\sigma(X+\epsilon Y)-\sigma(X)}{\epsilon},
(\#eq:ddd1stress)\\
d_+^{(2)}\sigma(X;Y,Y)&amp;:=\lim_{\epsilon\downarrow 0}\frac{\sigma(X+\epsilon\ Y)-\sigma(X)-\epsilon\ d\sigma(X)(Y)}{\frac12\epsilon^2},
(\#eq:ddd2stress)\\
d_+^{(3)}\sigma(X;Y,Y,Y)&amp;:=\lim_{\epsilon\downarrow 0}\frac{\sigma(X+\epsilon\ Y)-\sigma(X)-\epsilon\ d_+\sigma(X)(Y)-\frac12\epsilon^2\ d_+^{(2)}\sigma(X)(Y,Y)}{\frac16\epsilon^3},
(\#eq:ddd3stress)
\end{align}\]</span> where <span class="math inline">\(\epsilon\downarrow 0\)</span> is understood as <span class="math inline">\(\epsilon\)</span> taking only strictly positive values in computing the limit, and where it is also understood that the one-sided limits exist. The directional derivatives used in optimization theory differ from the usual derivatives of analysis because the limits in functions that define them are over the one-dimensional positive real axis and are one-sided (from the right). You may wonder why we need to go as high as order three, but just you wait.</p>
<p>Note that we write <span class="math inline">\(d_+\sigma(X;Y)\)</span> (with a semi-colon) instead of <span class="math inline">\(d_+\sigma(X,Y)\)</span> (with a comma) to emphasize the different roles of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Also note that by “directional derivatives” we will always mean “one-sided directional derivatives”, because the two-sided ones are of limited usefulness in optimization. This is especially true for the two-sided directional derivative defined by <span class="math display">\[\begin{equation}
d\sigma(X;Y):=\lim_{\epsilon\rightarrow 0}\frac{\sigma(X+\epsilon Y)-\sigma(X)}{\epsilon}.
(\#eq:twosided)
\end{equation}\]</span> The two-sided derivative may not exist, while we can still make useful statements of the minima of non-differentiable functions using the one-sided version. Of course for totally differentiable functions in the classical sense the two directional derivatives are equal.</p>
<p>For the higher directional derivatives we can also use alternative, and slightly more general, definitions that follow directly from the idea that the <span class="math inline">\(k^{th}\)</span> directional derivative is the directional derivative of the <span class="math inline">\((k-1)^{th}\)</span> one. In each step of the recursion we now use a different direction, instead of using the fixed direction <span class="math inline">\(Y\)</span> in all steps. Thus <span class="math display">\[\begin{align}
d_+^{(2)}\sigma(X;Y,Z)&amp;:=\lim_{\epsilon\downarrow 0}\frac{d_+\sigma(X+\epsilon Z;Y)-d_+\sigma(X;Y)}{\epsilon},(\#eq:altdd2)\\
d_+^{(3)}\sigma(X;Y,Z,U)&amp;:=\lim_{\epsilon\downarrow 0}\frac{d_+^2\sigma(X+\epsilon U;Y,Z)-d_+^2\sigma(X;Y,Z)}{\epsilon}.(\#eq:altdd3)
\end{align}\]</span></p>
<p>Note again that <span class="math inline">\(d_+\sigma(X)\)</span> is a function on <span class="math inline">\(\mathbb{R}^{n\times p}\)</span>, <span class="math inline">\(d_+^2\sigma(X)\)</span> a is a function on <span class="math inline">\(\mathbb{R}^{n\times p}\otimes\mathbb{R}^{n\times p}\)</span>, and <span class="math inline">\(d_+^3\sigma(X)\)</span> is a function on <span class="math inline">\(\mathbb{R}^{n\times p}\otimes\mathbb{R}^{n\times p}\otimes\mathbb{R}^{n\times p}\)</span>.</p>
<p>If <span class="math inline">\(\sigma\)</span> is differentiable at <span class="math inline">\(X\)</span> then <span class="math inline">\(d_+\sigma(X), d_+^{(2)}\sigma(X),\)</span> and <span class="math inline">\(d_+^{(3)}\sigma(X)\)</span> are the usual first, second, and third derivatives of <span class="math inline">\(\sigma\)</span> at <span class="math inline">\(X\)</span>. In this differentiable case they are, respectively, a linear function, a symmetric bilinear function, and a super-symmetric trilinear function.</p>
<p>We can use the directional derivatives to expand <span class="math inline">\(\sigma(X+\epsilon Y)\)</span> in powers of <span class="math inline">\(\epsilon\)</span>. This gives an expansion of the form <span class="math display">\[\begin{align}
\begin{split}
\sigma(X+\epsilon Y)&amp;=\sigma(X)+\epsilon d_+\sigma(X)(Y)+\frac12\epsilon^2d^{(2)}_+\sigma(X)(Y,Y)+\\&amp;+\frac16\epsilon^2d_+{(3)}\sigma(X)(Y,Y,Y)+\text{o}(\epsilon^3),
\end{split}(\#eq:desexp)
\end{align}\]</span> where <span class="math inline">\(\text{o}(\epsilon^3)\)</span> stand for any function of <span class="math inline">\(\epsilon&gt;0\)</span> such that <span class="math display">\[\begin{equation}
\lim_{\substack{\epsilon\downarrow 0\\\epsilon\not= 0}}
\frac{\text{o}(\epsilon^3)}{\epsilon^3}\rightarrow 0.
(\#eq:littleo)
\end{equation}\]</span></p>
<section id="distances" class="level4" data-number="2.3.0.1">
<h4 data-number="2.3.0.1" class="anchored" data-anchor-id="distances"><span class="header-section-number">2.3.0.1</span> Distances</h4>
<p>Let us look at the directional differentiability of the distances <span class="math inline">\(d_{ij}\)</span> themselves first. Since <span class="math inline">\(\rho\)</span> is a straightfoward weighted sum of distances and <span class="math inline">\(\eta^2\)</span> is a weighted sum of squared distances, the only directional derivatives we really need are those of the squared distances and the distances.</p>
<p>The problems with differentiability are clearly not caused by the squared distances, which form the <span class="math inline">\(\eta^2\)</span> component in equation @ref(eq:expand). The squared distance <span class="math inline">\(d_{ij}^2(X)=\text{tr}\ X'A_{ij}X\)</span> is a quadratic function, and thus it is everywhere infinitely many times continuously differentiable.</p>
<p>On the other hand, <span class="math inline">\(d_{ij}(X)=\sqrt{\text{tr}\ X'A_{ij}X}\)</span> is not differentiable at points where <span class="math inline">\(d_{ij}(X)=0\)</span>, i.e.&nbsp;where <span class="math inline">\(x_i=x_j\)</span>, because the square root is not differentiable at zero. For MDS this means that if <span class="math inline">\(d_{ij}(X)=0\)</span> for one or more <span class="math inline">\((i,j)\)</span> with <span class="math inline">\(w_{ij}\delta_{ij}&gt;0\)</span> then both <span class="math inline">\(\rho\)</span> and <span class="math inline">\(\sigma\)</span> are not differentiable at <span class="math inline">\(X\)</span>.</p>
<p>For the avalanche of furmulas that will follow in this section it is convenient to define <span class="math inline">\(c_{ij}(X,Y):=\text{tr}\ Y'A_{ij}X=(y_i-y_j)'(x_i-x_j)\)</span>. Note that <span class="math inline">\(c_{ij}(X,X)=d_{ij}^2(X)\)</span> and <span class="math inline">\(c_{ij}(Y,Y)=d_{ij}^2(Y)\)</span>. Now <span class="math display">\[\begin{align}
d_+d_{ij}^2(X;Y)&amp;=2c_{ij}(X,Y),(\#eq:ddddd1)\\
d_+^2d_{ij}^2(X;Y,Z)&amp;=2c{ij}(Y,Z),(\#eq:ddddd2)\\
d_+^3d_{ij}^2(X;Y,Z,U)&amp;=0.(\#eq:ddddd3)
\end{align}\]</span></p>
<p>More involved calculations are needed for the directional derivatives of <span class="math inline">\(d_{ij}\)</span>. First the “problematic” case <span class="math inline">\(d_{ij}(X)=0\)</span>. We have <span class="math display">\[\begin{align}
d_+d_{ij}(X;Y)&amp;=d_{ij}(Y),(\#eq:dddzero1)\\
d_+^2d_{ij}(X;Y)&amp;=0,(\#eq:dddzero2)\\
d_+^3d_{ij}(X;Y)&amp;=0.(\#eq:dddzero3)\\
\end{align}\]</span></p>
<p>Note that <span class="math inline">\(d_+d_{ij}(X)\)</span> is continuous but not linear in <span class="math inline">\(Y\)</span>, which also implies <span class="math inline">\(d_{ij}\)</span> is not differentiable at <span class="math inline">\(X\)</span>. Also note that if we define the two-sided directional derivative of <span class="math inline">\(d_{ij}\)</span> at <span class="math inline">\(X\)</span> in the direction <span class="math inline">\(Y\)</span> <span class="math display">\[\begin{equation}
dd_{ij}(X;Y):=\lim_{\epsilon\rightarrow 0}\frac{d_{ij}(X+\epsilon Y)-d_{ij}(X)}{\epsilon},
(\#eq:wronglim)
\end{equation}\]</span> then this limit only exists if also <span class="math inline">\(d_{ij}(Y)=0\)</span>. The limit from the right is <span class="math inline">\(d_{ij}(Y)\)</span>, but the limit from the left is <span class="math inline">\(-d_{ij}(Y)\)</span>. This illustrates that the two-sided directional derivative does not give much useful information on the behavior of the distance at zero.</p>
<p>If <span class="math inline">\(d_{ij}(X)&gt;0\)</span> we have continuous differentiability of all orders at <span class="math inline">\(X\)</span>. To expand <span class="math display">\[\begin{equation}
d_{ij}(X+\epsilon Y)=\sqrt{d_{ij}^2(X)+2\ \epsilon\ c_{ij}(X,Y)+\epsilon^2\ d_{ij}^2(Y)}
(\#eq:ddd1exp)
\end{equation}\]</span> we use the truncated Maclaurin series for the square root <span class="math display">\[\begin{equation}
\sqrt{1+x}=1+\frac12x-\frac18x^2+\frac{1}{16}x^3+\text{o}(x^3).
(\#eq:maclaurin)
\end{equation}\]</span> For the distance this gives the series <span class="math display">\[\begin{align}
\begin{split}
d_{ij}(X+\epsilon Y)&amp;=d_{ij}(X)+\epsilon\frac{1}{d_{ij}(X)}c_{ij}(X,Y)+\\
&amp;+\frac12\epsilon^2\frac{1}{d_{ij}(X)}\left\{d_{ij}^2(Y)-\frac{c_{ij}^2(X,Y)}{d_{ij}^2(X)}\right\}+\\
&amp;-\frac12\epsilon^3\frac{c_{ij}(X,Y)}{d_{ij}^3(X)}\left\{d_{ij}^2(Y)-\frac{c_{ij}^2(X,Y)}{d_{ij}^2(X)}
\right\}+\text{o}(\epsilon^3),
\end{split}(\#eq:sqdexp)
\end{align}\]</span> and consequently <span class="math display">\[\begin{align}
d_+d_{ij}(X;Y)&amp;=\frac{1}{d_{ij}(X)}c_{ij}(X,Y),(\#eq:expdfinal1)\\
d_+^{(2)}d_{ij}(X;Y,Y)&amp;=\frac{1}{d_{ij}(X)}\left\{d_{ij}^2(Y)-\frac{c_{ij}^2(X,Y)}
{d_{ij}^2(X)}\right\},(\#eq:expdfinal2)\\
d_+^{(3)}d_{ij}(X;Y,Y,Y)&amp;=-3\frac{c_{ij}(X,Y)}{d_{ij}^3(X)}\left\{d_{ij}^2(Y)-\frac{c_{ij}^2(X,Y)}{d_{ij}^2(X)}\right\}.(\#eq:expdfinal3)
\end{align}\]</span></p>
<p>Formulas for the mixed directional derivatives from equations @ref(eq:altdd2) and @ref(eq:altdd3) are necessarily more complicated. Again assuming <span class="math inline">\(d_{ij}(X)&gt;0\)</span> we find <span class="math display">\[\begin{equation}
d_+^{(2)}d_{ij}(X;Y,Z)=\frac{1}{d_{ij}(X)}\left\{c_{ij}(Y,Z)-\frac{c_{ij}(X,Y)c_{ij}(X,Z)}{d_{ij}^2(X)}\right\},
(\#eq:altddd2)
\end{equation}\]</span> which reduces to @ref(eq:expdfinal2) if <span class="math inline">\(Y=Z\)</span>. And, with <span class="math inline">\(95\%\)</span> certainty, <span class="math display">\[\begin{align}
\begin{split}
d_+^{(3)}d_{ij}(X;Y,Z,U)&amp;=3\frac{c_{ij}(X,Y)c_{ij}(X,Z)c_{ij}(X,U)}{d_{ij}^5(X)}+\\&amp;-\frac{c_{ij}(X,Y)c_{ij}(U,Y)+
c_{ij}(X,Z)c_{ij}(U,Z)+c_{ij}(X,U)c_{ij}(Y,Z)}{d_{ij}^3(X)}
\end{split}
(\#eq:ddmix3)
\end{align}\]</span> which is obviously symmetric in <span class="math inline">\(Y, Z,\)</span> and <span class="math inline">\(U\)</span>, and if <span class="math inline">\(Y=Z=U\)</span> it reduces to @ref(eq:expdfinal3).</p>
<p>Again, we have to be careful if <span class="math inline">\(d_{ij}(X)=0\)</span>. In that case we know from equation @ref(eq:dddzero1) that <span class="math inline">\(d_+d_{ij}(X;Y)=d_{ij}(Y)\)</span>. Also <span class="math display">\[\begin{equation}
d_+d_{ij}(X+\epsilon Z;Y)=\begin{cases}d_{ij}(Y)&amp;\text{ if }d_{ij}(Z)=0,\\
\frac{1}{d_{ij}(Z)}c_{ij}(Y,Z)&amp;\text{ if }d_{ij}(Z)&gt;0,
\end{cases}
(\#eq:dmixedzero)
\end{equation}\]</span> and thus <span class="math inline">\(d_+^{(2)}d_{ij}(X;Y,Z)=0\)</span> when <span class="math inline">\(d_{ij}(Z)=0\)</span>, but when <span class="math inline">\(d_{ij}(Z)&gt;0\)</span> the limit defining <span class="math inline">\(d_+^{(2)}d_{ij}(X;Y,Z)\)</span> does not exist unless <span class="math inline">\(Z=Y\)</span>. If <span class="math inline">\(Z=Y\)</span> we have <span class="math inline">\(d_+^{(2)}d_{ij}(X;Y,Y)=0\)</span>, in accordance with @ref(eq:dddzero2).</p>
</section>
<section id="secrhostress" class="level4" data-number="2.3.0.2">
<h4 data-number="2.3.0.2" class="anchored" data-anchor-id="secrhostress"><span class="header-section-number">2.3.0.2</span> Rho and Stress</h4>
<p>We now use the results from the previous section to compute directional derivatives of <span class="math inline">\(\rho\)</span> and <span class="math inline">\(\sigma\)</span>. They are general, in the sense that they cover cases in which some <span class="math inline">\(d_{ij}(X)&gt;0\)</span> and some <span class="math inline">\(d_{ij}(X)=0\)</span>. To handle one or more zero distances we define <span class="math display">\[\begin{equation}
\xi(X;Y):=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}\{w_{ij}\delta_{ij}d_{ij}(Y)\mid w_{ij}\delta_{ij}&gt;0\text{ and }d_{ij}(X)=0\}.
(\#eq:xidef)
\end{equation}\]</span></p>
<p>The directional derivatives of <span class="math inline">\(\rho\)</span> at <span class="math inline">\(X\)</span> in direction <span class="math inline">\(Y\)</span> are <span class="math display">\[\begin{align}
d_+\rho(X;Y)&amp;=\text{tr}\ Y'B(X)X+\xi(X,Y),(\#eq:rhoder1)\\
d_+^{(2)}\rho(X;Y,Y)&amp;=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\frac{\delta_{ij}}{d_{ij}(X)}\left\{d_{ij}^2(Y)-\frac{c_{ij}^2(X,Y)}
{d_{ij}^2(X)}\right\},(\#eq:rhoders2)\\
d_+^{(3)}\rho(X;Y,Y,Y)&amp;=-3\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\frac{\delta_{ij}c_{ij}(X,Y)}{d_{ij}^3(X)}\left\{d_{ij}^2(Y)-\frac{c_{ij}^2(X,Y)}{d_{ij}^2(X)}\right\}.(\#eq:rhoders3)
\end{align}\]</span></p>
<p>Note that by the CS inequality <span class="math display">\[\begin{equation}
d_{ij}^2(Y)-\frac{c_{ij}^2(X,Y)}{d_{ij}^2(X)}\geq 0
(\#eq:posterm)
\end{equation}\]</span> for all <span class="math inline">\(Y\)</span>, and thus <span class="math inline">\(d_+^{(2)}\rho(X)(Y,Y)\geq 0\)</span>.</p>
<p>The directional derivatives for stress at <span class="math inline">\(X\)</span> in direction <span class="math inline">\(Y\)</span> are <span class="math display">\[\begin{align}
d_+\sigma(X;Y)&amp;=\text{tr}\ Y'(V-B(X))X-\xi(X,Y),(\#eq:stressders1)\\
d_+^{(2)}\sigma(X;Y,Y)&amp;=\text{tr}\ Y'VY-\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\frac{\delta_{ij}}{d_{ij}(X)}\left\{d_{ij}^2(Y)-\frac{c_{ij}^2(X,Y)}
{d_{ij}^2(X)}\right\},(\#eq:stressders2)\\
d_+^{(3)}\sigma(X;Y,Y,Y)&amp;=3\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\frac{\delta_{ij}c_{ij}(X,Y)}{d_{ij}^3(X)}\left\{d_{ij}^2(Y)-\frac{c_{ij}^2(X,Y)}{d_{ij}^2(X)}\right\}.(\#eq:stressders3)
\end{align}\]</span></p>
<p>Note that we can also write @ref(eq:stressders2) as <span class="math display">\[\begin{equation}
d_+^{(2)}\sigma(X;Y,Y)=\text{tr}\ Y'(V-B(X))Y+\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\frac{\delta_{ij}}{d_{ij}(X)}\frac{c_{ij}^2(X,Y)}
{d_{ij}^2(X)}.
(\#eq:stressders2alt)
\end{equation}\]</span></p>
<p>Combining @ref(eq:stressders2) and @ref(eq:stressders2alt) shows <span class="math display">\[\begin{equation}
\text{tr}\ Y'(V-B(X))Y\lesssim d_+^{(2)}\sigma(X;Y,Y)\lesssim\text{tr}\ Y'VY.
(\#eq:stressdes2ineq)
\end{equation}\]</span></p>
<p>A convenient upper bound for <span class="math inline">\(d_+^{(3)}\sigma(X;Y,Y,Y)\)</span> is also useful. From @ref(eq:stressders3) and the CS inequality <span class="math display">\[\begin{equation}
d_+^{(3)}\sigma(X;Y,Y,Y)\leq 3\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\frac{\delta_{ij}|c_{ij}(X,Y)|}{d_{ij}^3(X)}d_{ij}^2(Y)\leq 3\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\frac{\delta_{ij}d_{ij}^3(Y)}{d_{ij}^2(X)}.
(\#eq:stressdes3ineq)
\end{equation}\]</span></p>
<p>Once more, in the differentiable case the subscript + in <span class="math inline">\(d_+\)</span> is not necessarily, but if one or more <span class="math inline">\(d_{ij}(X)=0\)</span> for which <span class="math inline">\(w_{ij}d_{ij}&gt;0\)</span> then the subscript is needed.</p>
</section>
<section id="expandstress" class="level4" data-number="2.3.0.3">
<h4 data-number="2.3.0.3" class="anchored" data-anchor-id="expandstress"><span class="header-section-number">2.3.0.3</span> expandStress</h4>
<p>The R function {, with arguments w, delta, x, and y, gives the zeroeth, first, second, or third terms in the expansion @ref(eq:desexp), using the formulas for directional derivatives in this section. We use an example with <span class="math inline">\(n=4\)</span> and <span class="math inline">\(p=2\)</span>. All weights and dissimilarities are equal. Configuration <span class="math inline">\(X\)</span> are the four corners of a square, perturbation <span class="math inline">\(Y\)</span> is a <span class="math inline">\(4\times 2\)</span> matrix of random standard normals.</p>
<p>The function takes an interval around zero for <span class="math inline">\(\epsilon\)</span> and computes the value of <span class="math inline">\(\sigma(X+\epsilon Y)\)</span> at 1000 points in that interval. It also computes the zero, first, second, or third order Maclaurin approximations to <span class="math inline">\(\sigma(X+\epsilon Y)\)</span>. In this first example <span class="math inline">\(X\)</span> is a local minimum and thus <span class="math inline">\(d_+\sigma(X;Y)=0\)</span>. The zero-order and first-order approximation are thus the same.</p>
<p>If the interval for <span class="math inline">\(\epsilon\)</span> is <span class="math inline">\([-1,1]\)</span> the sum of squares of the differences between <span class="math inline">\(\sigma(X+\epsilon Y)\)</span> and its approximations of different orders are</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>     [,1]                
[1,] +917.455222294744431
[2,] +917.455222294744885
[3,] +468.774392806713536
[4,] +185.377911517761618</code></pre>
</div>
</div>
<p>If the interval is <span class="math inline">\([-.1,.1]\)</span> the errors of approximation are</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>     [,1]              
[1,] +0.133635212939684
[2,] +0.133635212939684
[3,] +0.000152399828616
[4,] +0.000001520085462</code></pre>
</div>
</div>
<p>And if the interval is <span class="math inline">\([-.01,.01]\)</span> the errors of approximation are</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>     [,1]              
[1,] +0.000013434031631
[2,] +0.000013434031631
[3,] +0.000000000149172
[4,] +0.000000000000015</code></pre>
</div>
</div>
<p>On the smaller intervals the error of second-order approximation is already very small, which is not surprising because twice-differentiable functions are pretty much convex quadratics close to a local minimum.</p>
</section>
<section id="partial-derivatives" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="partial-derivatives"><span class="header-section-number">2.3.1</span> Partial Derivatives</h3>
<p>In the differentiable case we now introduce <em>gradients</em> and <em>Hessians</em>. The gradient at <span class="math inline">\(X\)</span> is a vector with first-order partial derivatives at <span class="math inline">\(X\)</span>, the Hessian is a matrix with second-order partial derivatives at <span class="math inline">\(X\)</span>. Partial derivatives are the commonly used tool for actual computation of derivatives.</p>
<p>We can obtain the partial derivatives from the directional derivatives in equations @ref(eq:stressders1), @ref(eq:stressders2), and @ref(eq:stressders3) by using the base vectors <span class="math inline">\(E_{is}=e_i^{\ }e_s'\)</span> to expand the perturbations <span class="math inline">\(Y, Z,\)</span> and <span class="math inline">\(U\)</span>. So the gradient of stress at <span class="math inline">\(X\)</span> is<br>
<span class="math display">\[\begin{equation}
\{\nabla\sigma(X)\}_{is}:=d_+\sigma(X;E_{is})=\text{tr}\ e_ie_s'(V-B(X))X=\{(V-B(X))X\}_{is}.
(\#eq:nabla1stress)
\end{equation}\]</span> The gradient <span class="math inline">\(\nabla\sigma(X)\)</span> is an <span class="math inline">\(n\times p\)</span> matrix, and <span class="math display">\[\begin{equation}
d_+\sigma(X;Y)=\text{tr}\ Y'\nabla\sigma(X)=\text{tr}\ Y'(V-B(X))X.
(\#eq:ipform)
\end{equation}\]</span></p>
<p>For the Hessian we use similar calculations, heavily relying on the Kronecker delta, and on cyclic permutations under the trace sign. First <span class="math display">\[\begin{equation}
c_{ij}(X,E_{ks})=\text{tr}\ X'(e_i-e_j)(e_i-e_j)'e_ks_s'=(x_{is}-x_{js})(\delta^{ik}-\delta^{jk}),
(\#eq:cxe1)
\end{equation}\]</span> and <span class="math display">\[\begin{equation}
c_{ij}(E_{ks},E_{lt})=\text{tr}\ e_te_l'(e_i-e_j)(e_i-e_j)'e_ke_s'=\delta^{st}(\delta^{ik}-\delta^{jk})(\delta^{il}-\delta^{jl}).
(\#eq:ce1e2)
\end{equation}\]</span></p>
<p>Thus, from equation @ref(eq:ddddd2), <span class="math display">\[\begin{equation}
\{\nabla^{(2)}d_{ij}^2(X)\}_{ks,lt}=d_+^{(2)}d_{ij}^2(X;E_{ks},E_{lt})=2\delta^{st}(\delta^{ik}-\delta^{jk})(\delta^{il}-\delta^{jl}),(\#eq:nabdd)
\end{equation}\]</span> and from equation @ref(eq:altddd2) <span class="math display">\[\begin{align}
\begin{split}
\{\nabla^{(2)}d_{ij}(X)\}_{ks,lt}&amp;=d_+^{(2)}d_{ij}(X;E_{ks},E_{lt})=\\&amp;=\frac{(\delta^{il}-\delta^{jl})(\delta^{ik}-\delta^{jk})}{d_{ij}(X)}\left\{\delta^{st}-\frac{(x_{is}-x_{js})(x_{it}-x_{jt})}{d_{ij}^2(X)}\right\}.
\end{split}(\#eq:nabd)
\end{align}\]</span></p>
<p>Now take the usual weighted sums of equations @ref(eq:nabdd) and @ref(eq:nabd) to find the Hessians of <span class="math inline">\(\rho\)</span> and <span class="math inline">\(\sigma\)</span>. To get relatively compact expressions we define the symmetric doubly-centered matrices <span class="math display">\[\begin{equation}
H_{st}(X):=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\frac{\delta_{ij}}{d_{ij}^3(X)}(x_{is}-x_{js})(x_{it}-x_{jt})A_{ij}.
(\#eq:defhmat)
\end{equation}\]</span> Then the Hessian of <span class="math inline">\(\rho\)</span> is <span class="math display">\[\begin{equation}
\{\nabla^{(2)}\rho(X)\}_{ks,lt}=\{\delta^{st}B(X)-H_{st}(X)\}_{kl},
(\#eq:nabla2rho)
\end{equation}\]</span> and that of <span class="math inline">\(\sigma\)</span> is <span class="math display">\[\begin{equation}
\{\nabla^{(2)}\sigma(X)\}_{ks,lt}=\{\delta^{st}(V-B(X))+H_{st}(X)\}_{kl}.
(\#eq:nabla2stress)
\end{equation}\]</span></p>
<p>This is all somewhat inconvenient because of the double indexing of rows and columns. <span class="math inline">\(\nabla^{(2)}\sigma(X)\)</span> is an element of <span class="math inline">\(\mathbb{R}^{n\times p}\otimes\mathbb{R}^{n\times p}\)</span> and we can represent it numerically either as a matrix or a four-dimensional array.</p>
<p>To cut the cord we use the <span class="math inline">\(\text{vec}\)</span> isomorphism from <span class="math inline">\(\mathbb{R}^{n\times p}\)</span> to <span class="math inline">\(\mathbb{R}^{np}\)</span>, and its inverse <span class="math inline">\(\text{vec}^{-1}\)</span>.</p>
<p>For computational purposes you can think of <span class="math inline">\(\nabla^2\sigma(X)\)</span> as an <span class="math inline">\(np\times np\)</span> matrix <span class="math inline">\(K(X)\)</span>, consisting of blocks of symmetric matrices of order <span class="math inline">\(n\)</span>, indexed by points, with <span class="math inline">\(p\)</span> row-blocks and <span class="math inline">\(p\)</span> column-blocks, indexed by dimensions. For <span class="math inline">\(s\not= t\)</span> block <span class="math inline">\((s,t)\)</span> is the matrix <span class="math inline">\(H_{st}(X)\)</span>, the diagonal blocks for <span class="math inline">\(s=t\)</span> are <span class="math inline">\((V-B(X))+H_{ss}(X)\)</span>. In the same way we can collect the blocks <span class="math inline">\(H_{st}(X)\)</span> in the <span class="math inline">\(np\times np\)</span> matrix <span class="math inline">\(H(X)\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\{\nabla^{(2)}\sigma(X;Y)\}_{ks}:=\sum_{l=1}^n\sum_{t=1}^p\{\nabla^{(2)}\sigma(X)\}_{ks,lt}y_{lt},
(\#eq:linform),
\end{equation}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{equation}
\text{tr}\ Y'\nabla^2\sigma(X)Z:=d_+^2\sigma(X;Y,Z)=\sum_{k=1}^n\sum_{s=1}^p\sum_{l=1}^n\sum_{t=1}^p\{\nabla^2\sigma(X)\}_{ks,lt}y_{ks}z_{lt}.
(\#eq:quadform)
\end{equation}\]</span></p>
<p>Thus</p>
<p><span class="math display">\[\begin{align}
\underline{\nabla}^{(2)}\rho(X)&amp;=I_p\otimes B(X)-\underline{H}(X),(\#eq:shorthess1)\\
\underline{\nabla}^{(2)}\sigma(X)&amp;=I_p\otimes(V-B(X))+\underline{H}(X).(\#eq:shorthess2)
\end{align}\]</span></p>
<p>I do not like to use vec and friends in formulas and derivations, so I try to avoid them. It is a different matter in computation, because in a computer <span class="math inline">\(Y\)</span> is the same as <span class="math inline">\(\text{vec}(Y)\)</span> anyway.</p>
<p>Because the Hessian is important throughout the book we want to make sure we have the correct formulas and code. One way to check this is to compare it to the numerical approximation of the Hessian from the package numDeriv (<span class="citation" data-cites="gilbert_varadhan_19">Gilbert and Varadhan (<a href="references.html#ref-gilbert_varadhan_19" role="doc-biblioref">2019</a>)</span>). Normally one checks if the code is correct by comparing it with the mathematics, but here we proceed the other way around.</p>
<p>Again we use the four corners of the square as an example, with weights and dissimilarities all equal. The largest absolute difference between the elements of the numerical and the analytical Hessian for ths example is 1.5404344^{-13}, which means that we basically have double-precision equality between the two. We repeat this for a random <span class="math inline">\(X\)</span>, because at a local minimum the approximation may be more precise. For the random configuration we find a maximum deviation of 0.2485061, a bit bigger, but still small.</p>
<p>Here are some useful properties of the Hessians.</p>
<div id="hessbounds" class="theorem">
<ol type="1">
<li><span class="math inline">\(0\lesssim H(X)\lesssim I_p\otimes B(X).\)</span></li>
<li><span class="math inline">\(I_p\otimes(V-B(X))\lesssim K(X)\lesssim I_p\otimes V.\)</span></li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(y=\text{vec}(Y)\)</span>. Then <span class="math display">\[
y'H(X)y=\sum_{s=1}^p\sum_{t=1}^p y_s'H_{st}(X)y_t=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\frac{\delta_{ij}}{d_{ij}^3(X)}c_{ij}^2(X,Y)\geq 0,
\]</span> and thus <span class="math inline">\(H(X)\gtrsim 0\)</span>. From () <span class="math inline">\(\nabla^2\rho(X)\gtrsim 0\)</span>, and thus <span class="math inline">\(I_p\otimes B(X)\gtrsim H(X)\)</span>. This proves the first part. The second part is immediate from the first part and ().</p>
</div>
<p>Another useful property. Let <span class="math inline">\(y=\text{vec}(Y)\)</span>.</p>
<div id="hesseigen" class="theorem">
<p>ozo</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[\begin{equation}
H(X)Y=\sum_{t=1}^p H_{st}(X)y_t=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\frac{\delta_{ij}c_{ij}(X,Y)}{d_{ij}^3(X)}(x_{is}-x_{js})(e_i-e_j).
(\#eq:eqhxy)
\end{equation}\]</span></p>
<p>If <span class="math inline">\(Y=X\)</span> then <span class="math inline">\(H(X)X=B(X)X\)</span> and thus <span class="math inline">\(\nabla^2\rho(X)X=0\)</span> and <span class="math inline">\(\nabla^2\sigma(X)X=VX\)</span>. If <span class="math inline">\(Y=XT\)</span> with <span class="math inline">\(T\)</span> anti-symmetric then <span class="math inline">\(c_{ij}(X,Y)=\text{tr}\ X'A_{ij}XT=0\)</span> and thus <span class="math inline">\(H(X)Y=0\)</span>. This implies <span class="math inline">\(\nabla^2\rho(X)Y=B(X)XT\)</span> and <span class="math inline">\(\nabla^2\sigma(X)Y=(V-B(X))XT\)</span>. If <span class="math inline">\(B(X)X=VX\)</span> then <span class="math inline">\(\nabla^2\rho(X)X=VX\)</span> and <span class="math inline">\(\nabla^2\sigma(X)X=0\)</span>. If <span class="math inline">\(Y=e\alpha'\)</span> then <span class="math inline">\(\nabla^2\rho(X)Y=\nabla^2\sigma(X)Y=0\)</span>.</p>
</div>
<p>In the example with the square the eigenvalues of <span class="math inline">\(\underline{H}(X)\)</span> are</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] +0.33333333 +0.19526215 +0.19526215 +0.19526215 +0.13807119 +0.00000000
[7] +0.00000000 -0.00000000</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] +0.52960443 +0.44261066 +0.27431040 +0.15854524 +0.07386436 +0.00000000
[7] -0.00000000 -0.00000000</code></pre>
</div>
</div>
<p>while those of <span class="math inline">\(\underline{\nabla}^2\sigma(X)\)</span> are</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] +0.33333333 +0.19526215 +0.13807119 +0.13807119 +0.13807119 +0.00000000
[7] +0.00000000 -0.00000000</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] +0.33333333 +0.25946897 +0.17478810 +0.05902293 +0.00000000 +0.00000000
[7] -0.10927733 -0.19627110</code></pre>
</div>
</div>
<p>We can derive nicer expressions for the higher derivatives in coefficient space (see section @ref(propcoefspace)). This was done, perhaps for the first time, in <span class="citation" data-cites="deleeuw_R_93c">De Leeuw (<a href="references.html#ref-deleeuw_R_93c" role="doc-biblioref">1993</a>)</span>, which was actually written around 1985. In <span class="citation" data-cites="kearsley_tapia_trosset_95">(<a href="references.html#ref-kearsley_tapia_trosset_95" role="doc-biblioref"><strong>kearsley_tapia_trosset_95?</strong></a>)</span> Newton’s method was used to minimize stress, so presumably they implemented some formula for the Hessian. In <span class="citation" data-cites="deleeuw_A_88b">De Leeuw (<a href="references.html#ref-deleeuw_A_88b" role="doc-biblioref">1988</a>)</span> the expression involving <span class="math inline">\(H(X)\)</span> from @ref(eq:defhmat) was first given in configuration space. We could use similar computations to obtain the third-order partial derivatives, but for now we have no need for them in this book.</p>
</section>
<section id="propspecexp" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="propspecexp"><span class="header-section-number">2.3.2</span> Special Expansions</h3>
<p><span class="math inline">\(c_{ij}(X,Y)=0\)</span> for all <span class="math inline">\(i&lt;j\)</span></p>
<section id="infinitesimal-rotations" class="level4" data-number="2.3.2.1">
<h4 data-number="2.3.2.1" class="anchored" data-anchor-id="infinitesimal-rotations"><span class="header-section-number">2.3.2.1</span> Infinitesimal Rotations</h4>
<p>Suppose <span class="math inline">\(Y=XT\)</span>, with <span class="math inline">\(T\)</span> antisymmetric, so that <span class="math inline">\(X+\epsilon Y=X(I+\epsilon T)\)</span>. Then <span class="math inline">\(c_{ij}(X,Y)=0\)</span> for all <span class="math inline">\(i&lt;j\)</span>, and thus from equations @ref(eq:stressders1), @ref(eq:stressders2), and @ref(eq:stressders3)</p>
<p><span class="math display">\[\begin{align}
d_+\sigma(X;Y)&amp;=0,(\#eq:stressdersa1)\\
d_+^{(2)}\sigma(X;Y,Y)&amp;=\text{tr}\ Y'(V-B(X))Y,(\#eq:stressdersa2)\\
d_+^{(3)}\sigma(X;Y,Y,Y)&amp;=0.(\#eq:stressdersa3)
\end{align}\]</span></p>
</section>
<section id="singularities" class="level4" data-number="2.3.2.2">
<h4 data-number="2.3.2.2" class="anchored" data-anchor-id="singularities"><span class="header-section-number">2.3.2.2</span> Singularities</h4>
<p>Suppose <span class="math inline">\(X=[\underline{X}\mid 0]\)</span> and <span class="math inline">\(Y=[0\mid\underline{Y}]\)</span> so that <span class="math inline">\(X+\epsilon Y=[\underline{X}\mid\epsilon\underline{Y}]\)</span>. Here <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <span class="math inline">\(n\times p\)</span>, <span class="math inline">\(underline{X}\)</span> is <span class="math inline">\(n\times r\)</span>, with <span class="math inline">\(r&lt;p\)</span>, and <span class="math inline">\(\underline{Y}\)</span> is <span class="math inline">\(n\times(p-r)\)</span>. Then again <span class="math inline">\(c_{ij}(X,Y)=0\)</span> for all <span class="math inline">\(i&lt;j\)</span>, and thus ()()() again.</p>
<p><span class="math display">\[\begin{equation}
\sigma(\underline{X}+\epsilon
\underline{Y})=\sigma(X)-\epsilon\sum_{d_{ij}(X)=0}w_{ij}\delta_{ij}d_{ij}(Y)
+\frac12\epsilon^2\text{tr}\ Y'(V-B(X))Y+o(\epsilon^2)
(\#eq:exzeroes)
\end{equation}\]</span></p>
</section>
<section id="singularities-1" class="level4" data-number="2.3.2.3">
<h4 data-number="2.3.2.3" class="anchored" data-anchor-id="singularities-1"><span class="header-section-number">2.3.2.3</span> Singularities</h4>
<p>Suppose <span class="math inline">\(\underline{X}=[X\mid 0]\)</span> and <span class="math inline">\(\underline{Y}=[Z\mid Y]\)</span> so that <span class="math inline">\(\underline{X}+\epsilon\underline{Y}=[X+\epsilon Z\mid\epsilon Y]\)</span>. Here <span class="math inline">\(\underline{X}\)</span> and <span class="math inline">\(\underline{Y}\)</span> are <span class="math inline">\(n\times p\)</span>, <span class="math inline">\(X\)</span> is <span class="math inline">\(n\times r\)</span>, with <span class="math inline">\(r&lt;p\)</span>, and <span class="math inline">\(Y\)</span> is <span class="math inline">\(n\times(p-r)\)</span>. Then</p>
<p><span class="math display">\[\begin{equation}
\sigma(\underline{X}+\epsilon
\underline{Y})=\sigma(X)-\epsilon\sum_{d_{ij}(X)=0}w_{ij}\delta_{ij}d_{ij}(Y)
+\frac12\epsilon^2\text{tr}\ Y'(V-B(X))Y+o(\epsilon^2)
(\#eq:exsingular)
\end{equation}\]</span></p>
</section>
</section>
</section>
<section id="propconvex" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="propconvex"><span class="header-section-number">2.4</span> Convexity</h2>
<p>Remember that a function <span class="math inline">\(f\)</span> on an open subset <span class="math inline">\(X\)</span> of a Euclidean space is <em>convex</em> if for all <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> in <span class="math inline">\(X\)</span> and <span class="math inline">\(0\leq\alpha\leq 1\)</span> we have <span class="math inline">\(f(\alpha x+(1-\alpha)y)\leq\alpha f(x)+(1-\alpha)f(y)\)</span>. Thus on the line segment connecting <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> the function <span class="math inline">\(f\)</span> is never above the line segment connecting <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(f(y)\)</span>. Convex functions are a.e. differentiable, in fact a.e. twice-differentiable. If the derivative exists at <span class="math inline">\(x\)</span> then for all <span class="math inline">\(y\)</span> we have <span class="math inline">\(f(y)\geq f(x)+df(x)(y-x)\)</span>, which says the function majorizes its tangent plane at <span class="math inline">\(x\)</span>. If the second derivative exists at <span class="math inline">\(x\)</span> then <span class="math inline">\(d^2f(x;y,y)\geq 0\)</span> for all <span class="math inline">\(y\)</span>, which says that the Hessian at <span class="math inline">\(x\)</span> is positive semidefinite.</p>
<p>Stress is definitely not a convex function of the configuration. If it actually was convex, or even convex and differentiable, then this book would be much shorter. Nevertheless convexity still play an important part in our development of MDS, ever since <span class="citation" data-cites="deleeuw_C_77">De Leeuw (<a href="references.html#ref-deleeuw_C_77" role="doc-biblioref">1977</a>)</span>.</p>
<section id="distances-1" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="distances-1"><span class="header-section-number">2.4.1</span> Distances</h3>
<p>The convexity in the MDS problem comes from the convexity of the distance and the squared distance. Although these are elementary facts, they are important in our context, so we give a proof.</p>
<div class="theorem">
<p>On <span class="math inline">\(\mathbb{R}^{n\times p}\)</span> both <span class="math inline">\(d{ij}\)</span> and <span class="math inline">\(d_{ij}^2\)</span> are convex.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>First, for <span class="math inline">\(0\leq\lambda\leq 1\)</span>, <span class="math display">\[\begin{equation}
d_{ij}^2(\lambda X+(1-\lambda)Y)
=\lambda^2d_{ij}^2(X) + (1-\lambda)^2d_{ij}^2(Y)+2\lambda(1-\lambda)(x_i-x_j)'(y_i-y_j)
(\#eq:lbdcv1)
\end{equation}\]</span> By corollary @ref(cor:amgmcs), <span class="math display">\[\begin{equation}
(x_i-x_j)'(y_i-y_j)\leq\sqrt{d_{ij}^2(X)d_{ij}^2(Y)}
\leq\frac12(d_{ij}^2(X)+d_{ij}^2(Y)).
(\#eq:lbdcv2)
\end{equation}\]</span> Combining @ref(eq:lbdcv1) and @ref(eq:lbdcv2) proves convexity of the squared distance.</p>
<p>Now use equation @ref(eq:lbdcv1) and the CS inequality in the form <span class="math inline">\((x_i-x_j)'(y_i-y_j)\leq d_{ij}(X)d_{ij}(Y)\)</span>. This gives <span class="math display">\[\begin{equation}
d_{ij}^2(\lambda X +(1-\lambda)Y)\leq (\lambda d_{ij}(X)+(1-\lambda)d_{ij}(Y))^2.
(\#eq:lbdcv3)
\end{equation}\]</span> Taking square roots on both sides of equation @ref(eq:lbdcv3) proves convexity of the distance.</p>
</div>
<div class="corrollary">
<p>Both <span class="math inline">\(\rho\)</span> and <span class="math inline">\(\eta\)</span> are norms on <span class="math inline">\(\mathbb{R}^{n\times p}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>homogeneous convex functions vanishing if and only if <span class="math inline">\(X=0\)</span>, which means they are both norms on <span class="math inline">\(\mathbb{R}^{n\times p}\)</span>.</p>
</div>
</section>
<section id="subdifdef" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="subdifdef"><span class="header-section-number">2.4.2</span> Subdifferentials</h3>
<p>Suppose <span class="math inline">\(f\)</span> is a real-valued finite convex function on the finite-dimensional inner-product space <span class="math inline">\(\mathcal{E}\)</span>. A <em>subgradient</em> of <span class="math inline">\(f\)</span> at <span class="math inline">\(x\in\mathcal{E}\)</span> is a vector <span class="math inline">\(z\in\mathcal{E}\)</span> such that <span class="math inline">\(f(y)\geq f(x)+\langle z,y-x\rangle\)</span> for all <span class="math inline">\(y\in\mathcal{E}\)</span>. The set of all subgradients at <span class="math inline">\(x\)</span> is the <em>subdifferential</em> at <span class="math inline">\(x\)</span>, written as <span class="math inline">\(\partial f(x)\)</span>. In general, the subdifferential is a non-empty, closed, and convex set (rock). If <span class="math inline">\(f\)</span> is differentiable at <span class="math inline">\(x\)</span> then the subdifferential is the singleton which has the gradient <span class="math inline">\(\nabla f(x)\)</span> as its sole element (rock).</p>
<p>Apply this to <span class="math inline">\(d_{ij}\)</span> on <span class="math inline">\(\mathbb{R}^{n\times p}\)</span>.</p>
<div id="subdifd" class="theorem">
<p>The subdifferential of <span class="math inline">\(d_{ij}\)</span> at <span class="math inline">\(X\)</span> is</p>
<ul>
<li><p>If <span class="math inline">\(d_{ij}(X)&gt;0\)</span> then <span class="math inline">\(\partial d_{ij}(X)=\left\{(e_i-e_j)\frac{(x_i-x_j)'}{d_{ij}(X)}\right\}\)</span></p></li>
<li><p>If <span class="math inline">\(d_{ij}(X)=0\)</span> then <span class="math inline">\(\partial d_{ij}(X)=\{Y\mid Y=(e_i-e_j)z' \text{ with } \|z\|\leq 1\}\)</span> <span class="math display">\[\partial d_{ij}(X)=\bigcup_{\|z\|\leq 1}\{(e_i-e_j)z'\}\]</span></p></li>
</ul>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If <span class="math inline">\(d_{ij}(X)=0\)</span> we must find the set of all <span class="math inline">\(Z\in\mathbb{R}^{n\times p}\)</span> such that <span class="math display">\[\begin{equation}
d_{ij}(Y)\geq\text{tr}\ Z'(Y-X)
(\#eq:subdifdefd)
\end{equation}\]</span> for all <span class="math inline">\(Y\in\mathbb{R}^{n\times p}\)</span>.</p>
<p>First of all @ref(eq:subdifdefd) must be true for all <span class="math inline">\(Y=\alpha X\)</span> with <span class="math inline">\(\alpha\geq 0\)</span>. Thus <span class="math inline">\((\alpha-1)\text{tr}\ Z'X\leq 0\)</span> for all <span class="math inline">\(\alpha\geq 0\)</span>, which implies <span class="math inline">\(\text{tr}\ Z'X=0\)</span>. We can use this to simplify @ref(eq:subdifdefd) to <span class="math inline">\(d_{ij}(Y)\geq\text{tr}\ Z'Y\)</span> for all <span class="math inline">\(Y\)</span>. Next, it follows that <span class="math inline">\(z_k=0\)</span> for <span class="math inline">\(k\not= i,j\)</span>. If <span class="math inline">\(z_k\not= 0\)</span> choose <span class="math inline">\(Y=e_kz_k'\)</span>. Then <span class="math inline">\(d_{ij}(Y)=0\)</span> and <span class="math inline">\(\text{tr} Z'Y=z_k'z_k&gt;0\)</span>. Now @ref(eq:subdifdefd) simplifies to <span class="math inline">\(d_{ij}(Y)\geq z_i'y_i+z_j'y_j\)</span> If <span class="math inline">\(y_i=z_i\)</span> and <span class="math inline">\(y_j=0\)</span> then we must have <span class="math inline">\(\|z_i\|\geq\|z_i\|^2\)</span> or <span class="math inline">\(\|z_i\|\leq 1\)</span>. In the same way <span class="math inline">\(\|z_j\|\leq 1\)</span>. Choose <span class="math inline">\(y_i=y_j=y\)</span> and some <span class="math inline">\(y\not= 0\)</span>. Then we must have <span class="math inline">\((z_i+z_j)'y\leq 0\)</span> for all <span class="math inline">\(y\)</span> and thus <span class="math inline">\(z_i=-z_j\)</span>. This proves the second part.</p>
</div>
<p>By the sum rule for convex subdifferentials (rock) <span class="math display">\[\begin{equation}
\partial\rho(X)=B(X)X+
\{Y\mid Y=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}\{w_{ij}\delta_{ij}(e_i-e_j)z_{ij}'\mid d_{ij}(X)=0\}\},
(\#eq:propsubdiffrho)
\end{equation}\]</span> where the <span class="math inline">\(z_{ij}\)</span> are arbitrary vectors satisfying <span class="math inline">\(\|z_{ij}\|\leq 1\|\)</span>.</p>
<p>Of course <span class="math inline">\(\partial d_{ij}^2(X)=\{2A_{ij}X\}\)</span> and thus <span class="math inline">\(\partial\eta^2(X)=\{2VX\}\)</span>.</p>
<p>But <span class="math inline">\(\sigma\)</span> is not convex, and we do not have a definition yet for the subdifferential of non-convex functions. We use the generalization introduced by Clarke. Suppose <span class="math inline">\(f\)</span> is locally Lipschitz, and thus differentiable almost everywhere. Let <span class="math inline">\(x^{(k)}\)</span> be a sequence of points converging to <span class="math inline">\(x_\infty\)</span>, with<br>
<span class="math inline">\(f\)</span> differentiable at all <span class="math inline">\(x^{(k)}\)</span>.</p>
<p>Then <span class="math inline">\(y\)</span> is in the Clarke subdifferential <span class="math inline">\(\partial_C^{\ }f(x)\)</span> if and only if <span class="math inline">\(y=\lim_{k\rightarrow\infty}\nabla f(x^{(k)})\)</span>.</p>
<p>Clarke <span class="math inline">\(\partial_C^{\ }\sigma(X)=\{VX\}-\partial\rho(X)\)</span></p>
<p>Combining this with #ref(eq:propsubdiffrho) gives</p>
<p><span class="math display">\[\begin{equation}
\partial\sigma(X)=(V-B(X))X-
\{Y\mid Y=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}\{w_{ij}\delta_{ij}(e_i-e_j)z_{ij}'\mid d_{ij}(X)=0\}\},
(\#eq:propsubdiffstress)
\end{equation}\]</span></p>
</section>
<section id="propdc" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="propdc"><span class="header-section-number">2.4.3</span> DC Functions</h3>
<p>In basic MDS</p>
<ol type="1">
<li><span class="math inline">\(\rho\)</span> is a non-negative convex function, homogeneous of degree one.</li>
<li><span class="math inline">\(\eta^2\)</span> is a non-negative convex quadratic form, homogeneous of degree two.</li>
<li><span class="math inline">\(\sigma\)</span> is a non-negative difference of two convex functions.</li>
</ol>
<p>This follows because <span class="math inline">\(\eta^2\)</span> is a weighted sum of squared distances and <span class="math inline">\(\rho\)</span> is a weighted sum of distances, both with non-negative coefficients, and thus they are both convex.</p>
<p>Real-valued functions that are differences of two convex functions are also known as a <em>DC functions</em> or <em>delta-convex functions</em>. DC functions are important in optimization, especially in non-convex and global optimization. For excellent reviews of the various properties of DC functions, see <span class="citation" data-cites="hiriart-urruty_88">Hiriart-Urruty (<a href="references.html#ref-hiriart-urruty_88" role="doc-biblioref">1988</a>)</span> or <span class="citation" data-cites="bacak_borwein_11">Bacak and Borwein (<a href="references.html#ref-bacak_borwein_11" role="doc-biblioref">2011</a>)</span>. Interesting for our purposes is that DC functions are almost everywhere twice differentiable, and that all two times continuously differentiable functions are DC.</p>
<p>It follows from the general properties of convex and DC functions that <span class="math inline">\(\sigma\)</span> is both uniformly continuous and locally Lipschitz, in fact Lipschitz on each compact subset of <span class="math inline">\(\mathbb{R}^{n\times p}\)</span> (<span class="citation" data-cites="rockafellar_70">Rockafellar (<a href="references.html#ref-rockafellar_70" role="doc-biblioref">1970</a>)</span>, theorem 10.4). The fact that <span class="math inline">\(\sigma\)</span> is only locally Lipshitz is due entirely to the quadratic part <span class="math inline">\(\eta^2\)</span>, because <span class="math inline">\(\rho\)</span> is globally Lipschitz.</p>
</section>
<section id="propnegdis" class="level3" data-number="2.4.4">
<h3 data-number="2.4.4" class="anchored" data-anchor-id="propnegdis"><span class="header-section-number">2.4.4</span> Negative Dissimilarities</h3>
<p>There are perverse situations in which some weights and/or dissimilarities are negative (<span class="citation" data-cites="heiser_91">Heiser (<a href="references.html#ref-heiser_91" role="doc-biblioref">1991</a>)</span>). Define <span class="math inline">\(w_{ij}^+:=\max(w_{ij},0)\)</span> and <span class="math inline">\(w_{ij}^-:=-\min(w_{ij},0)\)</span>. Thus both <span class="math inline">\(w_{ij}^+\)</span> and <span class="math inline">\(w_{ij}^-\)</span> are non-negative, and <span class="math inline">\(w_{ij}=w_{ij}^+-w_{ij}^-\)</span>. Make the same decomposition of the <span class="math inline">\(\delta_{ij}\)</span>.</p>
<p>Then <span class="math display">\[\begin{equation}
\rho(X)=\sum (w_{ij}^+\delta_{ij}^++w_{ij}^-\delta_{ij}^-)d_{ij}(X)-\sum(w_{ij}^+\delta_{ij}^-+w_{ij}^-\delta_{ij}^+)d_{ij}(X),
(\#eq:proprhoneg)
\end{equation}\]</span> and <span class="math display">\[\begin{equation}
\eta^2(X)=\sum w_{ij}^+d_{ij}^2(X)-\sum w_{ij}^-d_{ij}^2(X).
(\#eq:propetaneg)
\end{equation}\]</span> Note that both <span class="math inline">\(\rho\)</span> and <span class="math inline">\(\eta^2\)</span> are no longer convex, but both are DC, and consequently so is <span class="math inline">\(\sigma\)</span>.</p>
<p>A bit more</p>
</section>
</section>
<section id="propstationary" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="propstationary"><span class="header-section-number">2.5</span> Stationary Points</h2>
<ul>
<li><p>A function <span class="math inline">\(f\)</span> has a <em>global minimum</em> on <span class="math inline">\(X\)</span> at <span class="math inline">\(\hat x\)</span> if <span class="math inline">\(f(\hat x)\leq f(x)\)</span> for all <span class="math inline">\(x\in X\)</span>.</p></li>
<li><p>A function <span class="math inline">\(f\)</span> has a <em>local minimum</em> on <span class="math inline">\(X\)</span> at <span class="math inline">\(\hat x\)</span> if there is a neighborhood <span class="math inline">\(\mathcal{N}\)</span> of <span class="math inline">\(\hat x\)</span> such that <span class="math inline">\(f(\hat x)\leq f(x)\)</span> for all <span class="math inline">\(x\in\mathcal{N}\cap X\)</span>.</p></li>
<li><p>A function <span class="math inline">\(f\)</span> has a <em>singular point</em> at <span class="math inline">\(x\)</span> if it is not differentiable at <span class="math inline">\(x\)</span>.</p></li>
<li><p>A function <span class="math inline">\(f\)</span> has a <em>stationary point</em> at <span class="math inline">\(x\)</span> if it is differentiable at <span class="math inline">\(x\)</span> and <span class="math inline">\(df(x)=0\)</span>.</p></li>
<li><p>A function <span class="math inline">\(f\)</span> has a <em>saddle point</em> at a stationary point <span class="math inline">\(x\)</span> if it is neither a local maximum nor a local minimum.</p></li>
<li><p>Global and local maxima of <span class="math inline">\(f\)</span> are global and local minima of <span class="math inline">\(-f\)</span>.</p></li>
</ul>
<div id="statpointsbound" class="theorem">
<p>At a stationary point of stress we have <span class="math inline">\(\eta(X)\leq 1\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If <span class="math inline">\(X\)</span> is stationary we have <span class="math inline">\(VX=B(X)X\)</span> and thus <span class="math inline">\(\rho(X)=\eta^2(X)\)</span>. Consequently <span class="math inline">\(\sigma(X)=1-2\rho(X)+\eta^2(X)=1-\eta^2(X)\)</span> and because <span class="math inline">\(\sigma(X)\geq 0\)</span> we see that <span class="math inline">\(X\)</span> must be in the ellipse <span class="math inline">\(\{Z\in\mathbb{R}^{n\times p}\mid\eta^2(Z)\leq 1\}\)</span>.</p>
</div>
<p>Theorem @ref(thm:statpointsbound) is important, because it means that we can require without loss of generality that <span class="math inline">\(X\)</span> is in the ellipsoidal disk <span class="math inline">\(\eta(X)\leq 1\)</span>, which is a compact convex set.</p>
<section id="local-maxima" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="local-maxima"><span class="header-section-number">2.5.1</span> Local Maxima</h3>
<div id="locmax" class="theorem">
<p>stress has a single local maximum at <span class="math inline">\(X=0\)</span> with value 1.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>At <span class="math inline">\(X=0\)</span> we have for the one-sided directional derivative <span class="math display">\[\begin{equation}
\mathbb{D}_+\sigma(0,Y)=\lim_{\alpha\downarrow 0}\frac{\sigma(0+\alpha Y)-\sigma(0)}{\alpha}=-2\rho(Y)\leq 0,
(\#eq:datzero)
\end{equation}\]</span> which implies that stress has a local maximum at zero.</p>
<p>To show that the local maximum is unique suppose that there is a local maximum at <span class="math inline">\(X\not= 0\)</span>. Then on the line through zero and <span class="math inline">\(X\)</span> there should be a local maximum at <span class="math inline">\(X\)</span> as well. But<br>
<span class="math display">\[\begin{equation}
\sigma(\alpha X)=1-2\alpha\rho(X)+\alpha^2\eta^2(X),
(\#eq:propnomax)
\end{equation}\]</span> is a convex quadratic, which consequently cannot have a local maximum at <span class="math inline">\(X\)</span>.</p>
</div>
</section>
<section id="proplocmin" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="proplocmin"><span class="header-section-number">2.5.2</span> Local Minima</h3>
<p>The main result on local minima of stress is due to <span class="citation" data-cites="deleeuw_A_84f">De Leeuw (<a href="references.html#ref-deleeuw_A_84f" role="doc-biblioref">1984</a>)</span>. We give a slight strengthening of the result, along the lines of <span class="citation" data-cites="deleeuw_E_18c">De Leeuw (<a href="references.html#ref-deleeuw_E_18c" role="doc-biblioref">2018</a>)</span>, with a slightly simplified proof. Theorem @ref(thm:locmin) proves that a necessary condition for a local minimum at <span class="math inline">\(X\)</span> is that for <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> with <span class="math inline">\(w_{ij}\delta_{ij}&gt;0\)</span> we have <span class="math inline">\(d_{ij}(X)&gt;0\)</span>, i.e. objects <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are mapped to different points.</p>
<div id="locmin" class="theorem">
<p>If stress has a local minimum at <span class="math inline">\(X\)</span> then * <span class="math inline">\(B(X)X=VX\)</span>. * <span class="math inline">\(d_{ij}(X)&gt;0\)</span> whenever <span class="math inline">\(w_{ij}\delta_{ij}&gt;0\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If stress has a local minimum at <span class="math inline">\(X\)</span> then <span class="math inline">\(d_+\sigma(X;Y)\geq 0\)</span> for all <span class="math inline">\(Y\)</span>. Equation @ref(eq:stressders1) tell us that <span class="math display">\[\begin{equation}
d_+\sigma(X;Y)=\text{tr}\ Y'(V-B(X))X-\mathop{\sum\sum}_{1\
\leq i&lt;j\leq n}\{w_{ij}\delta_{ij}d_{ij}(Y)\mid d_{ij}(X)=0\text{ and } w_{ij}\delta_{ij}&gt;0\}.
(\#eq:dirderagain)
\end{equation}\]</span>ain) \end{equation} Consider a direction <span class="math inline">\(Y\)</span> with all <span class="math inline">\(d_{ij}(Y)&gt;0\)</span> and such that then <span class="math inline">\(d_+\sigma(X;Y)\leq 0\)</span>. This is always possible, because if we have and <span class="math inline">\(Y\)</span> with <span class="math inline">\(d_+\sigma(X;Y)&gt;0\)</span> we simply switch to <span class="math inline">\(-Y\)</span>. Now <span class="math inline">\(d_+\sigma(X;Y)\)</span> in @ref(eq:dirderagain) is the sum of two terms which are both non-positive, and they satisfy <span class="math inline">\(d_+\sigma(X;Y)\geq 0\)</span> if and only if they are both zero. For the second term this means that at a local minimum the summation is empty and there is no <span class="math inline">\(d_{ij}(X)=0\)</span> whenever <span class="math inline">\(w_{ij}\delta_{ij}=0\)</span>. For the first term it means that <span class="math inline">\((V-B(X))X=0\)</span>.</p>
</div>
<p><span class="citation" data-cites="deleeuw_A_84f">De Leeuw (<a href="references.html#ref-deleeuw_A_84f" role="doc-biblioref">1984</a>)</span> concluded that if <span class="math inline">\(w_{ij}\delta_{ij}&gt;0\)</span> for all <span class="math inline">\(i&lt;j\)</span> then stress is differentiable at a local minimum. But more is true, because it s not necessary to require <span class="math inline">\(w_{ij}\delta_{ij}&gt;0\)</span> for this result.</p>
<div id="locmindif" class="corollary">
<p>If stress has a local minimum at <span class="math inline">\(X\)</span> then it is differentiable at <span class="math inline">\(X\)</span> and has <span class="math inline">\(\nabla\sigma(X)=0\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>At a local minimum we can indeed have <span class="math inline">\(d_{ij}(X)=0\)</span> if <span class="math inline">\(w_{ij}\delta_{ij}=0\)</span>. But if <span class="math inline">\(w_{ij}=0\)</span> then stress does not depend on <span class="math inline">\(d_{ij}(X)\)</span> at all, so <span class="math inline">\(d_{ij}(X)=0\)</span> does not influence differentiability. If <span class="math inline">\(w_{ij}&gt;0\)</span> and <span class="math inline">\(\delta_{ij}=0\)</span> then stress depends on <span class="math inline">\(d_{ij}(X)\)</span> only through the term <span class="math inline">\(w_{ij}d_{ij}^2(X)\)</span>, which is differentiable even if <span class="math inline">\(d_{ij}(X)=0\)</span>.</p>
</div>
<p>Theorem @ref(thm:locmin) and its corollary @ref(cor:locmindif) are the main reason why, at least in basic scaling, we can largely ignore the problems with differentiability. These results have been extended to least squares MDS with Minkovski distances by <span class="citation" data-cites="groenen_mathar_heiser_95">Groenen, Mathar, and Heiser (<a href="references.html#ref-groenen_mathar_heiser_95" role="doc-biblioref">1995</a>)</span>. In a neighborhood of each local minimum the loss function is differentiable, so eventually convergent descent algorithms do not have problems with non-differentiable ridges. This result is of major importance for both practical and theoretical reasons, as emphasized for example by <span class="citation" data-cites="pliner_96">Pliner (<a href="references.html#ref-pliner_96" role="doc-biblioref">1996</a>)</span>.</p>
<p>Second order necessary conditions (since differentiable at local minimum)</p>
</section>
<section id="propsaddle" class="level3" data-number="2.5.3">
<h3 data-number="2.5.3" class="anchored" data-anchor-id="propsaddle"><span class="header-section-number">2.5.3</span> Saddle Points</h3>
<p>At a saddle point <span class="math inline">\(X\)</span> stress is differentiable and <span class="math inline">\(d\sigma(X)=0\)</span>. But there are directions of decrease and increase from <span class="math inline">\(X\)</span>, and thus stress does not have a local minimum there.</p>
<p>If <span class="math inline">\(VX=B(X)X\)</span> and <span class="math inline">\(d_{ij}(X)=0\)</span> for some <span class="math inline">\(w_{ij}\delta_{ij}&gt;0\)</span> then there is an <span class="math inline">\(Y\)</span> such that <span class="math inline">\(\mathbb{D}_+\sigma(X,Y)&lt;0\)</span>.</p>
<p>Theo Suppose <span class="math inline">\(VX=B(X)X\)</span> then <span class="math inline">\(V(X\mid 0)=B(X|0)(X|0)\)</span></p>
<p>Corr Suppose <span class="math inline">\(VX=B(X)X\)</span> and <span class="math inline">\(X\)</span> is of rank <span class="math inline">\(r&lt;p\)</span>. Then <span class="math inline">\(XL=(Z|0)\)</span> and thus <span class="math inline">\(VZ=B(Z)Z\)</span>.</p>
<p>If <span class="math inline">\(VX=B(X)X\)</span> and <span class="math inline">\(X\)</span> is singular then <span class="math inline">\(X\)</span> is a saddle point.</p>
</section>
<section id="an-example" class="level3" data-number="2.5.4">
<h3 data-number="2.5.4" class="anchored" data-anchor-id="an-example"><span class="header-section-number">2.5.4</span> An Example</h3>
<p>Let’s look at a small example, already analyzed in many different places (e.g. <span class="citation" data-cites="deleeuw_A_88b">De Leeuw (<a href="references.html#ref-deleeuw_A_88b" role="doc-biblioref">1988</a>)</span>, <span class="citation" data-cites="trosset_mathar_97">Trosset and Mathar (<a href="references.html#ref-trosset_mathar_97" role="doc-biblioref">1997</a>)</span>). It has four points, all dissimilarities to one and all weights are equal to <span class="math inline">\(\frac16\)</span>.</p>
<section id="regular-tetrahedron" class="level4" data-number="2.5.4.1">
<h4 data-number="2.5.4.1" class="anchored" data-anchor-id="regular-tetrahedron"><span class="header-section-number">2.5.4.1</span> Regular Tetrahedron</h4>
<p>In three dimensions the global minimum is equal to zero, with the points mapped into the vertices of a regular tetrahedron. Points can be assigned to the vertices in <span class="math inline">\(4! = 24\)</span> ways, and each such assignment defines a global minimum. In fact each assigment defines a continuum of global minimizers, because all rotations of any of the regular tetrahedra also give global minimizers. It seems as if there are 24 rotation manifolds with global minimizers. But some of the 24 assigments of points to vertices are equivalent in the sense that they are rotations of each other (which includes reflections). It turns out there are three equivalence classes of eight assignments each. Thus there are three different disjoint rotation manifolds of global minimizers, not 24.</p>
<p>The stress value for any regular tetrahedron is zero, and the eigenvalues of the Hessian are</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code> [1] +0.333333 +0.166667 +0.166667 +0.166667 +0.083333 +0.083333 +0.000000
 [8] +0.000000 +0.000000 +0.000000 -0.000000 -0.000000</code></pre>
</div>
</div>
<p>For four points in three dimensions we have <span class="math inline">\(np-\frac12p(p+1)=6\)</span> non-trivial eigenvalues. Since the six largest values are all positive our regular tetrahedra are isolated, in the sense that if we move away from the each of the corresponding rotation manifolds the stress increases.</p>
</section>
<section id="singularity" class="level4" data-number="2.5.4.2">
<h4 data-number="2.5.4.2" class="anchored" data-anchor-id="singularity"><span class="header-section-number">2.5.4.2</span> Singularity</h4>
<p>The next stationary point is somewhat deviously constructed. We take four points equally spaced on a line (a stationary point in one dimension) and add two zero dimensions. Then we do a random rotation of configuration to somwhat hide its singularity. We already know this configuration defines a saddle point in three-dimensional configuration space. For the resulting configuration the stress is 0.083333, and the eigenvalues of the Hessian are</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code> [1] +0.333333 +0.333333 +0.333333 +0.000000 -0.000000 -0.000000 -0.000000
 [8] -0.000000 -0.166667 -0.166667 -0.277778 -0.277778</code></pre>
</div>
</div>
<p>There are four negative eigenvalues, and thus we confirm we have a saddlepoint.</p>
</section>
<section id="equilateral-triangle-with-centroid" class="level4" data-number="2.5.4.3">
<h4 data-number="2.5.4.3" class="anchored" data-anchor-id="equilateral-triangle-with-centroid"><span class="header-section-number">2.5.4.3</span> Equilateral Triangle with Centroid</h4>
<p>The next configuration is an interesting one. It is in two-dimensions, with three points in the corners of an equilateral triangle, and a fourth point in the centroid of the first three. The 24 assigments of the four points to the four positions in thus case give four rotational equivalence classes of six assignments each. This particular arrangements has four disjoint rotational manifolds. The stress is 0.033494, and the eigenvalues of the Hessian are</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] +0.333333 +0.255983 +0.255983 +0.000000 +0.000000 +0.000000 +0.000000
[8] -0.000000</code></pre>
</div>
</div>
<p>Note that the Hessian is positive semi-definite, with three positive eigenvalues. This made <span class="citation" data-cites="deleeuw_A_88b">De Leeuw (<a href="references.html#ref-deleeuw_A_88b" role="doc-biblioref">1988</a>)</span> think that this arrangement of points defined a non-isolated local minimum. Bad mistake. Since <span class="math inline">\(3 &lt; np - \frac12p(p+1) = 5\)</span> the singular Hessian does not guarantee that we have a local minimum. <span class="citation" data-cites="trosset_mathar_97">Trosset and Mathar (<a href="references.html#ref-trosset_mathar_97" role="doc-biblioref">1997</a>)</span> showed, using symbolic computations, that the configuration and its permutations and rotations defines a family of saddle points. Further on in this section we will look in more detail what happens in this case.</p>
</section>
<section id="square" class="level4" data-number="2.5.4.4">
<h4 data-number="2.5.4.4" class="anchored" data-anchor-id="square"><span class="header-section-number">2.5.4.4</span> Square</h4>
<p>Four points in the corners of a square give the global minimum in two dimensions (<span class="citation" data-cites="deleeuw_stoop_A_84">De Leeuw and Stoop (<a href="references.html#ref-deleeuw_stoop_A_84" role="doc-biblioref">1984</a>)</span>). There are three isolated rotational manifolds for such squares, all with stress 0.014298, and with eigenvalues of the Hessian</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] +0.333333 +0.195262 +0.138071 +0.138071 +0.138071 +0.000000 +0.000000
[8] +0.000000</code></pre>
</div>
</div>
<p>In one dimension the global minimum is attained for four points equally spaced on a line. Thus there are <span class="math inline">\(n!\)</span> different global minimizers but by reflection only <span class="math inline">\(\frac12(n!)\)</span> give different distances.</p>
</section>
<section id="non-global-local-minima" class="level4" data-number="2.5.4.5">
<h4 data-number="2.5.4.5" class="anchored" data-anchor-id="non-global-local-minima"><span class="header-section-number">2.5.4.5</span> Non-global Local Minima</h4>
<p>It turns out be be difficult in our example to find non-global local minima. In an heroic effort we looked at 100,000 smacof runs with a random start to find other local minima. In 9.9996^{4} cases smacof converges to the square, in 4 it stops at the equilateral triangle with center, but only because the limit on the number of iterations (1000) is reached. This confirms the computational results reported by <span class="citation" data-cites="deleeuw_A_88b">De Leeuw (<a href="references.html#ref-deleeuw_A_88b" role="doc-biblioref">1988</a>)</span> and <span class="citation" data-cites="trosset_mathar_97">Trosset and Mathar (<a href="references.html#ref-trosset_mathar_97" role="doc-biblioref">1997</a>)</span>. It also confirms the theoretical<br>
result that gradient descent algorithms with random starts almost surely avoid saddle points and converge to local minima (<span class="citation" data-cites="lee_simchowitz_jordan_recht_16">Lee et al. (<a href="references.html#ref-lee_simchowitz_jordan_recht_16" role="doc-biblioref">2016</a>)</span>), although avoiding the saddle points may take exponential time (<span class="citation" data-cites="du_jin_lee_jordan_poczos_singh_17">Du et al. (<a href="references.html#ref-du_jin_lee_jordan_poczos_singh_17" role="doc-biblioref">2017</a>)</span>). In any case, it seems safe to conjecture that for our small and maximally symmetric example all local minima are global.</p>
<p><span class="citation" data-cites="trosset_mathar_97">Trosset and Mathar (<a href="references.html#ref-trosset_mathar_97" role="doc-biblioref">1997</a>)</span>, in their search for non-global local minima, consequently are forced to use another example. They used equal weights, but choose the dissimilarities as the Euclidean distances between the four corners of a square, ordered counterclockwise from the origin. Thus the global minimum of stress is zero. In 1000 smacof runs with random start we find a this zero local minimum 599 times, while we converge 401 times to another stationary point with stress 0.0334936.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="properties_files/figure-html/tmzplot-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Trosset/Mathar Configurations</figcaption>
</figure>
</div>
</div>
</div>
<p>The two configurations are plotted in figure @ref(fig:tmzplot), with the global minimizer in red. The non-global configuration (in blue) is rotated to best least squares fit with the first one, using simple Procrustus (<span class="citation" data-cites="gower_dijksterhuis_04">Gower and Dijksterhuis (<a href="references.html#ref-gower_dijksterhuis_04" role="doc-biblioref">2004</a>)</span>). Note that it is a rectangle, but not a square. The eigenvalues of the Hessian at the non-global minimum configuration are</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] +0.333333 +0.211325 +0.122008 +0.122008 +0.122008 +0.000000 +0.000000
[8] +0.000000</code></pre>
</div>
</div>
<p>verifying that we indeed have an isolated local minimum. <span class="citation" data-cites="trosset_mathar_97">Trosset and Mathar (<a href="references.html#ref-trosset_mathar_97" role="doc-biblioref">1997</a>)</span> verify this using a mix of symbolic and floating point calculation.</p>
<p>We can generate an additional example using the function equalDelta() in equaldelta.R. Its arguments are <span class="math inline">\(n, p, m\)</span>, where <span class="math inline">\(n\)</span> is the order of the dissimilarity and weight matrices, which have all their non-diagonal elements equal. Argument <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> define the space of configuration matrices, and <span class="math inline">\(m\)</span> is the number of smacof runs with a random start.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] 658</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 342</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>itel      1  eiff    0.0000000000  sold    0.0381249159  snew    0.0381249159  
itel      2  eiff    0.0000000000  sold    0.0381249159  snew    0.0381249159  
itel      3  eiff    0.0000000000  sold    0.0381249159  snew    0.0381249159  
itel      4  eiff    0.0000000000  sold    0.0381249159  snew    0.0381249159  </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>itel      1  eiff    0.0000000000  sold    0.0357265590  snew    0.0357265590  
itel      2  eiff    0.0000000000  sold    0.0357265590  snew    0.0357265590  </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] +0.200000 +0.129354 +0.129354 +0.102242 +0.102242 +0.079940 +0.079940
 [8] +0.005686 +0.005686 -0.000000 -0.000000 -0.000000</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] +0.200000 +0.114739 +0.114739 +0.107180 +0.073205 +0.073205 +0.051287
 [8] +0.051287 +0.039230 +0.000000 +0.000000 -0.000000</code></pre>
</div>
</div>
</section>
<section id="directions-of-descent" class="level4" data-number="2.5.4.6">
<h4 data-number="2.5.4.6" class="anchored" data-anchor-id="directions-of-descent"><span class="header-section-number">2.5.4.6</span> Directions of Descent</h4>
<p>We now go back to the stationary equilateral triangle with center. We have seen that the gradient at this configuration is zero and the Hessian is positive semi-definite but rank-deficient. A <em>descent direction</em> at <span class="math inline">\(X\)</span> is any configuration <span class="math inline">\(Y\)</span> such that <span class="math inline">\(\sigma(X+\epsilon Y)&lt;\sigma(X)\)</span> if <span class="math inline">\(\epsilon\)</span> is small enough. In our example, with <span class="math inline">\(X\)</span> the triangle with center, we must choose <span class="math inline">\(Y\)</span> in the null space of the Hessian, because otherwise <span class="math inline">\(Y\)</span> is a direction of accent. The null space has two trivial dimensions, <span class="math inline">\(X\)</span> and <span class="math inline">\(XA\)</span> with <span class="math inline">\(A\)</span> anti-symmetric. The non-trivial null space has dimension three, and we choose a basis of three orthonormal directions. Then</p>
<p><span class="math display">\[
\sigma(X+\epsilon Y)=\sigma(X)+0+0+\frac16\epsilon^3d^3\sigma(X)(Y,Y,Y)+o(\epsilon^3),
\]</span> and we can find a descent direction if <span class="math inline">\(d^3\sigma(X)(Y,Y,Y)\not= 0\)</span>.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] +0.066987 -0.000000 +0.000000 -0.186030</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] +0.066987 +0.000000 +0.000000 +0.293283</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] +0.066987 -0.000000 +0.000000 -0.117607</code></pre>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-bacak_borwein_11" class="csl-entry" role="listitem">
Bacak, M., and J. M. Borwein. 2011. <span>“<span class="nocase">On Difference Convexity of Locally Lipschitz Functions</span>.”</span> <em>Optimization</em> 60 (8-9): 961–78.
</div>
<div id="ref-borg_groenen_05" class="csl-entry" role="listitem">
Borg, I., and P. J. F. Groenen. 2005. <em>Modern Multidimensional Scaling</em>. Second Edition. Springer.
</div>
<div id="ref-borg_leutner_83" class="csl-entry" role="listitem">
Borg, I., and D. Leutner. 1983. <span>“<span class="nocase">Dimensional Models for the Perception of Rectangles</span>.”</span> <em>Perception and Psychophysics</em> 34: 257–69.
</div>
<div id="ref-deleeuw_C_77" class="csl-entry" role="listitem">
De Leeuw, J. 1977. <span>“Applications of Convex Analysis to Multidimensional Scaling.”</span> In <em>Recent Developments in Statistics</em>, edited by J. R. Barra, F. Brodeau, G. Romier, and B. Van Cutsem, 133–45. Amsterdam, The Netherlands: North Holland Publishing Company.
</div>
<div id="ref-deleeuw_A_84f" class="csl-entry" role="listitem">
———. 1984. <span>“<span class="nocase">Differentiability of Kruskal’s Stress at a Local Minimum</span>.”</span> <em>Psychometrika</em> 49: 111–13.
</div>
<div id="ref-deleeuw_A_88b" class="csl-entry" role="listitem">
———. 1988. <span>“Convergence of the Majorization Method for Multidimensional Scaling.”</span> <em>Journal of Classification</em> 5: 163–80.
</div>
<div id="ref-deleeuw_R_93c" class="csl-entry" role="listitem">
———. 1993. <span>“Fitting Distances by Least Squares.”</span> Preprint Series 130. Los Angeles, CA: UCLA Department of Statistics. <a href="https://jansweb.netlify.app/publication/deleeuw-r-93-c/deleeuw-r-93-c.pdf">https://jansweb.netlify.app/publication/deleeuw-r-93-c/deleeuw-r-93-c.pdf</a>.
</div>
<div id="ref-deleeuw_E_18c" class="csl-entry" role="listitem">
———. 2018. <span>“<span class="nocase">Differentiability of Stress at Local Minima</span>.”</span> 2018.
</div>
<div id="ref-deleeuw_heiser_C_82" class="csl-entry" role="listitem">
De Leeuw, J., and W. J. Heiser. 1982. <span>“Theory of Multidimensional Scaling.”</span> In <em>Handbook of Statistics, Volume <span>II</span></em>, edited by P. R. Krishnaiah and L. Kanal. Amsterdam, The Netherlands: North Holland Publishing Company.
</div>
<div id="ref-deleeuw_mair_A_09c" class="csl-entry" role="listitem">
De Leeuw, J., and P. Mair. 2009. <span>“<span class="nocase">Multidimensional Scaling Using Majorization: SMACOF in R</span>.”</span> <em>Journal of Statistical Software</em> 31 (3): 1–30. <a href="https://www.jstatsoft.org/article/view/v031i03">https://www.jstatsoft.org/article/view/v031i03</a>.
</div>
<div id="ref-deleeuw_stoop_A_84" class="csl-entry" role="listitem">
De Leeuw, J., and I. Stoop. 1984. <span>“Upper Bounds for Kruskal’s Stress.”</span> <em>Psychometrika</em> 49: 391–402.
</div>
<div id="ref-delfour_12" class="csl-entry" role="listitem">
Delfour, M. C. 2012. <em>Introduction to Optimization and Semidifferential Calculus</em>. SIAM.
</div>
<div id="ref-du_jin_lee_jordan_poczos_singh_17" class="csl-entry" role="listitem">
Du, S., C. Jin, J. D. Lee, M. I. Jordan, B. Póczos, and A. Singh. 2017. <span>“<span class="nocase">Gradient Descent Can Take Exponential Time to Escape Saddle Points</span>.”</span> In <em>NIPS’17: Proceedings of the 31st International Conference on Neural Information Processing Systems</em>, 1067—1077.
</div>
<div id="ref-gilbert_varadhan_19" class="csl-entry" role="listitem">
Gilbert, P., and R. Varadhan. 2019. <em><span class="nocase">numDeriv: Accurate Numerical Derivatives</span></em>. <a href="https://CRAN.R-project.org/package=numDeriv">https://CRAN.R-project.org/package=numDeriv</a>.
</div>
<div id="ref-gower_dijksterhuis_04" class="csl-entry" role="listitem">
Gower, J. C., and G. B. Dijksterhuis. 2004. <em>Procrustus Problems</em>. Oxford University Press.
</div>
<div id="ref-groenen_mathar_heiser_95" class="csl-entry" role="listitem">
Groenen, P. J. F., R. Mathar, and W. J. Heiser. 1995. <span>“<span class="nocase">The Majorization Approach to Multidimensional Scaling for Minkowski Distances</span>.”</span> <em>Journal of Classification</em> 12: 3–19.
</div>
<div id="ref-heiser_91" class="csl-entry" role="listitem">
Heiser, W. J. 1991. <span>“<span class="nocase">A Generalized Majorization Method for Least Squares Multidimensional Scaling of Pseudodistances that May Be Negative</span>.”</span> <em>Psychometrika</em> 56 (1): 7–27.
</div>
<div id="ref-hiriart-urruty_88" class="csl-entry" role="listitem">
Hiriart-Urruty, J.-B. 1988. <span>“Generalized Differentiability / Duality and Optimization for Problems Dealing with Differences of Convex Functions.”</span> In <em>Convexity and Duality in Optimization</em>, edited by Ponstein. J., 37–70. Lecture Notes in Economics and Mathematical Systems 256. Springer.
</div>
<div id="ref-lee_simchowitz_jordan_recht_16" class="csl-entry" role="listitem">
Lee, J. D., M. Simchowitz, M. I. Jordan, and B. Recht. 2016. <span>“<span class="nocase">Gradient Descent Converges to Minimizers</span>.”</span>
</div>
<div id="ref-ortega_rheinboldt_70" class="csl-entry" role="listitem">
Ortega, J. M., and W. C. Rheinboldt. 1970. <em><span class="nocase">Iterative Solution of Nonlinear Equations in Several Variables</span></em>. New York, N.Y.: Academic Press.
</div>
<div id="ref-pliner_96" class="csl-entry" role="listitem">
Pliner, V. 1996. <span>“<span class="nocase">Metric Unidimensional Scaling and Global Optimization</span>.”</span> <em>Journal of Classification</em> 13: 3–18.
</div>
<div id="ref-rockafellar_70" class="csl-entry" role="listitem">
Rockafellar, R. T. 1970. <em>Convex Analysis</em>. Princeton University Press.
</div>
<div id="ref-trosset_mathar_97" class="csl-entry" role="listitem">
Trosset, M. W., and R. Mathar. 1997. <span>“<span class="nocase">On the Existence on Nonglobal Minimizers of the STRESS Criterion for Metric Multidimensional Scaling</span>.”</span> In <em>Proceedings of the Statistical Computing Section</em>, 158–62. Alexandria, VA: American Statistical Association.
</div>
</div>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./intro.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./spaces.html" class="pagination-link" aria-label="Stress Spaces">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Stress Spaces</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>