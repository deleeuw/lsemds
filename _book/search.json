[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Least Squares Euclidean Multidimensional Scaling",
    "section": "",
    "text": "Note\nThis book will be expanded/updated frequently. The directory github.com/deleeuw/stress has a pdf version, a html version, the bib file, the complete Rmd file with the codechunks, and the R and C source code. All suggestions for improvement of text or code are welcome, and some would be really beneficial. For example, I only use base R graphics, nothing more fancy, because base graphics is all I know.\nAll text and code are in the public domain and can be copied, modified, and used by anybody in any way they see fit. Attribution will be appreciated, but is not required. For completeness we include a slighty modified version of the Unlicense as appendix @ref(apunlicense).\nI number and label all displayed equations. Equations are displayed, instead of inlined, if and only if one of the following is true.\n\nThey are important.\nThey are referred to elsewhere in the text.\nNot displaying them messes up the line spacing.\n\nAll code chunks in the text are named. Theorems, lemmas, chapters, sections, subsections and so on are also named and numbered, using bookdown/Rmarkdown.\nI have been somewhat hesitant to use lemmas, theorems, and corollaries in this book. But ultimately they enforce precision and provide an excellent organizational tool. If there is a proof of a lemma, theorem, or corollary, it ends with a \\(\\square\\).\nAnother idiosyncracy: if a line in multiline displayed equation ends with “=”, then the next line begins with “=”. If it ends with “+”, then the next line begin with “+”, and if it ends with “-” the next line begins with “+” as well. I’ll try to avoid ending a line with “+” or “-”, especially with “-”, but if it happens you are warned. A silly example is\n\\[\\begin{align}\n&(x+y)^2-\\\\\n&+4x=\\\\\n&=x^2+y^2-2x=\\\\\n&=(x-y)^2\\geq\\\\\n&\\geq 0.\n\\end{align}\\]\nJust as an aside: if I refer to something that has been mentioned “above” I mean something that comes earlier in the book and “below” refers to anything that comes later. This always confuses me, so I had to write it down.\nThe dilemma of whether to use “we” or “I” throughout the book is solved in the usual way. If I feel that a result is the work of a group (me, my co-workers, and the giants on whose shoulders we stand) then I use “we”. If it’s an individual decision, or something personal, then I use “I”. The default is “we”, as it always should be in scientific writing.\nMost of the individual chapters also have some of the necessary mathematical background material, both notation and results, sometimes with specific eleborations that seem useful for the book. Sometimes this background material is quite extensive. Examples are splines, majorization, unweighting, monotone regression, and the basic Zangwill and Ostrowski fixed point theorems we need for convergence analysis of our algorithms.\nThere is an appendix @ref(apcode) with code, and an appendix @ref(apdatasets) with data sets. These contain brief descriptions and links to the supplementary materials directories, which contain the actual code and data.\nSomething about code and R/C\nI will use this note to thank Rstudio, in particular J.J. Allaire and Yihui Xi, for their contributions to the R universe, and for their promotion of open source software and open access publications. Not too long ago I was an ardent LaTeX user, firmly convinced I would never use anything else again in my lifetime. In the same way thatI was convinced before I would never use anything besides, in that order, FORTRAN, PL/I, APL, and (X)Lisp. And PHP/Apache/MySQL. But I lived too long. And then, in my dotage, lo and behold, R, Rstudio, (R)Markdown, bookdown, blogdown, Git, Github, Netlify came along.\n\n\n\nForrest Young, Bepi Pinner, Jean-Marie Bouroche, Yoshio Takane, Jan de Leeuw at La Jolla, August 1975",
    "crumbs": [
      "Note"
    ]
  },
  {
    "objectID": "notation.html",
    "href": "notation.html",
    "title": "Notation and Reserved Symbols",
    "section": "",
    "text": "Spaces\n\\(\\text{vec}\\) and \\(\\text{vec}^{-1}\\)",
    "crumbs": [
      "Notation and Reserved Symbols"
    ]
  },
  {
    "objectID": "notation.html#spaces",
    "href": "notation.html#spaces",
    "title": "Notation and Reserved Symbols",
    "section": "",
    "text": "\\(\\mathbb{R}^n\\) is the space of all real vectors, i.e. all \\(n\\)-element tuples of real numbers. Typical elements of \\(\\mathbb{R}^n\\) are \\(x,y,z\\). The element of \\(x\\) in position \\(i\\) is \\(x_i\\). Defining a vector by its elements is done with \\(x=\\{x_i\\}\\).\n\\(\\mathbb{R}^n\\) is equipped with the inner product \\(\\langle x,y\\rangle=x'y=\\sum_{i=1}^nx_iy_i\\) and the norm \\(\\|x\\|=\\sqrt{x'x}\\).\nThe canonical basis for \\(\\mathbb{R}^n\\) is the \\(n-\\)tuple \\((e_1,cdots,e_n)\\), where \\(e_i\\) has element \\(i\\) equal to \\(+1\\) and all other elements equal to zero. Thus \\(\\|e_i\\|=1\\) and \\(\\langle e_i,e_j\\rangle=\\delta^{ij}\\), with \\(\\delta^{ij}\\) the Kronecker delta (equal to one if \\(i=j\\) and zero otherwise). Note that \\(x_i=\\langle e_i,x\\rangle\\).\n\\(\\mathbb{R}\\) is the real line and \\(\\mathbb{R}_+\\) is the half line of non-negative numbers.\n\\(\\mathbb{R}^{n\\times m}\\) is the space of all \\(n\\times m\\) real matrices. Typical elements of \\(\\mathbb{R}^{n\\times m}\\) are \\(A,B,C\\). The element of \\(A\\) in row \\(i\\) and column \\(j\\) is \\(a_{ij}\\). Defining a matrix by its elements is done with \\(A=\\{a_{ij}\\}\\).\n\\(\\mathbb{R}^{n\\times m}\\) is equipped with the inner product \\(\\langle A,B\\rangle=\\text{tr} A'B=\\sum_{i=1}^n\\sum_{j=1}^ma_{ij}b_{ij}\\) and the norm \\(\\|A\\|=\\sqrt{\\text{tr}\\ A'A}\\).\nThe canonical basis for \\(\\mathbb{R}^{n\\times m}\\) is the \\(nm-\\)tuple \\((E_{11},cdots,E_{nm})\\), where \\(E_{ij}\\) has element \\((i,j)\\) equal to \\(+1\\) and all other elements equal to zero. Thus \\(\\|E_{ij}\\|=1\\) and \\(\\langle E_{ij},E_{kl}\\rangle=\\delta^{ik}\\delta^{jl}\\).",
    "crumbs": [
      "Notation and Reserved Symbols"
    ]
  },
  {
    "objectID": "notation.html#matrices",
    "href": "notation.html#matrices",
    "title": "Notation and Reserved Symbols",
    "section": "Matrices",
    "text": "Matrices\n\n\\(a_{i\\bullet}\\) is row \\(i\\) of matrix \\(A\\), \\(a_{\\bullet j}\\) is column \\(j\\).\n\\(a_{i\\star}\\) is the sum of row \\(i\\) of matrix \\(A\\), \\(a_{\\star j}\\) is the sum of column \\(j\\).\n\\(A'\\) is the transpose of \\(A\\), and \\(\\text{diag}(A)\\) is the diagonal matrix with the diagonal elements of \\(A\\). The inverse of a square matrix \\(A\\) is \\(A^{-1}\\), the Moore-Penrose generalized inverse of any matrix \\(A\\) is \\(A^+\\).\nIf \\(A\\) and \\(B\\) are two \\(n\\times m\\) matrices then their Hadamard (or elementwise) product \\(C=A\\times B\\) has elements \\(c_{ij}=a_{ij}b_{ij}\\). The Hadamard quotient is \\(C=A/B\\), with elements \\(c_{ij}=a_{ij}/b_{ij}\\). The Hadamard power is \\(A^{(k)}=A^{(p-1)}\\times A\\).\nDC matrices. Centering matrix. \\(J_n=I_n-n^{-1}E_n\\). We do not use gthe subscripts if the order is obvious from the context.",
    "crumbs": [
      "Notation and Reserved Symbols"
    ]
  },
  {
    "objectID": "notation.html#functions",
    "href": "notation.html#functions",
    "title": "Notation and Reserved Symbols",
    "section": "Functions",
    "text": "Functions\n\n\\(f,g,h,\\cdots\\) are used for functions or mappings. \\(f:X\\rightarrow Y\\) says that \\(f\\) maps \\(X\\) into \\(Y\\).\n\\(\\sigma\\) is used for all real-valued least squares loss functions.",
    "crumbs": [
      "Notation and Reserved Symbols"
    ]
  },
  {
    "objectID": "notation.html#mds",
    "href": "notation.html#mds",
    "title": "Notation and Reserved Symbols",
    "section": "MDS",
    "text": "MDS\n\n\\(\\Delta=\\{\\delta_{ij\\cdots}\\}\\) is a matrix or array of dissimilarities.\n\\(\\langle \\mathbb{X},d\\rangle\\) is a metric space, with \\(d:\\mathcal{X}\\otimes\\mathcal{X}\\rightarrow\\mathbb{R}_+\\) the distance function. If \\(X\\) is is an ordered n-tuple \\((x_1,\\cdots,x_n)\\) of elements of \\(\\mathcal{X}\\) then \\(D(X)\\) is \\(\\{d(x_i,x_j)\\}\\), the elements of which we also write as \\(d_{ij}(X)\\).\nSummation over the elements of vector \\(x\\in\\mathbb{R}^n\\) is \\(\\sum_{i=1}^n x_i\\). Summation over the elements of matrix \\(A\\in\\mathbb{R}^{n\\times m}\\) is \\(\\sum_{i=1}^n\\sum_{j=1}^m a_{ij}\\). Summation over the elements above the diagonal of \\(A\\) is \\(\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}a_{ij}\\).\nConditional summation is, for example, \\(\\sum_{i=1}^n \\{x_i\\mid x_i&gt;0\\}\\).\n\nIteration",
    "crumbs": [
      "Notation and Reserved Symbols"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "This book is definitely not an impartial and balanced review of all of multidimensional scaling (MDS) theory and history. It emphasizes computation, and the mathematics needed for computation. In addition, it is a summary of over 50 years of MDS work by me, either solo or together with my many excellent current or former co-workers and co-authors. It is heavily biased in favor of the smacof formulation of MDS (De Leeuw (1977), De Leeuw and Heiser (1977), De Leeuw and Mair (2009)), and the corresponding majorization (or MM) algorithms. And, moreover, I am shamelessly squeezing in as many references to my published and unpublished work as possible, with links to the corresponding pdf’s if they are available. Thus this book is also a jumpstation into my bibliography.\nI have not organized the book along historical lines because most of the early techniques and results have been either drastically improved or completely abandoned. Nevertheless, some personal historical perspective may be useful. I will put most of it in this preface, so uninterested readers can easily skip it.\nI got involved in MDS in 1968 when John van de Geer returned from a visit to Clyde Coombs in Michigan and started the Department of Data Theory in the Division of Social Sciences at Leiden University. I was John’s first hire, although I was still a graduate student at the time.\nRemember that Clyde Coombs was running the Michigan Mathematical Psychology Program, and he had just published his remarkable book “A Theory of Data” (Coombs (1964)). The name of the new department in Leiden was taken from the title of that book, and Coombs was one of the first visitors to give a guest lecture there.\nThis is maybe the place to clear up some possible misunderstandings about the name “Data Theory”. Coombs was mainly interested in a taxonomy of data types, and in pointing out that “data” were not limited to a table or data-frame of objects by variables. In addition, there were also similarity ratings, paired comparisons, and unfolding data. Coombs also emphasized that data were often non-metric, i.e. ordinal or categorical, and that it was possible to analyze these ordinal or categorical relationships directly, without first constructing numerical scales to which classical techniques could be applied. One of the new techniques discussed in Coombs (1964) was a ordinal form of MDS, in which not only the data but also the representation of the data in Euclidean space were non-metric.\nJohn van de Geer had just published Van de Geer (1967). In that book, and in the subsequent book Van de Geer (1971), he developed his unique geometric approach to multivariate analysis. Relationship between variables, and between variables and individuals, were not just discussed using matrix algebra, but were also visualized in diagrams. This was related to the geometric representations in Coombs’ Theory of Data, but it concentrated on numerical data in the form of rectangular matrices of objects by variables.\nLooking back it is easy to see that both Van de Geer and Coombs influenced my approach to data analysis. I inherited the emphasis on non-metric data and on visualization. But, from the beginning, I interpreted “Data Theory” as “Data Analysis”, with my emphasis shifting to techniques, loss functions, implementations, algorithms, optimization, computing, and programming. This is of interest because in 2020 my former Department of Statistics at UCLA, together with the Department of Mathematics, started a bachelor’s program in Data Theory, in which “Emphasis is placed on the development and theoretical support of a statistical model or algorithmic approach. Alternatively, students may undertake research on the foundations of data science, studying advanced topics and writing a senior thesis.” This sounds like a nice hybrid of Data Theory and Data Analysis, with a dash of computer science mixed in.\nComputing and optimization were in the air in 1968, not so much because of Coombs, but mainly because of Roger Shepard, Joe Kruskal, and Doug Carroll at Bell Labs in Murray Hill. John’s other student Eddie Roskam and I were fascinated by getting numerical representations from ordinal data by minimizing explicit least squares loss functions. Eddie wrote his dissertation in 1968 (Roskam (1968)). In 1973 I went to Bell Labs for a year, and Eddie went to Michigan around the same time to work with Jim Lingoes, resulting in Lingoes and Roskam (1973).\nMy first semi-publication was De Leeuw (1968), quickly followed by a long sequence of other, admittedly rambling, internal reports. Despite this very informal form of publication the sheer volume of them got the attention of Joe Kruskal and Doug Carroll, and I was invited to spend the academic year 1973-1974 at Bell Laboratories. That visit somewhat modified my cavalier approach to publication, but I did not become half-serious in that respect until meeting with Forrest Young and Yoshio Takane at the August 1975 US-Japan seminar on MDS in La Jolla. Together we used the alternating least squares approach to algorithm construction that I had developed since 1968 into a quite formidable five-year publication machine, with at its zenith Takane, Young, and De Leeuw (1977).\nIn La Jolla I gave the first presentation of the majorization method for MDS, later known as smacof, with the first formal convergence proof. The canonical account of smacof was published in a conference paper (De Leeuw (1977)). Again I did not bother to get the results into a journal or into some other more effective form of publication. The basic theory for what became known as smacof was also presented around the same time in another book chapter De Leeuw and Heiser (1977).\nIn 1978 I was invited to the Fifth International Symposium on Multivariate Analysis in Pittsburgh to present what became De Leeuw and Heiser (1980). There I met Nan Laird, one of the authors of the basic paper on the EM algorithm (Dempster, Laird, and Rubin (1977)). I remember enthusiastically telling her on the conference bus that EM and smacof were both special case of the general majorization approach to algorithm construction, which was consequently born around the same time. But that is a story for a companion volume, which currently only exists in a very preliminary stage (https://github.com/deleeuw/bras).\nMy 1973 PhD thesis (De Leeuw (1973), reprinted as De Leeuw (1984)) was actually my second attempt at a dissertation. I had to get a PhD, any PhD, before going to Bell Labs, because of the difference between the Dutch and American academic title and reward systems. I started writing a dissertation on MDS, in the spirit of what later became De Leeuw and Heiser (1982). But halfway through I lost interest and got impatient, and I decided to switch to nonlinear multivariate analysis. This second attempt did produced a finished dissertation (De Leeuw (1973)), which grew over time, with the help of multitudes, into Gifi (1990). But that again is a different history, which I will tell some other time in yet another companion volume (https://github.com/deleeuw/gifi). For a long time I did not do much work on MDS, until the arrival of Patrick Mair and the R language led to a resurgence of my interest, and ultimately to De Leeuw and Mair (2009) and Mair, Groenen, and De Leeuw (2022).\nI consider this MDS book to be a summary and extension of the basic papers De Leeuw (1977), De Leeuw and Heiser (1977), De Leeuw and Heiser (1980), De Leeuw and Heiser (1982), and De Leeuw (1988), all written 30-40 years ago. Footprints in the sands of time. It can also be seen as an elaboration of the more mathematical and computational sections of the excellent and comprehensive textbook of Borg and Groenen (2005). That book has much more information about the origins, the data, and the applications of MDS, as well as on the interpretation of MDS solutions. In this book I concentrate almost exclusively on the mathematical, computational, and programming aspects of MDS.\nFor those who cannot get enough of me, there is a data base of my published and unpublished reports and papers since 1965, with links to pdf’s, at https://jansweb.netlify.app/publication/.\nThere are many, many people I have to thank for my scientific education. Sixty years is a long time, and consequently many excellent teachers and researchers have crossed my path. I will gratefully mention the academics who had a major influence on my work and who are not with us any more, since I will join them in the not too distant future: Louis Guttman (died 1987), Clyde Coombs (died 1988), Warren Torgerson (died 1999), Forrest Young (died 2006), John van de Geer (died 2008), Joe Kruskal (died 2010), Doug Carroll (died 2011), and Rod McDonald (died 2012).\n\n\n\n\nBorg, I., and P. J. F. Groenen. 2005. Modern Multidimensional Scaling. Second Edition. Springer.\n\n\nCoombs, C. H. 1964. A Theory of Data. Wiley.\n\n\nDe Leeuw, J. 1968. “Nonmetric Multidimensional Scaling.” Research Note 010-68. Department of Data Theory FSW/RUL. https://jansweb.netlify.app/publication/deleeuw-r-68-9/deleeuw-r-68-g.pdf.\n\n\n———. 1973. “Canonical Analysis of Categorical Data.” PhD thesis, University of Leiden, The Netherlands. https://jansweb.netlify.app/publication/deleeuw-b-73/deleeuw-b-73.pdf.\n\n\n———. 1977. “Applications of Convex Analysis to Multidimensional Scaling.” In Recent Developments in Statistics, edited by J. R. Barra, F. Brodeau, G. Romier, and B. Van Cutsem, 133–45. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\n———. 1984. Canonical Analysis of Categorical Data. Leiden, The Netherlands: DSWO Press. https://jansweb.netlify.app/publication/deleeuw-b-84/deleeuw-b-84.pdf.\n\n\n———. 1988. “Convergence of the Majorization Method for Multidimensional Scaling.” Journal of Classification 5: 163–80.\n\n\nDe Leeuw, J., and W. J. Heiser. 1977. “Convergence of Correction Matrix Algorithms for Multidimensional Scaling.” In Geometric Representations of Relational Data, edited by J. C. Lingoes, 735–53. Ann Arbor, Michigan: Mathesis Press.\n\n\n———. 1980. “Multidimensional Scaling with Restrictions on the Configuration.” In Multivariate Analysis, Volume V, edited by P. R. Krishnaiah, 501–22. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\n———. 1982. “Theory of Multidimensional Scaling.” In Handbook of Statistics, Volume II, edited by P. R. Krishnaiah and L. Kanal. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\nDe Leeuw, J., and P. Mair. 2009. “Multidimensional Scaling Using Majorization: SMACOF in R.” Journal of Statistical Software 31 (3): 1–30. https://www.jstatsoft.org/article/view/v031i03.\n\n\nDempster, A. P., N. M. Laird, and D. B. Rubin. 1977. “Maximum Likelihood for Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society B39: 1–38.\n\n\nGifi, A. 1990. Nonlinear Multivariate Analysis. New York, N.Y.: Wiley.\n\n\nLingoes, J. C., and E. E. Roskam. 1973. “A Mathematical and Empirical Analysis of Two Multidimensional Scaling Algorithms.” Psychometrika 38: Monograph Supplement.\n\n\nMair, P., P. J. F. Groenen, and J. De Leeuw. 2022. “More on Multidimensional Scaling in R: smacof Version 2.” Journal of Statistical Software 102 (10): 1–47. https://www.jstatsoft.org/article/view/v102i10.\n\n\nRoskam, E. E. 1968. “Metric Analysis of Ordinal Data in Psychology.” PhD thesis, University of Leiden.\n\n\nTakane, Y., F. W. Young, and J. De Leeuw. 1977. “Nonmetric Individual Differences in Multidimensional Scaling: An Alternating Least Squares Method with Optimal Scaling Features.” Psychometrika 42: 7–67.\n\n\nVan de Geer, J. P. 1967. Inleiding in de Multivariate Analyse. Van Loghum Slaterus.\n\n\n———. 1971. Introduction to Multivariate Analysis for the Social Sciences. San Francisco, CA: Freeman.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Brief History\nDe Leeuw and Heiser (1980)\nThis section has a different emphasis. We limit ourselves to developments in Euclidean MDS, and to contributions with direct computational consequences that have a direct or indirect link to psychometrics, and to work before 1960. This is reviewed ably in the presidential address of W. S. Torgerson (1965).\nOur history review takes the form of brief summaries of what we consider to be milestone papers or books.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#introhist",
    "href": "intro.html#introhist",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1.1 Milestones\nW. S. Torgerson (1952) W. S. Torgerson (1965)\nShepard (1962a) Shepard (1962b)\nKruskal (1964a) Kruskal (1964b)\nGuttman (1968)\nDe Leeuw (1977) De Leeuw and Heiser (1977)\nThere was some early work by Richardson, Messick, Abelson and Torgerson who combined Thurstonian scaling of similarities with the mathematical results of Schoenberg (1935) and Young and Householder (1938).\nDespite these early contributions it makes sense, certainly from the point of view of my personal history, but probably more generally, to think of MDS as starting as a widely discussed, used, and accepted technique since the book by W. S. Torgerson (1958). This was despite the fact that in the fifties and sixties computing eigenvalues and eigenvectors of a matrix of size 20 or 30 was still a considerable challenge.\nA few years later the popularity of MDS got a large boost by developments centered at Bell Telephone Laboratories in Murray Hill, New Jersey, the magnificent precursor of Silicon Valley. First there was nonmetric MDS by Shepard (1962a), Shepard (1962b) and Kruskal (1964a), Kruskal (1964b), And later another major development was the introduction of individual difference scaling by Carroll and Chang (1970) and Harshman (1970). Perhaps even more important was the development of computer implementations of these new techniques. Some of the early history of nonmetric MDS is in De Leeuw (2017a).\nAround the same time there were interesting theoretical contributions in Coombs (1964), which however did not much influence the practice of MDS. ….. And several relatively minor variations of the Bell Laboratories approach were proposed by Guttman (1968), but Guttman’s influence on further MDS implementations turned out to be fairly localized and limited.\nThe main development in comptational MDS after the Bell Laboratories surge was probably smacof. Initially, in De Leeuw (1977), this stood for Scaling by Maximizing a Convex Function. Later it was also used to mean Scaling by Majorizing a Complicated Function. Whatever. In this book smacof just stands for smacof. No italics, no boldface, no capitals.\nThe first smacof programs were written in 1977 in FORTRAN at the Department of Data Theory in Leiden (Heiser and De Leeuw (1977)). Eventually they migrated to SPSS (for example, Meulman and Heiser (2012)) and to R (De Leeuw and Mair (2009)). The SPSS branch and the R branch have diverged somewhat, and they continue to be developed independently.\nParallel to this book there is an attempt to rewrite the various smacof programs in C, with the necessary wrappers to call them from R (De Leeuw (2017b)). The C code, with makefiles and test routines, is at github.com/deleeuw/smacof",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#introbasic",
    "href": "intro.html#introbasic",
    "title": "1  Introduction",
    "section": "1.2 Basic MDS",
    "text": "1.2 Basic MDS\nFollowing Kruskal, and to a lesser extent Shepard, we measure the fit of distances to dissimilarities using an explicit real-valued loss function (or badness-of-fit measure), which is minimized over the possible maps of the objects into the metric space. This is a very general definition of MDS, covering all kinds of variations of the target metric space and of the way fit is measured. Obviously we will not discuss all these possible forms of MDS, which also includes various techniques more properly discussed as cluster analysis, classification, or discrimination.\nTo fix our scope we first define basic MDS, which is short for Least Squares Euclidean Metric MDS. It is defined as MDS with the following characteristics.\n\nThe metric space is a Euclidean space.\nThe dissimilarities are numerical, symmetric, and non-negative.\nThe loss function is a weighted sum of squares of the residuals, which are the differences between dissimilarities and Euclidean distances.\nWeights are numerical, symmetric, and non-negative.\nSelf-dissimilarities are zero and the corresponding terms in the loss function also have weight zero.\n\nBy a Euclidean space we mean a finite dimensional vector space, with addition and scalar multiplication, and with an inner product that defines the distances. For the inner product of vectors \\(x\\) and \\(y\\) we write \\(\\langle x,y\\rangle\\). The norm of \\(x\\) is defined as \\(\\|x\\|:=\\sqrt{\\langle x,x\\rangle}\\), and the distance between \\(x\\) and \\(y\\) is \\(d(x,y):=\\|x-y\\|\\).\nThe loss function we use is called stress. It was first explicitly introduced in MDS as raw stress by Kruskal (1964a) and Kruskal (1964b). We define stress in a slightly different way, because we want to be consistent over the whole range of the smacof versions and implementations. In smacof stress is the real-valued function \\(\\sigma\\), defined on the space \\(\\mathbb{R}^{n\\times p}\\) of configurations, as\n\\[\n\\sigma(X):=\\frac14\\sum_{i=1}^n\\sum_{j=1}^n w_{ij}(\\delta_{ij}-d_{ij}(X))^2.\n\\tag{1.1}\\]\nNote that we use \\(:=\\) for definitions, i.e. for concepts and symbols that are not standard mathematical usage, when they occur for the first time in this book. Through the course of the book it will probably become clear why the mysterious factor \\(\\frac14\\) is there. Clearly it has no influence on the actual minimization of the loss function.\nIn Equation 1.1 we use the following objects and symbols.\n\n\\(W=\\{w_{ij}\\}\\) is a symmetric, non-negative, and hollow matrix of weights, where hollow means zero diagonal.\n\\(\\Delta=\\{\\delta_{ij}\\}\\) is a symmetric, non-negative, and hollow matrix of dissimilarities.\n\\(X\\) is an \\(n\\times p\\) configuration, containing coordinates of \\(n\\) points in \\(p\\) dimensions.\n\\(D(X)=\\{d_{ij}(X)\\}\\) is a symmetric, non-negative, and hollow matrix of Euclidean distances between the \\(n\\) points in \\(X\\). Thus \\(d_{ij}(X):=\\sqrt{\\sum_{s=1}^p(x_{is}-x_{js})^2}\\).\n\nNote that symmetry and hollowness of the basic objects \\(W\\), \\(\\Delta\\), and \\(D\\) allows us carry out the summation of the weighted squared residuals in formula (Equation 1.1) over the upper (or lower) diagonal elements only. Thus we can also write \\[\n\\sigma(X):=\\frac12\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n} w_{ij}(\\delta_{ij}-d_{ij}(X))^2.\n\\tag{1.2}\\] We use the notation \\(\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}\\) for summation over the lower-diagonal elements of a matrix.\nThe function \\(D\\), which computes the distance matrix \\(D(X)\\) from a configuration \\(X\\), is matrix-valued. It maps the \\(n\\times p\\)-dimensional configuration space \\(\\mathbb{R}^{n\\times p}\\) into the set \\(D(\\mathbb{R}^{n\\times p})\\) of Euclidean distance matrices between \\(n\\) points in \\(\\mathbb{R}^p\\), which is a subset of the convex cone of hollow, symmetric, non-negative matrices in the linear space \\(\\mathbb{R}^{n\\times n}\\) (Datorro (2018)).\nIn basic MDS the weights and dissimilarities are given numbers, and we minimize stress over all \\(n\\times p\\) configurations \\(X\\). Note that the dimensionality \\(p\\) is also supposed to be known beforehand, and that MDS in \\(p\\) dimensions is different from MDS in \\(q\\not= p\\) dimensions. We sometimes emphasize this by writing \\(pMDS\\), which indicates that we will map the points into \\(p\\)-dimensional space.\nTwo boundary cases that will interest us are Unidimensional Scaling or UDS, where \\(p=1\\), and Full-dimensional Scaling or FDS, where \\(p=n\\). Thus UDS is 1MDS and FDS is nMDS. Most actual MDS applications in the sciences use 1MDS, 2MDS or 3MDS, because configurations in one, two, or three dimensions can easily be plotted with standard graphics tools. Note that MDS is not primarily a tool to tests hypotheses about dimensionality and to find meaningful dimensions. It is a mostly a mapping tool for data reduction, to graphically find interesting aspects of dissimilarity matrices.\nThe projections on the dimensions are usually ignored, it is the configuration of points that is the interesting outcome. This distinguishes MDS from, for example, factor analysis. There is no Varimax, Oblimax, Quartimax, and so on. Exceptions are confirmatory applications of MDS in genetic mapping along the chromosome, in archeological seriation, in testing psychological theories of cognition and representation, in the conformation of molecules, and in geographic and geological applications. In these areas the dimensionality and general structure of the configuration are given by prior knowledge, we just do not know the precise location and distances of the points. For more discussion of the different uses of MDS we refer to De Leeuw and Heiser (1982).\n\n1.2.1 Kruskal’s stress\nEquation 1.1 differs from Kruskal’s original stress in at least three ways: in Kruskal’s use of the square root, in our use of weights, and in our different approach to normalization.\nWe have paid so much attention to Kruskal’s original definition, because the choices made there will play a role in the normalization discussion in the ordinal scaling chapter (section ?sec-nmdsnorm), in the comparison of Kruskal’s and Guttman’s approach to ordinal MDS (sections @ref(nmdskruskal) and @ref(nmdsguttman)), and in our discussions about the differences between Kruskal’s stress @ref(eq:kruskalstressfinal) and smacof’s stress @ref(eq:stressall) in the next three sections of this chapter.\n\n1.2.1.1 Square root\nLet’s discuss the square root first. Using it or not using it does not make a difference for the minimization problem. Using the square root, however, does give a more sensible root-mean-square scale, in which stress is homogeneous of degree one, instead of degree two. But I do not want to compute all those unnecessary square roots in my algorithms, and I do not want to drag them along through my derivations. Moreover the square root potentially causes problems with differentiability at those \\(X\\) where \\(\\sigma(X)\\) is zero. Thus, througout the book, we do not use the square root in our formulas and derivations. In fact, we do not even use it in our computer programs, except at the very last moment when we return the final stress after the algorithm has completed.\n\n\n1.2.1.2 Weights\nThere were no weights \\(W=\\{w_{ij}\\}\\) in the original definition of stress by Kruskal (1964a), and neither are they there in most of the basic later contributions to MDS by Guttman, Lingoes, Roskam, Ramsay, or Young. We will use weights throughout the book, because they have various interesting applications within basic MDS, without unduly complicating the derivations and computations. In Groenen and Van de Velden (2016), section 6, the various uses of weights in the stress loss function are enumerated. They generously, but correctly, attribute the consistent use of weights in MDS to me. I quote from their paper:\n\n\nHandling missing data is done by specifying \\(w_{ij} = 0\\) for missings and 1 otherwise thereby ignoring the error corresponding to the missing dissimilarities.\nCorrecting for nonuniform distributions of the dissimilarities to avoid dominance of the most frequently occurring dissimilarities.\nMimicking alternative fit functions for MDS by minimizing Stress with \\(w_{ij}\\) being a function of the dissimilarities.\nUsing a power of the dissimilarities to emphasize the ﬁtting of either large or small dissimilarities.\nSpecial patterns of weights for speciﬁc models.\nUsing a speciﬁc choice of weights to avoid nonuniqueness.\n\n\nIn some situations, for example for huge data sets, it is computationally convenient, or even necessary, to minimize the influence of the weights on the computations. We can use majorization to turn the problem from a weighted least squares problem to an iterative unweighted least squares problem. The technique, which we call unweighting, is discussed in detail in section @ref(minunweight).\n\n\n1.2.1.3 Normalization\nThis section deals with a rather trivial problem, which has however caused problems in various stages of smacof’s 45-year development history. Because the problem is trivial, and the choices that must be made are to a large extent arbitrary, it has been overlooked and somewhat neglected.\nIn basic MDS we scale the weights and dissimilarities. It is clear that if we multiply the weights or dissimilarities by a constant, then the optimal approximating distances \\(D(X)\\) and the optimal configuration \\(X\\) will be multiplied by the same constant. That is exactly why Kruskal’s raw stress had to be normalized. Consequently we in basic MDS we always scale weights and dissimilarities by\n\\[\\begin{align}\n\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}&=1,(\\#eq:scaldiss1)\\\\\n\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}^{\\ }\\delta_{ij}^2&=1.(\\#eq:scaldiss2)\n\\end{align}\\]\nThis simplifies our formulas and makes them look better (see, for example, section @ref(propexpand) and section @ref(secrhostress)). It presupposes, of course, that \\(w_{ij}\\delta_{ij}\\not=0\\) for at least one \\(i\\not= j\\), which we will happily assume in the sequel, because otherwise the MDS problem is trivial. Note that if all weights are equal (which we call the unweighted case) then they are equal to \\(1/\\binom{n}{2}\\) and thus we require \\(\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}\\delta_{ij}^2=\\frac12n(n-1)\\).\nUsing normalized dissimilarities amounts to the same defining stress as\n\\[\\begin{equation}\n\\sigma(X)=\\frac12\\frac{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\delta_{ij}^2-d_{ij}(X))^2}{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\delta_{ij}^2}.\n(\\#eq:stressrat)\n\\end{equation}\\]\nThis is useful to remember when we discuss the various normalizations for non-metric MDS in section @ref(nmdsnorm).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#seclocglob",
    "href": "intro.html#seclocglob",
    "title": "1  Introduction",
    "section": "1.3 Local and Global",
    "text": "1.3 Local and Global\nIn basic MDS our goal is to compute both \\(\\min_X\\sigma(X)\\) and \\(\\mathop{\\text{Argmin}}_X\\sigma(X)\\), where \\(\\sigma(X)\\) is defined as @ref(eq:stressall), and where we minimize over all configurations in \\(\\mathbb{R}^{n\\times p}\\).\nIn this book we study both the properties of the stress loss function and a some of its generalizations, and the various ways to minimize these loss functions over configurations (and sometimes over transformations of the dissimilarities as well).\nEmphasis local minima\nCompute stationary points\nNote we use the notation \\(\\mathop{\\text{Argmin}}_{x\\in X}f(x)\\) for the set of minimizers of \\(f\\) over \\(X\\). Thus \\(z\\in\\mathop{\\text{Argmin}}_{x\\in X}f(x)\\) means \\(z\\) minimizes \\(f\\) over \\(X\\), i.e. \\(f(z)=\\min_{x\\in X} f(x)\\). If it is clear from theory that the minimum is necessarily unique, we use \\(\\text{argmin}\\) instead of \\(\\text{Argmin}\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#introgeneralize",
    "href": "intro.html#introgeneralize",
    "title": "1  Introduction",
    "section": "1.4 Generalizations",
    "text": "1.4 Generalizations\nThe most important generalizations of basic MDS we will study in later chapters of this book are discussed briefly in the following sections.\n\n1.4.1 Non-metric MDS\nBasic MDS is a form of Metric Multidimensional Scaling or MMDS, in which dissimilarities are either known or missing. In chapter @ref(nonmtrmds) we relax this assumption. Dissimilarities may be partly known, for example we may know they are in some interval, we may only know their order, or we may know them up to some smooth transformation. MDS with partly known dissimilarities is Non-metric Multidimensional Scaling or NMDS. Completely unknown (missing) dissimilarities are an exception, because we can just handle this in basic MDS by setting the corresponding weights equal to zero.\nIn NMDS we minimize stress over all configurations, but also over the unknown dissimilarities. What we know about them (the interval they are in, the transformations that are allowed, the order they are in) defines a subset of the space of non-negative, hollow, and symmetric matrices. Any matrix in that subset is a matrix of what Takane, Young, and De Leeuw (1977) call disparities, i.e. imputed dissimilarities. The imputation provides the missing information and transforms the non-numerical information we have about the dissimilarities into a numerical matrix of disparities. Clearly this is an optimistic imputation, in the sense that it chooses from the set of admissible disparities to minimize stress (for a given configuration).\nOne more terminological point. Often non-metric is reserved for ordinal MDS, in which we only know a (partial or complete) order of the dissimilarities. Allowing linear or polynomial transformations of the dissimilarities, or estimating an additive constant, is then supposed to be a form of metric MDS. There is something to be said for that. Maybe it makes sense to distinguish non-metric in the wide sense (in which stress must be minimized over both \\(X\\) and \\(\\Delta\\)) and non-metric in the narrow sense in which the set of admissible disparities is defined by linear inequalities. Nonmetric in the narrow sense will also be called ordinal MDS or OMDS.\nIt is perhaps useful to remember that Kruskal (1964a) introduced explicit loss functions in MDS to put the somewhat heuristic NMDS techniques of Shepard (1962a) onto a firm mathematical and computational foundation. Thus, more or less from the beginning of iterative least squares MDS, there was a focus on non-metric rather than metric MDS, and this actually contributed a great deal to the magic and success of the technique. In this book most of the results are derived for basic MDS, which is metric MDS, with non-metric MDS as a relatively straightforward extension not discussed until chapter @ref(nonmtrmds). So, at least initially, we take the numerical values of the dissimilarities seriously, as do W. S. Torgerson (1958) and Shepard (1962a), Shepard (1962b).\nIt may be the case that in the social and behavioural sciences only the ordinal information in the dissimilarities is reliable and useful. But, since 1964, MDS has also been applied in molecular conformation, chemometrics, genetic sequencing, archelogical seriation, and in network design and location analysis. In these areas the numerical information in the dissimilarities is usually meaningful and should not be thrown out right away. Also, the use of the Shepard plot, with dissimilarities on the horizontal axis and fitted distances on the vertical axis, suggests there is more to dissimilarities than just their rank order.\n\n\n1.4.2 fstress\nInstead of defining the residuals in the least squares loss function as \\(\\delta_{ij}-d_{ij}(X)\\) chapter @ref(chrstress) discusses the more general cases where the residuals are \\(f(\\delta_{ij})-f(d_{ij}(X))\\) for some known non-negative increasing function \\(f\\). This defines the fstress loss function.\nIf \\(f(x)=x^r\\) with \\(r&gt;0\\) then fstress is called rstress. Thus stress is rstress with \\(r=1\\), also written as 1stress or \\(\\sigma_1\\). In more detail we will also look at \\(r=2\\), which is called sstress by Takane, Young, and De Leeuw (1977). In chapter @ref(chsstressstrain) we look at the problem of minimizing sstress and weighted version strain. The case of rstress with \\(r\\rightarrow 0\\) is also of interest, because it leads to the loss function in Ramsay (1977).\n\n\n1.4.3 Constraints\nInstead of minimizing stress over all \\(X\\) in \\(\\mathbb{R}^{n\\times p}\\) we will look in chapter @ref(cmds) at various generalizations where minimization is over a subset \\(\\mathcal{\\Omega}\\) of \\(\\mathbb{R}^{n\\times p}\\). This is often called Constrained Multidimensional Scaling or CMDS.\nThe distinction may be familiar from factor analysis, where we distinguish between exploratory and confirmatory factor analysis. If we have prior information about the parameters then incorporating that prior information in the analysis will generally lead to more precise and more interpretable estimates. The risk is, of course that if our prior information is wrong, if it is just prejudice, then we will have a solution which is precise but incorrect. We have the famous trade-off between bias and variance. In MDS this trade-off does not seem to apply directly, because the necessary replication frameworks are missing.\nand we do not attach much value to locating the true configuration.\n\\[\n\\min_{X\\in\\Omega}\\sigma(X)\n\\]\n\\[\n\\min_X\\sigma(X)+\\lambda\\kappa(X,\\Omega)\n\\] where \\(\\kappa(X,\\Omega)\\geq 0\\) and \\(\\kappa(X,\\Omega)=0\\) if and only if \\(X\\in\\Omega\\).\n\n\n1.4.4 Individual Differences\nNow consider the situation in which we have \\(m\\) different dissimilarity matrices \\(\\Delta_k\\) and \\(m\\) different weight matrices \\(W_k\\). We generalize basic MDS by defining \\[\\begin{equation}\n\\sigma(X_1,\\cdots,X_m):=\\frac12\\sum_{k=1}^m\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ijk}(\\delta_{ijk}-d_{ij}(X_k))^2,\n(\\#eq:replistress)\n\\end{equation}\\] and minimize this over the \\(X_k\\).\nThere are two simple ways to deal with this generalization. The first is to put no further constraints on the \\(X_k\\). This means solving \\(m\\) separate basic MDS problems, one for each \\(k\\). The second way is to require that all \\(X_k\\) are equal. As shown in more detail in section @ref(indifrepl) this reduced to a single basic MDS problem with dissimilarities that are a weighted sum of the \\(\\Delta_k\\). So both these approaches do not really bring anything new.\nMinimizing @ref(eq:replistress) becomes more interesting if we constrain the \\(X_k\\) in various ways. Usually this is done by making sure they have a component that is common to all \\(k\\) and a component that is specific or unique to each \\(k\\). This approach, which generalizes constrained MDS, is discussed in detail in chapter @ref(chindif).\n\n\n1.4.5 Distance Asymmetry\nWe have seen in section @ref(datasym) of this chapter that in basic MDS the assumption that \\(W\\) and \\(\\Delta\\) are symmetric and hollow can be made without loss of generality. The simple partitioning which proved this was based on the fact that \\(D(X)\\) is always symmetric and hollow. By the way, the assumption that \\(W\\) and \\(D\\) are non-negative cannot be made without loss of generality, as we will see below.\nIn chapter @ref(asymmds) we relax the assumption that \\(D(X)\\) is symmetric (still requiring it to be non-negative and hollow). This could be called Asymmetric MDS, or AMDS. I was reluctant at first to include this chapter, because asymmetric distances do not exist. And certainly are not Euclidean distances, so they are not covered by the title of this book. But as long as we stay close to Euclidean distances, least squares, and the smacof approach, I now feel reasonably confident the chapter is not too much of a foreign body.\nWhen Kruskal introduced gradient-based methods to minimize stress he also discussed the possibility to use Minkovski metrics other than the Euclidean metric. This certainly was part of the appeal of the new methods, in fact it seemed as if the gradient methods made it possible to use any distance function whatsoever. This initial feeling of empowerment was somewhat naive, because it ignored the seriousness of the local minimum problem, the combinatorial nature of one-dimensional and city block scaling, the problems with nonmetric unfolding, and the problematic nature of gradient methods if the distances are not everywhere differentiable. All these complications will be discussed in this book. But it made me decide to ignore Minkovski distances (and hyperbolic and elliptic non-Euclidean distances), because life with stress is complicated and challenging enough as it is.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#models-and-techniques",
    "href": "intro.html#models-and-techniques",
    "title": "1  Introduction",
    "section": "1.5 Models and Techniques",
    "text": "1.5 Models and Techniques\nTruth, error\n\n\n\n\nCarroll, J. D., and J. J. Chang. 1970. “Analysis of Individual Differences in Multidimensional scaling via an N-way generalization of \"Eckart-Young\" Decomposition.” Psychometrika 35: 283–319.\n\n\nCoombs, C. H. 1964. A Theory of Data. Wiley.\n\n\nDatorro, J. 2018. Convex Optimization and Euclidean Distance Geometry. Second Edition. Palo Alto, CA: Meebo Publishing. https://ccrma.stanford.edu/~dattorro/0976401304.pdf.\n\n\nDe Leeuw, J. 1977. “Applications of Convex Analysis to Multidimensional Scaling.” In Recent Developments in Statistics, edited by J. R. Barra, F. Brodeau, G. Romier, and B. Van Cutsem, 133–45. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\n———. 2017a. “Shepard Non-metric Multidimensional Scaling.” 2017. https://jansweb.netlify.app/publication/deleeuw-e-17-e/deleeuw-e-17-e.pdf.\n\n\n———. 2017b. “Tweaking the SMACOF Engine.” 2017.\n\n\nDe Leeuw, J., and W. J. Heiser. 1977. “Convergence of Correction Matrix Algorithms for Multidimensional Scaling.” In Geometric Representations of Relational Data, edited by J. C. Lingoes, 735–53. Ann Arbor, Michigan: Mathesis Press.\n\n\n———. 1980. “Multidimensional Scaling with Restrictions on the Configuration.” In Multivariate Analysis, Volume V, edited by P. R. Krishnaiah, 501–22. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\n———. 1982. “Theory of Multidimensional Scaling.” In Handbook of Statistics, Volume II, edited by P. R. Krishnaiah and L. Kanal. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\nDe Leeuw, J., and P. Mair. 2009. “Multidimensional Scaling Using Majorization: SMACOF in R.” Journal of Statistical Software 31 (3): 1–30. https://www.jstatsoft.org/article/view/v031i03.\n\n\nGroenen, P. J. F., and M. Van de Velden. 2016. “Multidimensional Scaling by Majorization: A Review.” Journal of Statistical Software 73 (8): 1–26. https://www.jstatsoft.org/index.php/jss/article/view/v073i08.\n\n\nGuttman, L. 1968. “A General Nonmetric Technique for Fitting the Smallest Coordinate Space for a Configuration of Points.” Psychometrika 33: 469–506.\n\n\nHarshman, R. A. 1970. “Foundations of the PARAFAC Procedure.” Working Papers in Phonetics 16. UCLA.\n\n\nHeiser, W. J., and J. De Leeuw. 1977. “How to Use SMACOF-I.” Department of Data Theory FSW/RUL.\n\n\nKruskal, J. B. 1964a. “Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesis.” Psychometrika 29: 1–27.\n\n\n———. 1964b. “Nonmetric Multidimensional Scaling: a Numerical Method.” Psychometrika 29: 115–29.\n\n\nMeulman, J. J., and W. J. Heiser. 2012. IBM SPSS Categories 21. IBM Corporation.\n\n\nRamsay, J. O. 1977. “Maximum Likelihood Estimation in Multidimensional Scaling.” Psychometrika 42: 241–66.\n\n\nSchoenberg, I. J. 1935. “Remarks to Maurice Frechet’s article: Sur la Definition Axiomatique d’une Classe d’Espaces Vectoriels Distancies Applicables Vectoriellement sur l’Espace de Hllbert.” Annals of Mathematics 36: 724–32.\n\n\nShepard, R. N. 1962a. “The Analysis of Proximities: Multidimensional Scaling with an Unknown Distance Function. I.” Psychometrika 27: 125–40.\n\n\n———. 1962b. “The Analysis of Proximities: Multidimensional Scaling with an Unknown Distance Function. II.” Psychometrika 27: 219–46.\n\n\nTakane, Y., F. W. Young, and J. De Leeuw. 1977. “Nonmetric Individual Differences in Multidimensional Scaling: An Alternating Least Squares Method with Optimal Scaling Features.” Psychometrika 42: 7–67.\n\n\nTorgerson, W. S. 1958. Theory and Methods of Scaling. New York: Wiley.\n\n\nTorgerson, W. S. 1952. “Multidimensional Scaling: I. Theory and Method.” Psychometrika 17 (4): 401–19.\n\n\n———. 1965. “Multidimensional Scaling of Similarity.” Psychometrika 30 (4): 379–93.\n\n\nYoung, G., and A. S. Householder. 1938. “Discussion of a Set of Points in Terms of Their Mutual Distances.” Psychometrika 3 (19-22).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "properties.html",
    "href": "properties.html",
    "title": "2  Properties of Stress",
    "section": "",
    "text": "2.1 Notation\nThe notation used in the \\(\\textrm{smacof}\\) approach to MDS first appeared in De Leeuw (1977), and was subsequently used in several of the later key smacof references, such as De Leeuw and Heiser (1982), De Leeuw (1988), chapter 8 of Borg and Groenen (2005), and De Leeuw and Mair (2009). We follow it in this book.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Properties of Stress</span>"
    ]
  },
  {
    "objectID": "properties.html#propnotation",
    "href": "properties.html#propnotation",
    "title": "2  Properties of Stress",
    "section": "",
    "text": "2.1.1 Expanding\nWe expand stress by writing out the squares of the residuals and then summing. Define\n\\[\\begin{align}\n\\eta_\\delta^2&:=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\delta_{ij}^2,(\\#eq:comps1)\\\\\n\\rho(X)&:=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\delta_{ij}d_{ij}(X),(\\#eq:comps2)\\\\\n\\eta^2(X)&:=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}d_{ij}^2(X).(\\#eq:comps3)\n\\end{align}\\]\nMore precisely, using conditional summation,\n\\[\\begin{align}\n\\rho(X)&:=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}\\left\\{w_{ij}\\delta_{ij}d_{ij}(X)\\mid w_{ij}\\delta_{ij}&gt;0\\right\\},(\\#eq:compszero1)\\\\\n\\eta^2(X)&:=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}\\left\\{w_{ij}d_{ij}^2(X)\\mid w_{ij}&gt;0\\right\\}(\\#eq:compszero2).\n\\end{align}\\]\nRemember that we have normalized by \\(\\eta_\\delta^2=1\\). With our newly defined functions \\(\\rho\\) and \\(\\eta^2\\) we can write stress as\n\\[\\begin{equation}\n\\sigma(X)=\\frac12(1+\\eta^2(X))-\\rho(X).\n(\\#eq:expand)\n\\end{equation}\\]\nThe CS inequality implies that for all \\(X\\)\n\\[\\begin{equation}\n\\rho(X)=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\delta_{ij}d_{ij}(X)\\leq\\eta_\\delta\\eta(X)=\\eta(X),\n(\\#eq:propcsrhoeta)\n\\end{equation}\\]\nand thus, from @ref(eq:expand),\n\\[\\begin{align}\n&\\frac12(1-\\eta(X))^2\\leq\\sigma(X)\\leq\\frac12(1+\\eta^2(X)),(\\#eq:propcssigeta1)\\\\\n&\\frac12(1-\\rho(X))^2\\leq\\sigma(X)\\leq\\frac12(1-2\\rho(X)).(\\#eq:propcssigeta2)\n\\end{align}\\]\n\n\n2.1.2 Matrix Expressions\nUsing matrix notation allows us to arrive at compact expressions, which suggest various mathematical and computational shortcuts. In order to use matrix notation for distances we mainly rely on the difference matrices \\(A_{ij}\\), which we now define.\n\nA unit vector \\(e_i\\) is a vector with element \\(i\\) equal to \\(+1\\) and all other elements equal to 0. A unit matrix \\(E_{ij}\\) is a matrix of the form \\(e_i^{\\ }e_j'\\),\nA diff matrix \\(A_{ij}\\) is a matrix of the form \\((e_i-e_j)(e_i-e_j)'\\).\n\nThe element in row \\(i\\) and column \\(j\\) of a matrix \\(X\\) is normally referred to as \\(x_{ij}\\). But in some cases, to prevent confusion, we use the notation \\(\\{X\\}_{ij}\\). Thus, for example, \\(\\{e_i\\}_j=\\delta^{ij}\\), where \\(\\delta^{ij}\\) is Kronecker’s delta (zero when \\(i=j\\) and one otherwise).\nThe diff matrices \\(A_{ij}\\) with \\(i\\not= j\\) have only four non-zero elements\n\\[\\begin{align}\n\\begin{split}\n\\{A_{ij}\\}_{ii}&=\\{A_{ij}\\}_{jj}=+1,\\\\\n\\{A_{ij}\\}_{ij}&=\\{A_{ij}\\}_{ji}=-1,\n\\end{split}\n(\\#eq:apaele)\n\\end{align}\\]\nand all other elements of \\(A_{ij}\\) are zero. Thus \\(A_{ij}=A_{ji}\\) and \\(A_{ii}=0\\). Diff matrices are symmetric, and positive semidefinite. They are also doubly-centered, which means that their rows and columns add up to zero. If \\(i\\not j\\) they are of rank one and have one eigenvalue equal to two, which means \\(A_{ij}^s=2^{s-1}A_{ij}\\). Also\n\\[\\begin{equation}\n\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n} A_{ij}=nI-ee'=nJ,\n(\\#eq:asum)\n\\end{equation}\\]\nwith \\(J\\) the centering matrix.\nWe begin our matrix expressions with \\(d_{ij}^2(X)=\\text{tr}\\ X'A_{ij}X\\). Define\n\\[\\begin{equation}\nV:=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}A_{ij},\n(\\#eq:vdef)\n\\end{equation}\\]\nso that\n\\[\\begin{equation}\n\\eta^2(X)=\\text{tr}\\ X'VX.\n(\\#eq:etav)\n\\end{equation}\\]\nThe matrix \\(V\\) has off-diagonal elements equal to \\(-w_{ij}\\) and diagonal elements \\(v_{ii}=\\sum_{j\\not= i} w_{ij}\\) It is symmetric, positive semi-definite, and doubly-centered. Thus it is singular, because \\(Ve=0\\).\nTo analyze the singularity of \\(V\\) in more detail we observe that \\(z'Vz=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(z_i-z_j)^2\\). This is zero if and only if all \\(w_{ij}(z_i-z_j)^2\\) are zero. If we permute the elements of \\(z\\) such that \\(z_1\\leq\\cdots\\leq z_n\\) then the matrix with elements \\((z_i-z_j)^2\\) can be partitioned such that the diagonal blocks, corresponding with tie-blocks in \\(z\\), are zero and the off-diagonal blocks are strictly positive. Thus \\(z'Vz=0\\) if and only if the corresponding off-diagonal blocks of \\(W\\) are zero. In other words, we can find a \\(z\\) such that \\(z'Vz=0\\) if and only if \\(W\\) is the direct sum of a number of smaller matrices. If this is not the case we call \\(W\\) irreducible, and \\(z'Vz&gt;0\\) for all \\(z\\not= e\\), so that the rank of \\(V\\) is \\(n-1\\).\nIf \\(W\\) is reducible the MDS problem separates into a number of smaller independent MDS problems. We will assume in the sequel, without any real loss of generality, that this does not occur, and that consequently \\(W\\) is irreducible.\nBecause of the singularity of the matrices involved we sometimes have to work with generalized inverses. We limit ourselves to the Moore-Penrose (MP) inverse, which can be defined in terms of the singular value decomposition. If the singular value decomposition is \\(X=K\\Lambda L'\\) with \\(K'K=L'L=I_r\\) and \\(\\Lambda\\) a positive definite diagonal matrix of order \\(r=\\text{rank}(X)\\), then the MP inverse of \\(X\\) is \\(X^+=L\\Lambda^{-1}K'\\).\nBecause of irreducibility the MP inverse of \\(V\\) is \\[\\begin{equation}\nV^+=(V+\\frac{ee'}{n})^{-1}-\\frac{ee'}{n}.\n(\\#eq:mpv)\n\\end{equation}\\] If all weights are equal, say to \\(w\\), then \\(V=nwJ\\) and \\(V^+=\\frac{1}{nw}J\\), with \\(J\\) the centering matrix \\(I-\\frac{1}{n}ee'\\).\nFinding an expression for \\(\\rho(X)\\) from @ref(eq:comps2) in matrix form is a bit more complicated. Define \\[\\begin{equation}\nr_{ij}(X):=\\begin{cases}0&\\text{ if }d_{ij}(X)=0,\\\\\n\\frac{\\delta_{ij}}{d_{ij}(X)}&\\text{ if }d_{ij}(X)&gt;0,\n\\end{cases}\n(\\#eq:rdef)\n\\end{equation}\\] and \\[\\begin{equation}\nB(X):=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}r_{ij}(X)A_{ij}.\n(\\#eq:bdef)\n\\end{equation}\\] Then we have \\[\\begin{equation}\n\\rho(X)=\\text{tr}\\ X'B(X)X.\n(\\#eq:rhob)\n\\end{equation}\\] Just like \\(V\\), the matrix-valued function \\(B\\) is symmetric, positive-semidefinite, and doubly-centered. If all dissimilarities and distances are positive then irreducibility of \\(W\\) implies that the rank of \\(B(X)\\) is equal to \\(n-1\\). Note that if \\(\\delta_{ij}=d_{ij}(X)&gt;0\\) for all \\(i,j\\) (perfect fit), then the \\(r_{ij}\\) from @ref(eq:rdef) are all equal to one, and \\(B(X)=V\\).\nIn @ref(eq:rdef) we have set \\(r_{ij}(X)=0\\) if \\(d_{ij}(X)=0\\). This is arbitrary. Since \\(b_{ij}(X)=r_{ij}(X)\\) if \\(d_{ij}(X)=0\\) we get a different matrix \\(B(X)\\) if we choose to set, say, \\(r_{ij}(X)=1\\) or \\(r_{ij}(X)=\\delta_{ij}\\) whenever \\(d_{ij}(X)=0\\). But \\[\\begin{equation}\nB(X)X=\\mathop{\\sum\\sum}_{1 \\leq i&lt;j\\leq n}w_{ij}r_{ij}(X)(e_i-e_j)(x_i-x_j)'\n(\\#eq:propbinvar)\n\\end{equation}\\] remains the same, no matter how we choose \\(r_{ij}(X)\\) for the \\(i&lt;j\\) with \\(d_{ij}(X)=0\\). And, consequently, \\(\\rho(X)=\\text{tr}\\ X'B(X)X\\) remains the same as well.\nWe now see, from equation @ref(eq:expand), that \\[\\begin{equation}\n\\sigma(X)=1-\\ \\text{tr}\\ X'B(X)X+\\frac12\\text{tr}\\ X'VX.\n(\\#eq:propmatexp)\n\\end{equation}\\]\n\n\n2.1.3 Coefficient Space\nObserve that we distinguish configuration space, which is the linear space \\(\\mathbb{R}^{n\\times p}\\) of \\(n\\times p\\) matrices, from the linear space \\(\\mathbb{R}^{np}\\) of \\(np\\) element vectors. The two spaces are isomorphic, and connected by the vec operator and its inverse.\nSome quick definitions. If \\(Y\\in\\mathbb{R}^{n\\times p}\\) is a configuration, then \\(\\text{vec}(Y)\\) is an \\(np\\)-element vector obtained by stacking the columns of \\(Y\\) on top of each other. Thus element \\((i,s)\\) of \\(Y\\) becomes element \\(i+(s-1)*n\\) of \\(\\text{vec}(Y)\\). If \\(Z=X+Y\\) in \\(\\mathbb{R}^{n\\times p}\\) then \\(\\text{vec}(Z)=\\text{vec}(X)+\\text{vec}(Y)\\) in \\(\\mathbb{R}^{np}\\), and if \\(Z=\\alpha Y\\) for some real number \\(\\alpha\\) then also \\(\\text{vec}(Z)=\\alpha\\text{vec}(Y)\\). Thus \\(\\text{vec}\\) is an isomorphism, and so is its inverse \\(\\text{vec}*{-1}\\), which transforms an \\(np\\)-element vector into an \\(n\\times p\\) matrix. In R we \\(\\text{vec}\\) a matrix by the as.vector function, which removes the dim attribute from the matrix, and we \\(\\text{vec}^{-1}\\) a vector by the matrix function, which adds the dim attribute to the vector.\nBut that is not all. Euclidean spaces are equipped with an inner product and a corresponding metric. The spaces \\(\\mathbb{R}^{n\\times p}\\) and \\(\\mathbb{R}^{np}\\) are also isometric inner product spaces. If \\(x\\) and \\(y\\) are in \\(\\mathbb{R}^{np}\\) then their inner product is \\[\n\\langle x, y\\rangle_{np}:=x'y=\\sum_{k=1}^{np}x_ky_k,\n\\] If \\(X\\) and \\(Y\\) are in \\(\\mathbb{R}^{n\\times p}\\) their inner product is \\[\n\\langle X,Y\\rangle_{n\\times p}:=\\text{tr}\\ X'Y=\\sum_{i=1}^{n}\\sum_{s=1}^px_{is}y_{is},\n\\] their lengths are \\(\\|X\\|=\\sqrt{\\text{tr}\\ X'X}\\) and \\(\\|Y\\|=\\sqrt{\\text{tr}\\ Y'Y}\\), and their distance is \\(\\|X-Y\\|\\). Now \\(\\langle x,y\\rangle=\\langle\\text{vec}(X),\\text{vec}(Y)\\rangle\\) and $|x-y|=|(X)-(Y)|.\nSome formulas in MDS are more easily expressed in \\(\\mathbb{R}^{np}\\) (see, for example, section @ref(propdiff)), but most of the time we prefer to work in the more intuitive space \\(\\mathbb{R}^{n\\times p}\\) of configurations (which is after all where our representations and pictures live).\nSuppose \\(Y_1,\\cdots,Y_r\\) are \\(r\\) linearly independent matrices in configuration space \\(\\mathbb{R}^{n\\times p}\\). We write \\(\\mathcal{Y}\\) for the \\(r\\)-dimensional subspace spanned by the basis \\(Y_1,\\cdots,Y_r\\). Of course if \\(r=np\\) then \\(\\mathcal{Y}=\\mathbb{R}^{n\\times p}\\).\nIf \\(X\\in\\mathcal{Y}\\) then there is a \\(\\theta\\) in coefficient space \\(\\mathbb{R}^r\\) such that \\(X=\\sum_{s=1}^r\\theta_s Y_s\\). We now parametrize basic MDS using the new variables \\(\\theta\\). Define\n\\[\\begin{equation}\n\\tilde d_{ij}^2(\\theta):=\\text{tr}\\ X'A_{ij}X=\\theta'\\tilde{A}_{ij}\\theta,\n(\\#eq:confpar)\n\\end{equation}\\]\nwith\n\\[\\begin{equation}\n\\{\\tilde A_{ij}\\}_{st}:=\\text{tr}\\ Y_s'A_{ij}Y_t.\n(\\#eq:conftildea)\n\\end{equation}\\]\nNow\n\\[\\begin{equation}\n\\tilde B(\\theta):=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\frac{\\delta_{ij}}{\\tilde d_{ij}(\\theta)}\\tilde A_{ij},\n(\\#eq:conftildeb)\n\\end{equation}\\]\nand\n\\[\\begin{equation}\n\\tilde V:=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\tilde A_{ij},\n(\\#eq:conftildev)\n\\end{equation}\\]\nand\n\\[\\begin{equation}\n\\tilde\\sigma(\\theta):=1-2\\tilde\\rho(\\theta)+\\tilde\\eta^2(\\theta)=1-2\\ \\theta'\\tilde B(\\theta)\\theta+\\theta'\\tilde V\\theta.\n(\\#eq:conftildesigma)\n\\end{equation}\\]\nFor the elements of \\(\\tilde B\\) and \\(\\tilde V\\) we see\n\\[\\begin{align}\n\\tilde b_{st}(\\theta)&=\\text{tr}\\ Y_s'B(X)Y_t,\\\\\n\\tilde v_{st}&=\\text{tr}\\ Y_s'VY_t.\n\\end{align}\\]\nMinimizing \\(\\sigma\\) over \\(X\\in\\mathcal{Y}\\) is now equivalent to minimizing \\(\\tilde\\sigma\\) over \\(\\theta\\in\\mathbb{R}^r\\).\nIf \\(\\mathcal{Y}=\\mathbb{R}^{n\\times p}\\) then, in a sense, this is just notational sleight of hand. Consider, for example, using the basis where the \\(Y_s\\) are the \\(np\\) matrices \\(e_i^{\\ }e_q'\\). Then\n\\[\\begin{equation}\n\\{\\tilde A_{ij}\\}_{kq,lv}:=\n\\delta^{qv}\\{A_{ij\\}_{kl}}\n(\\#eq:conftildecanon)\n\\end{equation}\\]\nUsing Kronecker products this can be written as \\(\\tilde A_{ij}=I_p\\otimes A_{ij}\\), the direct sum of \\(p\\) copies of \\(A_{ij}\\). Obviously if \\(\\theta=\\text{vec}(Y)\\) then \\(d_{ij}^2(Y)=\\theta'\\tilde A_{ij}\\theta\\). Also \\(\\tilde B(\\theta)=I_p\\otimes B(Y)\\) and \\(\\tilde V=I_p\\otimes V\\). The only thing that changes by moving from configuration space to coefficient space, using the canonical basis of \\(\\mathbb{R}^{n\\times p}\\), is that the configuration gets strung out to a vector, and the matrices \\(A_{ij}\\) get blown up to \\(p\\) copies of themselves.\nBut nevertheless it is clear that coefficient space allows us to use different bases as well, and allows us to use bases for proper subspaces of dimension \\(r&lt;np\\). This can be the \\(p(n-1)\\)-dimensional space of centered configurations, or the \\(np-\\frac12p(p+1)\\)- dimensional subspace of lower diagonal centered configurations. These configurations can be used to eliminate translational and rotational indeterminacy from basic MDS.\nBut the basis can also define a subspace of configurations with, for example, a rectangular lattice pattern, with the edges of the rectangle parallel to the horizontal and vertical axes (Borg and Leutner (1983)) or, for that matter, configurations \\(X\\) constrained to satisfy any number of (consistent) linear equality constraints. If \\(r&lt;np-\\frac12p(p+1)\\) then these applications are properly discussed as constrained multidimensional scaling or CMDS. A discussion of various forms of CMDS is in chapter @ref(cmds).\n\n\n2.1.4 Our Friends CS and AM/GM\nPerhaps the most frequently used mathematical results in this book are two elementary inequalities: the Cauchy-Schwartz and the Aritmetic-Geometric Mean inequalities. They are so important that we give them their own section in the book, their own acronyms CS and AM/GM, and we include their statements and even proofs.\n\nIf \\(x\\) and \\(y\\) are vectors in a Euclidean space \\(X\\) then \\(\\langle x,y\\rangle\\leq\\|x\\|\\|y\\|\\), with equality if and only if there is a real \\(\\alpha\\) such that \\(x=\\alpha y\\).\n\n\nProof. If \\(x=0\\) and/or \\(y=0\\) then obviously the result is trivially true. If \\(x\\not 0\\) and \\(y\\not= 0\\) then consider \\(f(\\alpha):=\\|x-\\alpha y\\|^2=\\|x\\|^2+\\alpha^2\\|y\\|^2-2\\alpha\\langle x,y\\rangle\\). Now \\[\\begin{equation}\n\\min_\\alpha f(\\alpha)=\\|x\\|^2-\\frac{\\langle x,y\\rangle^2}{\\|y\\|^2}\\geq 0.\n(\\#eq:csproof)\n\\end{equation}\\] It follows that \\(\\langle x,y\\rangle^2\\leq\\|x\\|^2\\|y\\|^2\\), which shows \\(-\\|x\\|\\|y\\|\\langle x,y\\rangle\\leq\\|x\\|\\|y\\|\\). We have \\(\\min_\\alpha f(\\alpha)=0\\) if and only if \\(x=\\alpha y\\) for some \\(\\alpha\\).\n\n\nIf \\(x\\) and \\(y\\) are two non-negative numbers, then \\(\\sqrt{xy}\\leq\\frac12(x+y)\\) with equality if and only if \\(x=y\\).\n\n\nProof. Follows directly from \\((\\sqrt(x)-\\sqrt(y))^2=x+y-2\\sqrt{xy}\\geq 0\\).\n\nThis can also be written as\n\nIf \\(x\\) and \\(y\\) are two non-negative numbers, then \\(xy\\leq\\frac12(x^2+y^2)\\) with equality if and only if \\(x=y\\).\n\nCombining CS and AM/GM gives\n\nIf \\(x\\) and \\(y\\) are vectors in a Euclidean space \\(X\\) then \\(\\langle x,y\\rangle\\leq\\frac12(\\|x\\|^2+\\|y\\|^2)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Properties of Stress</span>"
    ]
  },
  {
    "objectID": "properties.html#propglobal",
    "href": "properties.html#propglobal",
    "title": "2  Properties of Stress",
    "section": "2.2 Global Properties",
    "text": "2.2 Global Properties\n\n2.2.1 Boundedness\n\n\\(\\sigma\\) is bounded below by zero and unbounded above.\n\n\nProof. Stress is a sum of squares, and thus it is non-negative, i.e. bounded below by zero. Because \\(\\sigma(\\alpha X)=1-\\alpha\\rho(X)+\\frac12\\alpha^2\\eta^2(X)\\) we see that for each \\(X\\not= 0\\) and for each \\(K&lt;+\\infty\\) there is an \\(\\alpha\\) such that \\(\\sigma(\\alpha X)&gt;K\\).\n\n\n\n2.2.2 Invariance\n\nWe have the following invariances.\n\nRotational Invariance: \\(\\sigma(XK)=\\sigma(X)\\) for all \\(K\\) with \\(K'K=KK'=I\\).\nTranslational Invariance: \\(\\sigma(X+eu')=\\sigma(X)\\) for all \\(u\\in\\mathbb{R}^p\\).\nReflectional Invariance: \\(\\sigma(XK)=\\sigma(X)\\) for all diagonal \\(K\\) with \\(k_{ss}=\\pm 1\\).\nEvenness: \\(\\sigma(-X)=\\sigma(X)\\).\n\n\n\nProof. Stress only depends on the distances between the points in the configuration, and thus it is invariant under rigid geometrical transformations (rotations, reflections, and translations). Note that reflectional and evenness are actually special cases of rotational invariance.\n\nIt follows directly that the minimizer of stress, if it exists, cannot possibly be unique. Whatever the value at a minimum, it is shared by all rigid transformations of the configuration.\nIt also follows from translational invariance that we can minimize stress over the \\(p(n-1)\\) dimensional subspace of \\(\\mathbb{R}^{n\\times p}\\) of all \\(n\\times p\\) matrices which are centered, i.e. have \\(e'X=0\\). Rotational invariance implies we can also require without loss of generality that \\(X\\) is orthogonal, i.e. that \\(X'X\\) is diagonal. This studied in more detail in section #ref(propconfspace).\n\n\n2.2.3 Continuity\nA real-valued function \\(f\\) on an open subset \\(X\\) of a Euclidean space is Lipschitz or Lipschitz continuous if there is a \\(K\\geq 0\\) such that \\(|f(x)-f(y)|\\leq K\\|x-y\\|\\) for all \\(x\\) and \\(y\\) in \\(X\\). The smallest \\(K\\) for which this inequality holds is called the Lipschitz constant of \\(f\\). Lipschitz functions are uniformly continuous, and thus continuous. Lipschitz functions are almost everywhere differentiable, and where the derivative exists there is an \\(L\\geq 0\\) such that \\(\\|df(x)\\|\\leq L\\). Thus differentiable functions with an unbounded derivative are not Lipschitz.\nA function \\(f\\) is locally Lipschitz on \\(X\\) if for each \\(x\\in X\\) there is a open neighborhood \\(\\mathcal{N}(x)\\) such that \\(f\\) is Lipschitz on \\(\\mathcal{N}(x)\\). A locally Lipschitz function is continuous and almost everywhere differentiable. Continuously differentiable functions and convex functions are all locally Lipschitz.\n\nOn \\(\\mathbb{R}^{n\\times p}\\)\n\n\\(d_{ij}\\) is Lipschitz continuous with Lipschitz constant \\(\\sqrt{2}\\).\n\\(d_{ij}^2\\) is locally Lipschitz, but not globally Lipschitz.\n\n\n\nProof. To show that \\(d_{ij}\\) is Lipschitz we use the reverse triangle inequality \\(|\\|x\\|-\\|y\\||\\leq\\|x - y\\|\\). It gives\n\\[\\begin{equation}\n|d_{ij}(X)-d_{ij}(Y)|=\n|\\|X'(e_i-e_j)\\|-\\|Y'(e_i-e_j)\\||\n\\leq\\|(X-Y)'(e_i-e_j)\\|\\leq\\sqrt{2}\\ \\|X-Y\\|.\n(\\#eq:revtrian)\n\\end{equation}\\]\nTo show this Lipschitz bound is sharp use \\(Y=0\\) and \\(X=\\begin{bmatrix}\\hfill x\\\\-x\\end{bmatrix}\\) with \\(\\|x\\|=1\\). Then \\(|d(X)-d(Y)|=2\\) and \\(\\|X-Y\\|=\\sqrt{2}\\).\nBecause \\(d_{ij}^2\\) is continuously differentiable it is locally Lipschitz, and because its derivative is unbounded it is not globally Lipschitz.\n\n\nOn \\(\\mathbb{R}^{n\\times p}\\)\n\n\\(\\rho\\) is Lipschitz continuous with Lipschitz constant \\(\\sqrt{2}\\ \\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\delta_{ij}\\).\n\\(\\eta^2\\) and \\(\\sigma\\) are both locally Lipschitz, but not globally Lipschitz.\n\n\n\nProof. This follows directly from theorem @ref(thm:proplip).\n\n\n\n2.2.4 Coercivity\nStress is not a quadratic function, and not even a convex function, of the configuration. But it is like a bowl shaped around the origin, with some bumps and creases, in a way we are going to make more precise. First a definition: A real-valued function \\(f\\) is coercive if for every sequence \\(\\{x_k\\}\\) with \\(\\lim_{k\\rightarrow\\infty}\\|x_k\\|=\\infty\\) we also have \\(\\lim_{k\\rightarrow\\infty}f(x_k)=+\\infty\\).\n\n\\(\\sigma\\) is coercive.\n\n\nProof. From @ref(eq:propcssigeta1) we have \\(\\sigma(X)\\geq\\frac12(1-\\eta(X))^2\\). Now \\(\\eta\\) is clearly coercive, and thus \\(\\sigma\\) is coercive.\n\nIt follows from coercivity that all level sets of stress \\(\\mathcal{L}_s:=\\{X\\mid \\sigma(X)=s\\}\\) are compact, and that there is at least one configuration for which the global minimum of stress is attained (Ortega and Rheinboldt (1970), section 4.3).\nThe following theorem provides even more bowl-shapedness.\n\nIf \\(X\\not=0\\) then on the ray \\(\\{Y\\mid Y=\\alpha X\\text{ with }\\alpha\\geq 0\\}\\) stress is an unbounded convex quadratic in \\(\\alpha\\). The minimum of this quadratic is at \\(\\alpha=\\rho(X)/\\eta^2(X)\\) and it is equal to \\(1-\\frac12\\rho^2(X)/\\eta^2(X)\\). There is a local maximum at the boundary \\(\\alpha=0\\), equal to 1.\n\n\nProof. We have \\(\\sigma(\\alpha X)=1-\\alpha\\rho(X)+\\frac12\\alpha^2\\eta^2(X)\\). The statements in the theorem follow easily from this.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Properties of Stress</span>"
    ]
  },
  {
    "objectID": "properties.html#propdiff",
    "href": "properties.html#propdiff",
    "title": "2  Properties of Stress",
    "section": "2.3 Differentiability",
    "text": "2.3 Differentiability\nThe fact that \\(d_{ij}\\) can be zero for some configurations creates problems with the differentiability of stress. These problems have been largely ignored in the MDS literature, and there are indeed reasons why they are not of great practical importance (see section @ref(proplocmin) of this chapter), at least not in basic MDS. But for reasons of completeness, and for later generalizations of basic MDS, we discuss zero distances and the resulting problems with differentiability in some detail.\nHistorically the complications caused by \\(d_{ij}(X)=0\\) were one of the reasons why I switched from differentiability to convexity in De Leeuw (1977) and from derivatives to directional derivatives in De Leeuw (1984). It turned out that at least some of the important characteristics of the smacof algorithm, and several important aspects of stress surfaces, were better described by inequalities than by equations.\n### Directional Derivatives\nBecause we are dealing with minimization of stress, which is not everywhere differentiable, we use one-sided directional derivatives. Our notation largely follows Delfour (2012).\nThe first three directional derivatives at \\(X\\) in the direction \\(Y\\) are defined recursively by \\[\\begin{align}\nd_+\\sigma(X;Y)&:=\\lim_{\\epsilon\\downarrow 0}\\frac{\\sigma(X+\\epsilon Y)-\\sigma(X)}{\\epsilon},\n(\\#eq:ddd1stress)\\\\\nd_+^{(2)}\\sigma(X;Y,Y)&:=\\lim_{\\epsilon\\downarrow 0}\\frac{\\sigma(X+\\epsilon\\ Y)-\\sigma(X)-\\epsilon\\ d\\sigma(X)(Y)}{\\frac12\\epsilon^2},\n(\\#eq:ddd2stress)\\\\\nd_+^{(3)}\\sigma(X;Y,Y,Y)&:=\\lim_{\\epsilon\\downarrow 0}\\frac{\\sigma(X+\\epsilon\\ Y)-\\sigma(X)-\\epsilon\\ d_+\\sigma(X)(Y)-\\frac12\\epsilon^2\\ d_+^{(2)}\\sigma(X)(Y,Y)}{\\frac16\\epsilon^3},\n(\\#eq:ddd3stress)\n\\end{align}\\] where \\(\\epsilon\\downarrow 0\\) is understood as \\(\\epsilon\\) taking only strictly positive values in computing the limit, and where it is also understood that the one-sided limits exist. The directional derivatives used in optimization theory differ from the usual derivatives of analysis because the limits in functions that define them are over the one-dimensional positive real axis and are one-sided (from the right). You may wonder why we need to go as high as order three, but just you wait.\nNote that we write \\(d_+\\sigma(X;Y)\\) (with a semi-colon) instead of \\(d_+\\sigma(X,Y)\\) (with a comma) to emphasize the different roles of \\(X\\) and \\(Y\\). Also note that by “directional derivatives” we will always mean “one-sided directional derivatives”, because the two-sided ones are of limited usefulness in optimization. This is especially true for the two-sided directional derivative defined by \\[\\begin{equation}\nd\\sigma(X;Y):=\\lim_{\\epsilon\\rightarrow 0}\\frac{\\sigma(X+\\epsilon Y)-\\sigma(X)}{\\epsilon}.\n(\\#eq:twosided)\n\\end{equation}\\] The two-sided derivative may not exist, while we can still make useful statements of the minima of non-differentiable functions using the one-sided version. Of course for totally differentiable functions in the classical sense the two directional derivatives are equal.\nFor the higher directional derivatives we can also use alternative, and slightly more general, definitions that follow directly from the idea that the \\(k^{th}\\) directional derivative is the directional derivative of the \\((k-1)^{th}\\) one. In each step of the recursion we now use a different direction, instead of using the fixed direction \\(Y\\) in all steps. Thus \\[\\begin{align}\nd_+^{(2)}\\sigma(X;Y,Z)&:=\\lim_{\\epsilon\\downarrow 0}\\frac{d_+\\sigma(X+\\epsilon Z;Y)-d_+\\sigma(X;Y)}{\\epsilon},(\\#eq:altdd2)\\\\\nd_+^{(3)}\\sigma(X;Y,Z,U)&:=\\lim_{\\epsilon\\downarrow 0}\\frac{d_+^2\\sigma(X+\\epsilon U;Y,Z)-d_+^2\\sigma(X;Y,Z)}{\\epsilon}.(\\#eq:altdd3)\n\\end{align}\\]\nNote again that \\(d_+\\sigma(X)\\) is a function on \\(\\mathbb{R}^{n\\times p}\\), \\(d_+^2\\sigma(X)\\) a is a function on \\(\\mathbb{R}^{n\\times p}\\otimes\\mathbb{R}^{n\\times p}\\), and \\(d_+^3\\sigma(X)\\) is a function on \\(\\mathbb{R}^{n\\times p}\\otimes\\mathbb{R}^{n\\times p}\\otimes\\mathbb{R}^{n\\times p}\\).\nIf \\(\\sigma\\) is differentiable at \\(X\\) then \\(d_+\\sigma(X), d_+^{(2)}\\sigma(X),\\) and \\(d_+^{(3)}\\sigma(X)\\) are the usual first, second, and third derivatives of \\(\\sigma\\) at \\(X\\). In this differentiable case they are, respectively, a linear function, a symmetric bilinear function, and a super-symmetric trilinear function.\nWe can use the directional derivatives to expand \\(\\sigma(X+\\epsilon Y)\\) in powers of \\(\\epsilon\\). This gives an expansion of the form \\[\\begin{align}\n\\begin{split}\n\\sigma(X+\\epsilon Y)&=\\sigma(X)+\\epsilon d_+\\sigma(X)(Y)+\\frac12\\epsilon^2d^{(2)}_+\\sigma(X)(Y,Y)+\\\\&+\\frac16\\epsilon^2d_+{(3)}\\sigma(X)(Y,Y,Y)+\\text{o}(\\epsilon^3),\n\\end{split}(\\#eq:desexp)\n\\end{align}\\] where \\(\\text{o}(\\epsilon^3)\\) stand for any function of \\(\\epsilon&gt;0\\) such that \\[\\begin{equation}\n\\lim_{\\substack{\\epsilon\\downarrow 0\\\\\\epsilon\\not= 0}}\n\\frac{\\text{o}(\\epsilon^3)}{\\epsilon^3}\\rightarrow 0.\n(\\#eq:littleo)\n\\end{equation}\\]\n\n2.3.0.1 Distances\nLet us look at the directional differentiability of the distances \\(d_{ij}\\) themselves first. Since \\(\\rho\\) is a straightfoward weighted sum of distances and \\(\\eta^2\\) is a weighted sum of squared distances, the only directional derivatives we really need are those of the squared distances and the distances.\nThe problems with differentiability are clearly not caused by the squared distances, which form the \\(\\eta^2\\) component in equation @ref(eq:expand). The squared distance \\(d_{ij}^2(X)=\\text{tr}\\ X'A_{ij}X\\) is a quadratic function, and thus it is everywhere infinitely many times continuously differentiable.\nOn the other hand, \\(d_{ij}(X)=\\sqrt{\\text{tr}\\ X'A_{ij}X}\\) is not differentiable at points where \\(d_{ij}(X)=0\\), i.e. where \\(x_i=x_j\\), because the square root is not differentiable at zero. For MDS this means that if \\(d_{ij}(X)=0\\) for one or more \\((i,j)\\) with \\(w_{ij}\\delta_{ij}&gt;0\\) then both \\(\\rho\\) and \\(\\sigma\\) are not differentiable at \\(X\\).\nFor the avalanche of furmulas that will follow in this section it is convenient to define \\(c_{ij}(X,Y):=\\text{tr}\\ Y'A_{ij}X=(y_i-y_j)'(x_i-x_j)\\). Note that \\(c_{ij}(X,X)=d_{ij}^2(X)\\) and \\(c_{ij}(Y,Y)=d_{ij}^2(Y)\\). Now \\[\\begin{align}\nd_+d_{ij}^2(X;Y)&=2c_{ij}(X,Y),(\\#eq:ddddd1)\\\\\nd_+^2d_{ij}^2(X;Y,Z)&=2c{ij}(Y,Z),(\\#eq:ddddd2)\\\\\nd_+^3d_{ij}^2(X;Y,Z,U)&=0.(\\#eq:ddddd3)\n\\end{align}\\]\nMore involved calculations are needed for the directional derivatives of \\(d_{ij}\\). First the “problematic” case \\(d_{ij}(X)=0\\). We have \\[\\begin{align}\nd_+d_{ij}(X;Y)&=d_{ij}(Y),(\\#eq:dddzero1)\\\\\nd_+^2d_{ij}(X;Y)&=0,(\\#eq:dddzero2)\\\\\nd_+^3d_{ij}(X;Y)&=0.(\\#eq:dddzero3)\\\\\n\\end{align}\\]\nNote that \\(d_+d_{ij}(X)\\) is continuous but not linear in \\(Y\\), which also implies \\(d_{ij}\\) is not differentiable at \\(X\\). Also note that if we define the two-sided directional derivative of \\(d_{ij}\\) at \\(X\\) in the direction \\(Y\\) \\[\\begin{equation}\ndd_{ij}(X;Y):=\\lim_{\\epsilon\\rightarrow 0}\\frac{d_{ij}(X+\\epsilon Y)-d_{ij}(X)}{\\epsilon},\n(\\#eq:wronglim)\n\\end{equation}\\] then this limit only exists if also \\(d_{ij}(Y)=0\\). The limit from the right is \\(d_{ij}(Y)\\), but the limit from the left is \\(-d_{ij}(Y)\\). This illustrates that the two-sided directional derivative does not give much useful information on the behavior of the distance at zero.\nIf \\(d_{ij}(X)&gt;0\\) we have continuous differentiability of all orders at \\(X\\). To expand \\[\\begin{equation}\nd_{ij}(X+\\epsilon Y)=\\sqrt{d_{ij}^2(X)+2\\ \\epsilon\\ c_{ij}(X,Y)+\\epsilon^2\\ d_{ij}^2(Y)}\n(\\#eq:ddd1exp)\n\\end{equation}\\] we use the truncated Maclaurin series for the square root \\[\\begin{equation}\n\\sqrt{1+x}=1+\\frac12x-\\frac18x^2+\\frac{1}{16}x^3+\\text{o}(x^3).\n(\\#eq:maclaurin)\n\\end{equation}\\] For the distance this gives the series \\[\\begin{align}\n\\begin{split}\nd_{ij}(X+\\epsilon Y)&=d_{ij}(X)+\\epsilon\\frac{1}{d_{ij}(X)}c_{ij}(X,Y)+\\\\\n&+\\frac12\\epsilon^2\\frac{1}{d_{ij}(X)}\\left\\{d_{ij}^2(Y)-\\frac{c_{ij}^2(X,Y)}{d_{ij}^2(X)}\\right\\}+\\\\\n&-\\frac12\\epsilon^3\\frac{c_{ij}(X,Y)}{d_{ij}^3(X)}\\left\\{d_{ij}^2(Y)-\\frac{c_{ij}^2(X,Y)}{d_{ij}^2(X)}\n\\right\\}+\\text{o}(\\epsilon^3),\n\\end{split}(\\#eq:sqdexp)\n\\end{align}\\] and consequently \\[\\begin{align}\nd_+d_{ij}(X;Y)&=\\frac{1}{d_{ij}(X)}c_{ij}(X,Y),(\\#eq:expdfinal1)\\\\\nd_+^{(2)}d_{ij}(X;Y,Y)&=\\frac{1}{d_{ij}(X)}\\left\\{d_{ij}^2(Y)-\\frac{c_{ij}^2(X,Y)}\n{d_{ij}^2(X)}\\right\\},(\\#eq:expdfinal2)\\\\\nd_+^{(3)}d_{ij}(X;Y,Y,Y)&=-3\\frac{c_{ij}(X,Y)}{d_{ij}^3(X)}\\left\\{d_{ij}^2(Y)-\\frac{c_{ij}^2(X,Y)}{d_{ij}^2(X)}\\right\\}.(\\#eq:expdfinal3)\n\\end{align}\\]\nFormulas for the mixed directional derivatives from equations @ref(eq:altdd2) and @ref(eq:altdd3) are necessarily more complicated. Again assuming \\(d_{ij}(X)&gt;0\\) we find \\[\\begin{equation}\nd_+^{(2)}d_{ij}(X;Y,Z)=\\frac{1}{d_{ij}(X)}\\left\\{c_{ij}(Y,Z)-\\frac{c_{ij}(X,Y)c_{ij}(X,Z)}{d_{ij}^2(X)}\\right\\},\n(\\#eq:altddd2)\n\\end{equation}\\] which reduces to @ref(eq:expdfinal2) if \\(Y=Z\\). And, with \\(95\\%\\) certainty, \\[\\begin{align}\n\\begin{split}\nd_+^{(3)}d_{ij}(X;Y,Z,U)&=3\\frac{c_{ij}(X,Y)c_{ij}(X,Z)c_{ij}(X,U)}{d_{ij}^5(X)}+\\\\&-\\frac{c_{ij}(X,Y)c_{ij}(U,Y)+\nc_{ij}(X,Z)c_{ij}(U,Z)+c_{ij}(X,U)c_{ij}(Y,Z)}{d_{ij}^3(X)}\n\\end{split}\n(\\#eq:ddmix3)\n\\end{align}\\] which is obviously symmetric in \\(Y, Z,\\) and \\(U\\), and if \\(Y=Z=U\\) it reduces to @ref(eq:expdfinal3).\nAgain, we have to be careful if \\(d_{ij}(X)=0\\). In that case we know from equation @ref(eq:dddzero1) that \\(d_+d_{ij}(X;Y)=d_{ij}(Y)\\). Also \\[\\begin{equation}\nd_+d_{ij}(X+\\epsilon Z;Y)=\\begin{cases}d_{ij}(Y)&\\text{ if }d_{ij}(Z)=0,\\\\\n\\frac{1}{d_{ij}(Z)}c_{ij}(Y,Z)&\\text{ if }d_{ij}(Z)&gt;0,\n\\end{cases}\n(\\#eq:dmixedzero)\n\\end{equation}\\] and thus \\(d_+^{(2)}d_{ij}(X;Y,Z)=0\\) when \\(d_{ij}(Z)=0\\), but when \\(d_{ij}(Z)&gt;0\\) the limit defining \\(d_+^{(2)}d_{ij}(X;Y,Z)\\) does not exist unless \\(Z=Y\\). If \\(Z=Y\\) we have \\(d_+^{(2)}d_{ij}(X;Y,Y)=0\\), in accordance with @ref(eq:dddzero2).\n\n\n2.3.0.2 Rho and Stress\nWe now use the results from the previous section to compute directional derivatives of \\(\\rho\\) and \\(\\sigma\\). They are general, in the sense that they cover cases in which some \\(d_{ij}(X)&gt;0\\) and some \\(d_{ij}(X)=0\\). To handle one or more zero distances we define \\[\\begin{equation}\n\\xi(X;Y):=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}\\{w_{ij}\\delta_{ij}d_{ij}(Y)\\mid w_{ij}\\delta_{ij}&gt;0\\text{ and }d_{ij}(X)=0\\}.\n(\\#eq:xidef)\n\\end{equation}\\]\nThe directional derivatives of \\(\\rho\\) at \\(X\\) in direction \\(Y\\) are \\[\\begin{align}\nd_+\\rho(X;Y)&=\\text{tr}\\ Y'B(X)X+\\xi(X,Y),(\\#eq:rhoder1)\\\\\nd_+^{(2)}\\rho(X;Y,Y)&=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\frac{\\delta_{ij}}{d_{ij}(X)}\\left\\{d_{ij}^2(Y)-\\frac{c_{ij}^2(X,Y)}\n{d_{ij}^2(X)}\\right\\},(\\#eq:rhoders2)\\\\\nd_+^{(3)}\\rho(X;Y,Y,Y)&=-3\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\frac{\\delta_{ij}c_{ij}(X,Y)}{d_{ij}^3(X)}\\left\\{d_{ij}^2(Y)-\\frac{c_{ij}^2(X,Y)}{d_{ij}^2(X)}\\right\\}.(\\#eq:rhoders3)\n\\end{align}\\]\nNote that by the CS inequality \\[\\begin{equation}\nd_{ij}^2(Y)-\\frac{c_{ij}^2(X,Y)}{d_{ij}^2(X)}\\geq 0\n(\\#eq:posterm)\n\\end{equation}\\] for all \\(Y\\), and thus \\(d_+^{(2)}\\rho(X)(Y,Y)\\geq 0\\).\nThe directional derivatives for stress at \\(X\\) in direction \\(Y\\) are \\[\\begin{align}\nd_+\\sigma(X;Y)&=\\text{tr}\\ Y'(V-B(X))X-\\xi(X,Y),(\\#eq:stressders1)\\\\\nd_+^{(2)}\\sigma(X;Y,Y)&=\\text{tr}\\ Y'VY-\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\frac{\\delta_{ij}}{d_{ij}(X)}\\left\\{d_{ij}^2(Y)-\\frac{c_{ij}^2(X,Y)}\n{d_{ij}^2(X)}\\right\\},(\\#eq:stressders2)\\\\\nd_+^{(3)}\\sigma(X;Y,Y,Y)&=3\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\frac{\\delta_{ij}c_{ij}(X,Y)}{d_{ij}^3(X)}\\left\\{d_{ij}^2(Y)-\\frac{c_{ij}^2(X,Y)}{d_{ij}^2(X)}\\right\\}.(\\#eq:stressders3)\n\\end{align}\\]\nNote that we can also write @ref(eq:stressders2) as \\[\\begin{equation}\nd_+^{(2)}\\sigma(X;Y,Y)=\\text{tr}\\ Y'(V-B(X))Y+\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\frac{\\delta_{ij}}{d_{ij}(X)}\\frac{c_{ij}^2(X,Y)}\n{d_{ij}^2(X)}.\n(\\#eq:stressders2alt)\n\\end{equation}\\]\nCombining @ref(eq:stressders2) and @ref(eq:stressders2alt) shows \\[\\begin{equation}\n\\text{tr}\\ Y'(V-B(X))Y\\lesssim d_+^{(2)}\\sigma(X;Y,Y)\\lesssim\\text{tr}\\ Y'VY.\n(\\#eq:stressdes2ineq)\n\\end{equation}\\]\nA convenient upper bound for \\(d_+^{(3)}\\sigma(X;Y,Y,Y)\\) is also useful. From @ref(eq:stressders3) and the CS inequality \\[\\begin{equation}\nd_+^{(3)}\\sigma(X;Y,Y,Y)\\leq 3\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\frac{\\delta_{ij}|c_{ij}(X,Y)|}{d_{ij}^3(X)}d_{ij}^2(Y)\\leq 3\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\frac{\\delta_{ij}d_{ij}^3(Y)}{d_{ij}^2(X)}.\n(\\#eq:stressdes3ineq)\n\\end{equation}\\]\nOnce more, in the differentiable case the subscript + in \\(d_+\\) is not necessarily, but if one or more \\(d_{ij}(X)=0\\) for which \\(w_{ij}d_{ij}&gt;0\\) then the subscript is needed.\n\n\n2.3.0.3 expandStress\nThe R function {, with arguments w, delta, x, and y, gives the zeroeth, first, second, or third terms in the expansion @ref(eq:desexp), using the formulas for directional derivatives in this section. We use an example with \\(n=4\\) and \\(p=2\\). All weights and dissimilarities are equal. Configuration \\(X\\) are the four corners of a square, perturbation \\(Y\\) is a \\(4\\times 2\\) matrix of random standard normals.\nThe function takes an interval around zero for \\(\\epsilon\\) and computes the value of \\(\\sigma(X+\\epsilon Y)\\) at 1000 points in that interval. It also computes the zero, first, second, or third order Maclaurin approximations to \\(\\sigma(X+\\epsilon Y)\\). In this first example \\(X\\) is a local minimum and thus \\(d_+\\sigma(X;Y)=0\\). The zero-order and first-order approximation are thus the same.\nIf the interval for \\(\\epsilon\\) is \\([-1,1]\\) the sum of squares of the differences between \\(\\sigma(X+\\epsilon Y)\\) and its approximations of different orders are\n\n\n     [,1]                \n[1,] +917.455222294744431\n[2,] +917.455222294744885\n[3,] +468.774392806713536\n[4,] +185.377911517761618\n\n\nIf the interval is \\([-.1,.1]\\) the errors of approximation are\n\n\n     [,1]              \n[1,] +0.133635212939684\n[2,] +0.133635212939684\n[3,] +0.000152399828616\n[4,] +0.000001520085462\n\n\nAnd if the interval is \\([-.01,.01]\\) the errors of approximation are\n\n\n     [,1]              \n[1,] +0.000013434031631\n[2,] +0.000013434031631\n[3,] +0.000000000149172\n[4,] +0.000000000000015\n\n\nOn the smaller intervals the error of second-order approximation is already very small, which is not surprising because twice-differentiable functions are pretty much convex quadratics close to a local minimum.\n\n\n2.3.1 Partial Derivatives\nIn the differentiable case we now introduce gradients and Hessians. The gradient at \\(X\\) is a vector with first-order partial derivatives at \\(X\\), the Hessian is a matrix with second-order partial derivatives at \\(X\\). Partial derivatives are the commonly used tool for actual computation of derivatives.\nWe can obtain the partial derivatives from the directional derivatives in equations @ref(eq:stressders1), @ref(eq:stressders2), and @ref(eq:stressders3) by using the base vectors \\(E_{is}=e_i^{\\ }e_s'\\) to expand the perturbations \\(Y, Z,\\) and \\(U\\). So the gradient of stress at \\(X\\) is\n\\[\\begin{equation}\n\\{\\nabla\\sigma(X)\\}_{is}:=d_+\\sigma(X;E_{is})=\\text{tr}\\ e_ie_s'(V-B(X))X=\\{(V-B(X))X\\}_{is}.\n(\\#eq:nabla1stress)\n\\end{equation}\\] The gradient \\(\\nabla\\sigma(X)\\) is an \\(n\\times p\\) matrix, and \\[\\begin{equation}\nd_+\\sigma(X;Y)=\\text{tr}\\ Y'\\nabla\\sigma(X)=\\text{tr}\\ Y'(V-B(X))X.\n(\\#eq:ipform)\n\\end{equation}\\]\nFor the Hessian we use similar calculations, heavily relying on the Kronecker delta, and on cyclic permutations under the trace sign. First \\[\\begin{equation}\nc_{ij}(X,E_{ks})=\\text{tr}\\ X'(e_i-e_j)(e_i-e_j)'e_ks_s'=(x_{is}-x_{js})(\\delta^{ik}-\\delta^{jk}),\n(\\#eq:cxe1)\n\\end{equation}\\] and \\[\\begin{equation}\nc_{ij}(E_{ks},E_{lt})=\\text{tr}\\ e_te_l'(e_i-e_j)(e_i-e_j)'e_ke_s'=\\delta^{st}(\\delta^{ik}-\\delta^{jk})(\\delta^{il}-\\delta^{jl}).\n(\\#eq:ce1e2)\n\\end{equation}\\]\nThus, from equation @ref(eq:ddddd2), \\[\\begin{equation}\n\\{\\nabla^{(2)}d_{ij}^2(X)\\}_{ks,lt}=d_+^{(2)}d_{ij}^2(X;E_{ks},E_{lt})=2\\delta^{st}(\\delta^{ik}-\\delta^{jk})(\\delta^{il}-\\delta^{jl}),(\\#eq:nabdd)\n\\end{equation}\\] and from equation @ref(eq:altddd2) \\[\\begin{align}\n\\begin{split}\n\\{\\nabla^{(2)}d_{ij}(X)\\}_{ks,lt}&=d_+^{(2)}d_{ij}(X;E_{ks},E_{lt})=\\\\&=\\frac{(\\delta^{il}-\\delta^{jl})(\\delta^{ik}-\\delta^{jk})}{d_{ij}(X)}\\left\\{\\delta^{st}-\\frac{(x_{is}-x_{js})(x_{it}-x_{jt})}{d_{ij}^2(X)}\\right\\}.\n\\end{split}(\\#eq:nabd)\n\\end{align}\\]\nNow take the usual weighted sums of equations @ref(eq:nabdd) and @ref(eq:nabd) to find the Hessians of \\(\\rho\\) and \\(\\sigma\\). To get relatively compact expressions we define the symmetric doubly-centered matrices \\[\\begin{equation}\nH_{st}(X):=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\frac{\\delta_{ij}}{d_{ij}^3(X)}(x_{is}-x_{js})(x_{it}-x_{jt})A_{ij}.\n(\\#eq:defhmat)\n\\end{equation}\\] Then the Hessian of \\(\\rho\\) is \\[\\begin{equation}\n\\{\\nabla^{(2)}\\rho(X)\\}_{ks,lt}=\\{\\delta^{st}B(X)-H_{st}(X)\\}_{kl},\n(\\#eq:nabla2rho)\n\\end{equation}\\] and that of \\(\\sigma\\) is \\[\\begin{equation}\n\\{\\nabla^{(2)}\\sigma(X)\\}_{ks,lt}=\\{\\delta^{st}(V-B(X))+H_{st}(X)\\}_{kl}.\n(\\#eq:nabla2stress)\n\\end{equation}\\]\nThis is all somewhat inconvenient because of the double indexing of rows and columns. \\(\\nabla^{(2)}\\sigma(X)\\) is an element of \\(\\mathbb{R}^{n\\times p}\\otimes\\mathbb{R}^{n\\times p}\\) and we can represent it numerically either as a matrix or a four-dimensional array.\nTo cut the cord we use the \\(\\text{vec}\\) isomorphism from \\(\\mathbb{R}^{n\\times p}\\) to \\(\\mathbb{R}^{np}\\), and its inverse \\(\\text{vec}^{-1}\\).\nFor computational purposes you can think of \\(\\nabla^2\\sigma(X)\\) as an \\(np\\times np\\) matrix \\(K(X)\\), consisting of blocks of symmetric matrices of order \\(n\\), indexed by points, with \\(p\\) row-blocks and \\(p\\) column-blocks, indexed by dimensions. For \\(s\\not= t\\) block \\((s,t)\\) is the matrix \\(H_{st}(X)\\), the diagonal blocks for \\(s=t\\) are \\((V-B(X))+H_{ss}(X)\\). In the same way we can collect the blocks \\(H_{st}(X)\\) in the \\(np\\times np\\) matrix \\(H(X)\\).\n\\[\\begin{equation}\n\\{\\nabla^{(2)}\\sigma(X;Y)\\}_{ks}:=\\sum_{l=1}^n\\sum_{t=1}^p\\{\\nabla^{(2)}\\sigma(X)\\}_{ks,lt}y_{lt},\n(\\#eq:linform),\n\\end{equation}\\]\nand\n\\[\\begin{equation}\n\\text{tr}\\ Y'\\nabla^2\\sigma(X)Z:=d_+^2\\sigma(X;Y,Z)=\\sum_{k=1}^n\\sum_{s=1}^p\\sum_{l=1}^n\\sum_{t=1}^p\\{\\nabla^2\\sigma(X)\\}_{ks,lt}y_{ks}z_{lt}.\n(\\#eq:quadform)\n\\end{equation}\\]\nThus\n\\[\\begin{align}\n\\underline{\\nabla}^{(2)}\\rho(X)&=I_p\\otimes B(X)-\\underline{H}(X),(\\#eq:shorthess1)\\\\\n\\underline{\\nabla}^{(2)}\\sigma(X)&=I_p\\otimes(V-B(X))+\\underline{H}(X).(\\#eq:shorthess2)\n\\end{align}\\]\nI do not like to use vec and friends in formulas and derivations, so I try to avoid them. It is a different matter in computation, because in a computer \\(Y\\) is the same as \\(\\text{vec}(Y)\\) anyway.\nBecause the Hessian is important throughout the book we want to make sure we have the correct formulas and code. One way to check this is to compare it to the numerical approximation of the Hessian from the package numDeriv (Gilbert and Varadhan (2019)). Normally one checks if the code is correct by comparing it with the mathematics, but here we proceed the other way around.\nAgain we use the four corners of the square as an example, with weights and dissimilarities all equal. The largest absolute difference between the elements of the numerical and the analytical Hessian for ths example is 1.5404344^{-13}, which means that we basically have double-precision equality between the two. We repeat this for a random \\(X\\), because at a local minimum the approximation may be more precise. For the random configuration we find a maximum deviation of 0.2485061, a bit bigger, but still small.\nHere are some useful properties of the Hessians.\n\n\n\\(0\\lesssim H(X)\\lesssim I_p\\otimes B(X).\\)\n\\(I_p\\otimes(V-B(X))\\lesssim K(X)\\lesssim I_p\\otimes V.\\)\n\n\n\nProof. Let \\(y=\\text{vec}(Y)\\). Then \\[\ny'H(X)y=\\sum_{s=1}^p\\sum_{t=1}^p y_s'H_{st}(X)y_t=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\frac{\\delta_{ij}}{d_{ij}^3(X)}c_{ij}^2(X,Y)\\geq 0,\n\\] and thus \\(H(X)\\gtrsim 0\\). From () \\(\\nabla^2\\rho(X)\\gtrsim 0\\), and thus \\(I_p\\otimes B(X)\\gtrsim H(X)\\). This proves the first part. The second part is immediate from the first part and ().\n\nAnother useful property. Let \\(y=\\text{vec}(Y)\\).\n\nozo\n\n\nProof. \\[\\begin{equation}\nH(X)Y=\\sum_{t=1}^p H_{st}(X)y_t=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\frac{\\delta_{ij}c_{ij}(X,Y)}{d_{ij}^3(X)}(x_{is}-x_{js})(e_i-e_j).\n(\\#eq:eqhxy)\n\\end{equation}\\]\nIf \\(Y=X\\) then \\(H(X)X=B(X)X\\) and thus \\(\\nabla^2\\rho(X)X=0\\) and \\(\\nabla^2\\sigma(X)X=VX\\). If \\(Y=XT\\) with \\(T\\) anti-symmetric then \\(c_{ij}(X,Y)=\\text{tr}\\ X'A_{ij}XT=0\\) and thus \\(H(X)Y=0\\). This implies \\(\\nabla^2\\rho(X)Y=B(X)XT\\) and \\(\\nabla^2\\sigma(X)Y=(V-B(X))XT\\). If \\(B(X)X=VX\\) then \\(\\nabla^2\\rho(X)X=VX\\) and \\(\\nabla^2\\sigma(X)X=0\\). If \\(Y=e\\alpha'\\) then \\(\\nabla^2\\rho(X)Y=\\nabla^2\\sigma(X)Y=0\\).\n\nIn the example with the square the eigenvalues of \\(\\underline{H}(X)\\) are\n\n\n[1] +0.33333333 +0.19526215 +0.19526215 +0.19526215 +0.13807119 +0.00000000\n[7] +0.00000000 -0.00000000\n\n\n[1] +0.52960443 +0.44261066 +0.27431040 +0.15854524 +0.07386436 +0.00000000\n[7] -0.00000000 -0.00000000\n\n\nwhile those of \\(\\underline{\\nabla}^2\\sigma(X)\\) are\n\n\n[1] +0.33333333 +0.19526215 +0.13807119 +0.13807119 +0.13807119 +0.00000000\n[7] +0.00000000 -0.00000000\n\n\n[1] +0.33333333 +0.25946897 +0.17478810 +0.05902293 +0.00000000 +0.00000000\n[7] -0.10927733 -0.19627110\n\n\nWe can derive nicer expressions for the higher derivatives in coefficient space (see section @ref(propcoefspace)). This was done, perhaps for the first time, in De Leeuw (1993), which was actually written around 1985. In (kearsley_tapia_trosset_95?) Newton’s method was used to minimize stress, so presumably they implemented some formula for the Hessian. In De Leeuw (1988) the expression involving \\(H(X)\\) from @ref(eq:defhmat) was first given in configuration space. We could use similar computations to obtain the third-order partial derivatives, but for now we have no need for them in this book.\n\n\n2.3.2 Special Expansions\n\\(c_{ij}(X,Y)=0\\) for all \\(i&lt;j\\)\n\n2.3.2.1 Infinitesimal Rotations\nSuppose \\(Y=XT\\), with \\(T\\) antisymmetric, so that \\(X+\\epsilon Y=X(I+\\epsilon T)\\). Then \\(c_{ij}(X,Y)=0\\) for all \\(i&lt;j\\), and thus from equations @ref(eq:stressders1), @ref(eq:stressders2), and @ref(eq:stressders3)\n\\[\\begin{align}\nd_+\\sigma(X;Y)&=0,(\\#eq:stressdersa1)\\\\\nd_+^{(2)}\\sigma(X;Y,Y)&=\\text{tr}\\ Y'(V-B(X))Y,(\\#eq:stressdersa2)\\\\\nd_+^{(3)}\\sigma(X;Y,Y,Y)&=0.(\\#eq:stressdersa3)\n\\end{align}\\]\n\n\n2.3.2.2 Singularities\nSuppose \\(X=[\\underline{X}\\mid 0]\\) and \\(Y=[0\\mid\\underline{Y}]\\) so that \\(X+\\epsilon Y=[\\underline{X}\\mid\\epsilon\\underline{Y}]\\). Here \\(X\\) and \\(Y\\) are \\(n\\times p\\), \\(underline{X}\\) is \\(n\\times r\\), with \\(r&lt;p\\), and \\(\\underline{Y}\\) is \\(n\\times(p-r)\\). Then again \\(c_{ij}(X,Y)=0\\) for all \\(i&lt;j\\), and thus ()()() again.\n\\[\\begin{equation}\n\\sigma(\\underline{X}+\\epsilon\n\\underline{Y})=\\sigma(X)-\\epsilon\\sum_{d_{ij}(X)=0}w_{ij}\\delta_{ij}d_{ij}(Y)\n+\\frac12\\epsilon^2\\text{tr}\\ Y'(V-B(X))Y+o(\\epsilon^2)\n(\\#eq:exzeroes)\n\\end{equation}\\]\n\n\n2.3.2.3 Singularities\nSuppose \\(\\underline{X}=[X\\mid 0]\\) and \\(\\underline{Y}=[Z\\mid Y]\\) so that \\(\\underline{X}+\\epsilon\\underline{Y}=[X+\\epsilon Z\\mid\\epsilon Y]\\). Here \\(\\underline{X}\\) and \\(\\underline{Y}\\) are \\(n\\times p\\), \\(X\\) is \\(n\\times r\\), with \\(r&lt;p\\), and \\(Y\\) is \\(n\\times(p-r)\\). Then\n\\[\\begin{equation}\n\\sigma(\\underline{X}+\\epsilon\n\\underline{Y})=\\sigma(X)-\\epsilon\\sum_{d_{ij}(X)=0}w_{ij}\\delta_{ij}d_{ij}(Y)\n+\\frac12\\epsilon^2\\text{tr}\\ Y'(V-B(X))Y+o(\\epsilon^2)\n(\\#eq:exsingular)\n\\end{equation}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Properties of Stress</span>"
    ]
  },
  {
    "objectID": "properties.html#propconvex",
    "href": "properties.html#propconvex",
    "title": "2  Properties of Stress",
    "section": "2.4 Convexity",
    "text": "2.4 Convexity\nRemember that a function \\(f\\) on an open subset \\(X\\) of a Euclidean space is convex if for all \\(x\\) and \\(y\\) in \\(X\\) and \\(0\\leq\\alpha\\leq 1\\) we have \\(f(\\alpha x+(1-\\alpha)y)\\leq\\alpha f(x)+(1-\\alpha)f(y)\\). Thus on the line segment connecting \\(x\\) and \\(y\\) the function \\(f\\) is never above the line segment connecting \\(f(x)\\) and \\(f(y)\\). Convex functions are a.e. differentiable, in fact a.e. twice-differentiable. If the derivative exists at \\(x\\) then for all \\(y\\) we have \\(f(y)\\geq f(x)+df(x)(y-x)\\), which says the function majorizes its tangent plane at \\(x\\). If the second derivative exists at \\(x\\) then \\(d^2f(x;y,y)\\geq 0\\) for all \\(y\\), which says that the Hessian at \\(x\\) is positive semidefinite.\nStress is definitely not a convex function of the configuration. If it actually was convex, or even convex and differentiable, then this book would be much shorter. Nevertheless convexity still play an important part in our development of MDS, ever since De Leeuw (1977).\n\n2.4.1 Distances\nThe convexity in the MDS problem comes from the convexity of the distance and the squared distance. Although these are elementary facts, they are important in our context, so we give a proof.\n\nOn \\(\\mathbb{R}^{n\\times p}\\) both \\(d{ij}\\) and \\(d_{ij}^2\\) are convex.\n\n\nProof. First, for \\(0\\leq\\lambda\\leq 1\\), \\[\\begin{equation}\nd_{ij}^2(\\lambda X+(1-\\lambda)Y)\n=\\lambda^2d_{ij}^2(X) + (1-\\lambda)^2d_{ij}^2(Y)+2\\lambda(1-\\lambda)(x_i-x_j)'(y_i-y_j)\n(\\#eq:lbdcv1)\n\\end{equation}\\] By corollary @ref(cor:amgmcs), \\[\\begin{equation}\n(x_i-x_j)'(y_i-y_j)\\leq\\sqrt{d_{ij}^2(X)d_{ij}^2(Y)}\n\\leq\\frac12(d_{ij}^2(X)+d_{ij}^2(Y)).\n(\\#eq:lbdcv2)\n\\end{equation}\\] Combining @ref(eq:lbdcv1) and @ref(eq:lbdcv2) proves convexity of the squared distance.\nNow use equation @ref(eq:lbdcv1) and the CS inequality in the form \\((x_i-x_j)'(y_i-y_j)\\leq d_{ij}(X)d_{ij}(Y)\\). This gives \\[\\begin{equation}\nd_{ij}^2(\\lambda X +(1-\\lambda)Y)\\leq (\\lambda d_{ij}(X)+(1-\\lambda)d_{ij}(Y))^2.\n(\\#eq:lbdcv3)\n\\end{equation}\\] Taking square roots on both sides of equation @ref(eq:lbdcv3) proves convexity of the distance.\n\n\nBoth \\(\\rho\\) and \\(\\eta\\) are norms on \\(\\mathbb{R}^{n\\times p}\\).\n\n\nProof. homogeneous convex functions vanishing if and only if \\(X=0\\), which means they are both norms on \\(\\mathbb{R}^{n\\times p}\\).\n\n\n\n2.4.2 Subdifferentials\nSuppose \\(f\\) is a real-valued finite convex function on the finite-dimensional inner-product space \\(\\mathcal{E}\\). A subgradient of \\(f\\) at \\(x\\in\\mathcal{E}\\) is a vector \\(z\\in\\mathcal{E}\\) such that \\(f(y)\\geq f(x)+\\langle z,y-x\\rangle\\) for all \\(y\\in\\mathcal{E}\\). The set of all subgradients at \\(x\\) is the subdifferential at \\(x\\), written as \\(\\partial f(x)\\). In general, the subdifferential is a non-empty, closed, and convex set (rock). If \\(f\\) is differentiable at \\(x\\) then the subdifferential is the singleton which has the gradient \\(\\nabla f(x)\\) as its sole element (rock).\nApply this to \\(d_{ij}\\) on \\(\\mathbb{R}^{n\\times p}\\).\n\nThe subdifferential of \\(d_{ij}\\) at \\(X\\) is\n\nIf \\(d_{ij}(X)&gt;0\\) then \\(\\partial d_{ij}(X)=\\left\\{(e_i-e_j)\\frac{(x_i-x_j)'}{d_{ij}(X)}\\right\\}\\)\nIf \\(d_{ij}(X)=0\\) then \\(\\partial d_{ij}(X)=\\{Y\\mid Y=(e_i-e_j)z' \\text{ with } \\|z\\|\\leq 1\\}\\) \\[\\partial d_{ij}(X)=\\bigcup_{\\|z\\|\\leq 1}\\{(e_i-e_j)z'\\}\\]\n\n\n\nProof. If \\(d_{ij}(X)=0\\) we must find the set of all \\(Z\\in\\mathbb{R}^{n\\times p}\\) such that \\[\\begin{equation}\nd_{ij}(Y)\\geq\\text{tr}\\ Z'(Y-X)\n(\\#eq:subdifdefd)\n\\end{equation}\\] for all \\(Y\\in\\mathbb{R}^{n\\times p}\\).\nFirst of all @ref(eq:subdifdefd) must be true for all \\(Y=\\alpha X\\) with \\(\\alpha\\geq 0\\). Thus \\((\\alpha-1)\\text{tr}\\ Z'X\\leq 0\\) for all \\(\\alpha\\geq 0\\), which implies \\(\\text{tr}\\ Z'X=0\\). We can use this to simplify @ref(eq:subdifdefd) to \\(d_{ij}(Y)\\geq\\text{tr}\\ Z'Y\\) for all \\(Y\\). Next, it follows that \\(z_k=0\\) for \\(k\\not= i,j\\). If \\(z_k\\not= 0\\) choose \\(Y=e_kz_k'\\). Then \\(d_{ij}(Y)=0\\) and \\(\\text{tr} Z'Y=z_k'z_k&gt;0\\). Now @ref(eq:subdifdefd) simplifies to \\(d_{ij}(Y)\\geq z_i'y_i+z_j'y_j\\) If \\(y_i=z_i\\) and \\(y_j=0\\) then we must have \\(\\|z_i\\|\\geq\\|z_i\\|^2\\) or \\(\\|z_i\\|\\leq 1\\). In the same way \\(\\|z_j\\|\\leq 1\\). Choose \\(y_i=y_j=y\\) and some \\(y\\not= 0\\). Then we must have \\((z_i+z_j)'y\\leq 0\\) for all \\(y\\) and thus \\(z_i=-z_j\\). This proves the second part.\n\nBy the sum rule for convex subdifferentials (rock) \\[\\begin{equation}\n\\partial\\rho(X)=B(X)X+\n\\{Y\\mid Y=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}\\{w_{ij}\\delta_{ij}(e_i-e_j)z_{ij}'\\mid d_{ij}(X)=0\\}\\},\n(\\#eq:propsubdiffrho)\n\\end{equation}\\] where the \\(z_{ij}\\) are arbitrary vectors satisfying \\(\\|z_{ij}\\|\\leq 1\\|\\).\nOf course \\(\\partial d_{ij}^2(X)=\\{2A_{ij}X\\}\\) and thus \\(\\partial\\eta^2(X)=\\{2VX\\}\\).\nBut \\(\\sigma\\) is not convex, and we do not have a definition yet for the subdifferential of non-convex functions. We use the generalization introduced by Clarke. Suppose \\(f\\) is locally Lipschitz, and thus differentiable almost everywhere. Let \\(x^{(k)}\\) be a sequence of points converging to \\(x_\\infty\\), with\n\\(f\\) differentiable at all \\(x^{(k)}\\).\nThen \\(y\\) is in the Clarke subdifferential \\(\\partial_C^{\\ }f(x)\\) if and only if \\(y=\\lim_{k\\rightarrow\\infty}\\nabla f(x^{(k)})\\).\nClarke \\(\\partial_C^{\\ }\\sigma(X)=\\{VX\\}-\\partial\\rho(X)\\)\nCombining this with #ref(eq:propsubdiffrho) gives\n\\[\\begin{equation}\n\\partial\\sigma(X)=(V-B(X))X-\n\\{Y\\mid Y=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}\\{w_{ij}\\delta_{ij}(e_i-e_j)z_{ij}'\\mid d_{ij}(X)=0\\}\\},\n(\\#eq:propsubdiffstress)\n\\end{equation}\\]\n\n\n2.4.3 DC Functions\nIn basic MDS\n\n\\(\\rho\\) is a non-negative convex function, homogeneous of degree one.\n\\(\\eta^2\\) is a non-negative convex quadratic form, homogeneous of degree two.\n\\(\\sigma\\) is a non-negative difference of two convex functions.\n\nThis follows because \\(\\eta^2\\) is a weighted sum of squared distances and \\(\\rho\\) is a weighted sum of distances, both with non-negative coefficients, and thus they are both convex.\nReal-valued functions that are differences of two convex functions are also known as a DC functions or delta-convex functions. DC functions are important in optimization, especially in non-convex and global optimization. For excellent reviews of the various properties of DC functions, see Hiriart-Urruty (1988) or Bacak and Borwein (2011). Interesting for our purposes is that DC functions are almost everywhere twice differentiable, and that all two times continuously differentiable functions are DC.\nIt follows from the general properties of convex and DC functions that \\(\\sigma\\) is both uniformly continuous and locally Lipschitz, in fact Lipschitz on each compact subset of \\(\\mathbb{R}^{n\\times p}\\) (Rockafellar (1970), theorem 10.4). The fact that \\(\\sigma\\) is only locally Lipshitz is due entirely to the quadratic part \\(\\eta^2\\), because \\(\\rho\\) is globally Lipschitz.\n\n\n2.4.4 Negative Dissimilarities\nThere are perverse situations in which some weights and/or dissimilarities are negative (Heiser (1991)). Define \\(w_{ij}^+:=\\max(w_{ij},0)\\) and \\(w_{ij}^-:=-\\min(w_{ij},0)\\). Thus both \\(w_{ij}^+\\) and \\(w_{ij}^-\\) are non-negative, and \\(w_{ij}=w_{ij}^+-w_{ij}^-\\). Make the same decomposition of the \\(\\delta_{ij}\\).\nThen \\[\\begin{equation}\n\\rho(X)=\\sum (w_{ij}^+\\delta_{ij}^++w_{ij}^-\\delta_{ij}^-)d_{ij}(X)-\\sum(w_{ij}^+\\delta_{ij}^-+w_{ij}^-\\delta_{ij}^+)d_{ij}(X),\n(\\#eq:proprhoneg)\n\\end{equation}\\] and \\[\\begin{equation}\n\\eta^2(X)=\\sum w_{ij}^+d_{ij}^2(X)-\\sum w_{ij}^-d_{ij}^2(X).\n(\\#eq:propetaneg)\n\\end{equation}\\] Note that both \\(\\rho\\) and \\(\\eta^2\\) are no longer convex, but both are DC, and consequently so is \\(\\sigma\\).\nA bit more",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Properties of Stress</span>"
    ]
  },
  {
    "objectID": "properties.html#propstationary",
    "href": "properties.html#propstationary",
    "title": "2  Properties of Stress",
    "section": "2.5 Stationary Points",
    "text": "2.5 Stationary Points\n\nA function \\(f\\) has a global minimum on \\(X\\) at \\(\\hat x\\) if \\(f(\\hat x)\\leq f(x)\\) for all \\(x\\in X\\).\nA function \\(f\\) has a local minimum on \\(X\\) at \\(\\hat x\\) if there is a neighborhood \\(\\mathcal{N}\\) of \\(\\hat x\\) such that \\(f(\\hat x)\\leq f(x)\\) for all \\(x\\in\\mathcal{N}\\cap X\\).\nA function \\(f\\) has a singular point at \\(x\\) if it is not differentiable at \\(x\\).\nA function \\(f\\) has a stationary point at \\(x\\) if it is differentiable at \\(x\\) and \\(df(x)=0\\).\nA function \\(f\\) has a saddle point at a stationary point \\(x\\) if it is neither a local maximum nor a local minimum.\nGlobal and local maxima of \\(f\\) are global and local minima of \\(-f\\).\n\n\nAt a stationary point of stress we have \\(\\eta(X)\\leq 1\\).\n\n\nProof. If \\(X\\) is stationary we have \\(VX=B(X)X\\) and thus \\(\\rho(X)=\\eta^2(X)\\). Consequently \\(\\sigma(X)=1-2\\rho(X)+\\eta^2(X)=1-\\eta^2(X)\\) and because \\(\\sigma(X)\\geq 0\\) we see that \\(X\\) must be in the ellipse \\(\\{Z\\in\\mathbb{R}^{n\\times p}\\mid\\eta^2(Z)\\leq 1\\}\\).\n\nTheorem @ref(thm:statpointsbound) is important, because it means that we can require without loss of generality that \\(X\\) is in the ellipsoidal disk \\(\\eta(X)\\leq 1\\), which is a compact convex set.\n\n2.5.1 Local Maxima\n\nstress has a single local maximum at \\(X=0\\) with value 1.\n\n\nProof. At \\(X=0\\) we have for the one-sided directional derivative \\[\\begin{equation}\n\\mathbb{D}_+\\sigma(0,Y)=\\lim_{\\alpha\\downarrow 0}\\frac{\\sigma(0+\\alpha Y)-\\sigma(0)}{\\alpha}=-2\\rho(Y)\\leq 0,\n(\\#eq:datzero)\n\\end{equation}\\] which implies that stress has a local maximum at zero.\nTo show that the local maximum is unique suppose that there is a local maximum at \\(X\\not= 0\\). Then on the line through zero and \\(X\\) there should be a local maximum at \\(X\\) as well. But\n\\[\\begin{equation}\n\\sigma(\\alpha X)=1-2\\alpha\\rho(X)+\\alpha^2\\eta^2(X),\n(\\#eq:propnomax)\n\\end{equation}\\] is a convex quadratic, which consequently cannot have a local maximum at \\(X\\).\n\n\n\n2.5.2 Local Minima\nThe main result on local minima of stress is due to De Leeuw (1984). We give a slight strengthening of the result, along the lines of De Leeuw (2018), with a slightly simplified proof. Theorem @ref(thm:locmin) proves that a necessary condition for a local minimum at \\(X\\) is that for \\(i\\) and \\(j\\) with \\(w_{ij}\\delta_{ij}&gt;0\\) we have \\(d_{ij}(X)&gt;0\\), i.e. objects \\(i\\) and \\(j\\) are mapped to different points.\n\nIf stress has a local minimum at \\(X\\) then * \\(B(X)X=VX\\). * \\(d_{ij}(X)&gt;0\\) whenever \\(w_{ij}\\delta_{ij}&gt;0\\).\n\n\nProof. If stress has a local minimum at \\(X\\) then \\(d_+\\sigma(X;Y)\\geq 0\\) for all \\(Y\\). Equation @ref(eq:stressders1) tell us that \\[\\begin{equation}\nd_+\\sigma(X;Y)=\\text{tr}\\ Y'(V-B(X))X-\\mathop{\\sum\\sum}_{1\\\n\\leq i&lt;j\\leq n}\\{w_{ij}\\delta_{ij}d_{ij}(Y)\\mid d_{ij}(X)=0\\text{ and } w_{ij}\\delta_{ij}&gt;0\\}.\n(\\#eq:dirderagain)\n\\end{equation}\\]ain) \\end{equation} Consider a direction \\(Y\\) with all \\(d_{ij}(Y)&gt;0\\) and such that then \\(d_+\\sigma(X;Y)\\leq 0\\). This is always possible, because if we have and \\(Y\\) with \\(d_+\\sigma(X;Y)&gt;0\\) we simply switch to \\(-Y\\). Now \\(d_+\\sigma(X;Y)\\) in @ref(eq:dirderagain) is the sum of two terms which are both non-positive, and they satisfy \\(d_+\\sigma(X;Y)\\geq 0\\) if and only if they are both zero. For the second term this means that at a local minimum the summation is empty and there is no \\(d_{ij}(X)=0\\) whenever \\(w_{ij}\\delta_{ij}=0\\). For the first term it means that \\((V-B(X))X=0\\).\n\nDe Leeuw (1984) concluded that if \\(w_{ij}\\delta_{ij}&gt;0\\) for all \\(i&lt;j\\) then stress is differentiable at a local minimum. But more is true, because it s not necessary to require \\(w_{ij}\\delta_{ij}&gt;0\\) for this result.\n\nIf stress has a local minimum at \\(X\\) then it is differentiable at \\(X\\) and has \\(\\nabla\\sigma(X)=0\\).\n\n\nProof. At a local minimum we can indeed have \\(d_{ij}(X)=0\\) if \\(w_{ij}\\delta_{ij}=0\\). But if \\(w_{ij}=0\\) then stress does not depend on \\(d_{ij}(X)\\) at all, so \\(d_{ij}(X)=0\\) does not influence differentiability. If \\(w_{ij}&gt;0\\) and \\(\\delta_{ij}=0\\) then stress depends on \\(d_{ij}(X)\\) only through the term \\(w_{ij}d_{ij}^2(X)\\), which is differentiable even if \\(d_{ij}(X)=0\\).\n\nTheorem @ref(thm:locmin) and its corollary @ref(cor:locmindif) are the main reason why, at least in basic scaling, we can largely ignore the problems with differentiability. These results have been extended to least squares MDS with Minkovski distances by Groenen, Mathar, and Heiser (1995). In a neighborhood of each local minimum the loss function is differentiable, so eventually convergent descent algorithms do not have problems with non-differentiable ridges. This result is of major importance for both practical and theoretical reasons, as emphasized for example by Pliner (1996).\nSecond order necessary conditions (since differentiable at local minimum)\n\n\n2.5.3 Saddle Points\nAt a saddle point \\(X\\) stress is differentiable and \\(d\\sigma(X)=0\\). But there are directions of decrease and increase from \\(X\\), and thus stress does not have a local minimum there.\nIf \\(VX=B(X)X\\) and \\(d_{ij}(X)=0\\) for some \\(w_{ij}\\delta_{ij}&gt;0\\) then there is an \\(Y\\) such that \\(\\mathbb{D}_+\\sigma(X,Y)&lt;0\\).\nTheo Suppose \\(VX=B(X)X\\) then \\(V(X\\mid 0)=B(X|0)(X|0)\\)\nCorr Suppose \\(VX=B(X)X\\) and \\(X\\) is of rank \\(r&lt;p\\). Then \\(XL=(Z|0)\\) and thus \\(VZ=B(Z)Z\\).\nIf \\(VX=B(X)X\\) and \\(X\\) is singular then \\(X\\) is a saddle point.\n\n\n2.5.4 An Example\nLet’s look at a small example, already analyzed in many different places (e.g. De Leeuw (1988), Trosset and Mathar (1997)). It has four points, all dissimilarities to one and all weights are equal to \\(\\frac16\\).\n\n2.5.4.1 Regular Tetrahedron\nIn three dimensions the global minimum is equal to zero, with the points mapped into the vertices of a regular tetrahedron. Points can be assigned to the vertices in \\(4! = 24\\) ways, and each such assignment defines a global minimum. In fact each assigment defines a continuum of global minimizers, because all rotations of any of the regular tetrahedra also give global minimizers. It seems as if there are 24 rotation manifolds with global minimizers. But some of the 24 assigments of points to vertices are equivalent in the sense that they are rotations of each other (which includes reflections). It turns out there are three equivalence classes of eight assignments each. Thus there are three different disjoint rotation manifolds of global minimizers, not 24.\nThe stress value for any regular tetrahedron is zero, and the eigenvalues of the Hessian are\n\n\n [1] +0.333333 +0.166667 +0.166667 +0.166667 +0.083333 +0.083333 +0.000000\n [8] +0.000000 +0.000000 +0.000000 -0.000000 -0.000000\n\n\nFor four points in three dimensions we have \\(np-\\frac12p(p+1)=6\\) non-trivial eigenvalues. Since the six largest values are all positive our regular tetrahedra are isolated, in the sense that if we move away from the each of the corresponding rotation manifolds the stress increases.\n\n\n2.5.4.2 Singularity\nThe next stationary point is somewhat deviously constructed. We take four points equally spaced on a line (a stationary point in one dimension) and add two zero dimensions. Then we do a random rotation of configuration to somwhat hide its singularity. We already know this configuration defines a saddle point in three-dimensional configuration space. For the resulting configuration the stress is 0.083333, and the eigenvalues of the Hessian are\n\n\n [1] +0.333333 +0.333333 +0.333333 +0.000000 -0.000000 -0.000000 -0.000000\n [8] -0.000000 -0.166667 -0.166667 -0.277778 -0.277778\n\n\nThere are four negative eigenvalues, and thus we confirm we have a saddlepoint.\n\n\n2.5.4.3 Equilateral Triangle with Centroid\nThe next configuration is an interesting one. It is in two-dimensions, with three points in the corners of an equilateral triangle, and a fourth point in the centroid of the first three. The 24 assigments of the four points to the four positions in thus case give four rotational equivalence classes of six assignments each. This particular arrangements has four disjoint rotational manifolds. The stress is 0.033494, and the eigenvalues of the Hessian are\n\n\n[1] +0.333333 +0.255983 +0.255983 +0.000000 +0.000000 +0.000000 +0.000000\n[8] -0.000000\n\n\nNote that the Hessian is positive semi-definite, with three positive eigenvalues. This made De Leeuw (1988) think that this arrangement of points defined a non-isolated local minimum. Bad mistake. Since \\(3 &lt; np - \\frac12p(p+1) = 5\\) the singular Hessian does not guarantee that we have a local minimum. Trosset and Mathar (1997) showed, using symbolic computations, that the configuration and its permutations and rotations defines a family of saddle points. Further on in this section we will look in more detail what happens in this case.\n\n\n2.5.4.4 Square\nFour points in the corners of a square give the global minimum in two dimensions (De Leeuw and Stoop (1984)). There are three isolated rotational manifolds for such squares, all with stress 0.014298, and with eigenvalues of the Hessian\n\n\n[1] +0.333333 +0.195262 +0.138071 +0.138071 +0.138071 +0.000000 +0.000000\n[8] +0.000000\n\n\nIn one dimension the global minimum is attained for four points equally spaced on a line. Thus there are \\(n!\\) different global minimizers but by reflection only \\(\\frac12(n!)\\) give different distances.\n\n\n2.5.4.5 Non-global Local Minima\nIt turns out be be difficult in our example to find non-global local minima. In an heroic effort we looked at 100,000 smacof runs with a random start to find other local minima. In 9.9996^{4} cases smacof converges to the square, in 4 it stops at the equilateral triangle with center, but only because the limit on the number of iterations (1000) is reached. This confirms the computational results reported by De Leeuw (1988) and Trosset and Mathar (1997). It also confirms the theoretical\nresult that gradient descent algorithms with random starts almost surely avoid saddle points and converge to local minima (Lee et al. (2016)), although avoiding the saddle points may take exponential time (Du et al. (2017)). In any case, it seems safe to conjecture that for our small and maximally symmetric example all local minima are global.\nTrosset and Mathar (1997), in their search for non-global local minima, consequently are forced to use another example. They used equal weights, but choose the dissimilarities as the Euclidean distances between the four corners of a square, ordered counterclockwise from the origin. Thus the global minimum of stress is zero. In 1000 smacof runs with random start we find a this zero local minimum 599 times, while we converge 401 times to another stationary point with stress 0.0334936.\n\n\n\n\n\nTrosset/Mathar Configurations\n\n\n\n\nThe two configurations are plotted in figure @ref(fig:tmzplot), with the global minimizer in red. The non-global configuration (in blue) is rotated to best least squares fit with the first one, using simple Procrustus (Gower and Dijksterhuis (2004)). Note that it is a rectangle, but not a square. The eigenvalues of the Hessian at the non-global minimum configuration are\n\n\n[1] +0.333333 +0.211325 +0.122008 +0.122008 +0.122008 +0.000000 +0.000000\n[8] +0.000000\n\n\nverifying that we indeed have an isolated local minimum. Trosset and Mathar (1997) verify this using a mix of symbolic and floating point calculation.\nWe can generate an additional example using the function equalDelta() in equaldelta.R. Its arguments are \\(n, p, m\\), where \\(n\\) is the order of the dissimilarity and weight matrices, which have all their non-diagonal elements equal. Argument \\(n\\) and \\(p\\) define the space of configuration matrices, and \\(m\\) is the number of smacof runs with a random start.\n\n\n[1] 658\n\n\n[1] 342\n\n\nitel      1  eiff    0.0000000000  sold    0.0381249159  snew    0.0381249159  \nitel      2  eiff    0.0000000000  sold    0.0381249159  snew    0.0381249159  \nitel      3  eiff    0.0000000000  sold    0.0381249159  snew    0.0381249159  \nitel      4  eiff    0.0000000000  sold    0.0381249159  snew    0.0381249159  \n\n\nitel      1  eiff    0.0000000000  sold    0.0357265590  snew    0.0357265590  \nitel      2  eiff    0.0000000000  sold    0.0357265590  snew    0.0357265590  \n\n\n [1] +0.200000 +0.129354 +0.129354 +0.102242 +0.102242 +0.079940 +0.079940\n [8] +0.005686 +0.005686 -0.000000 -0.000000 -0.000000\n\n\n [1] +0.200000 +0.114739 +0.114739 +0.107180 +0.073205 +0.073205 +0.051287\n [8] +0.051287 +0.039230 +0.000000 +0.000000 -0.000000\n\n\n\n\n2.5.4.6 Directions of Descent\nWe now go back to the stationary equilateral triangle with center. We have seen that the gradient at this configuration is zero and the Hessian is positive semi-definite but rank-deficient. A descent direction at \\(X\\) is any configuration \\(Y\\) such that \\(\\sigma(X+\\epsilon Y)&lt;\\sigma(X)\\) if \\(\\epsilon\\) is small enough. In our example, with \\(X\\) the triangle with center, we must choose \\(Y\\) in the null space of the Hessian, because otherwise \\(Y\\) is a direction of accent. The null space has two trivial dimensions, \\(X\\) and \\(XA\\) with \\(A\\) anti-symmetric. The non-trivial null space has dimension three, and we choose a basis of three orthonormal directions. Then\n\\[\n\\sigma(X+\\epsilon Y)=\\sigma(X)+0+0+\\frac16\\epsilon^3d^3\\sigma(X)(Y,Y,Y)+o(\\epsilon^3),\n\\] and we can find a descent direction if \\(d^3\\sigma(X)(Y,Y,Y)\\not= 0\\).\n\n\n[1] +0.066987 -0.000000 +0.000000 -0.186030\n\n\n[1] +0.066987 +0.000000 +0.000000 +0.293283\n\n\n[1] +0.066987 -0.000000 +0.000000 -0.117607\n\n\n\n\n\n\nBacak, M., and J. M. Borwein. 2011. “On Difference Convexity of Locally Lipschitz Functions.” Optimization 60 (8-9): 961–78.\n\n\nBorg, I., and P. J. F. Groenen. 2005. Modern Multidimensional Scaling. Second Edition. Springer.\n\n\nBorg, I., and D. Leutner. 1983. “Dimensional Models for the Perception of Rectangles.” Perception and Psychophysics 34: 257–69.\n\n\nDe Leeuw, J. 1977. “Applications of Convex Analysis to Multidimensional Scaling.” In Recent Developments in Statistics, edited by J. R. Barra, F. Brodeau, G. Romier, and B. Van Cutsem, 133–45. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\n———. 1984. “Differentiability of Kruskal’s Stress at a Local Minimum.” Psychometrika 49: 111–13.\n\n\n———. 1988. “Convergence of the Majorization Method for Multidimensional Scaling.” Journal of Classification 5: 163–80.\n\n\n———. 1993. “Fitting Distances by Least Squares.” Preprint Series 130. Los Angeles, CA: UCLA Department of Statistics. https://jansweb.netlify.app/publication/deleeuw-r-93-c/deleeuw-r-93-c.pdf.\n\n\n———. 2018. “Differentiability of Stress at Local Minima.” 2018.\n\n\nDe Leeuw, J., and W. J. Heiser. 1982. “Theory of Multidimensional Scaling.” In Handbook of Statistics, Volume II, edited by P. R. Krishnaiah and L. Kanal. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\nDe Leeuw, J., and P. Mair. 2009. “Multidimensional Scaling Using Majorization: SMACOF in R.” Journal of Statistical Software 31 (3): 1–30. https://www.jstatsoft.org/article/view/v031i03.\n\n\nDe Leeuw, J., and I. Stoop. 1984. “Upper Bounds for Kruskal’s Stress.” Psychometrika 49: 391–402.\n\n\nDelfour, M. C. 2012. Introduction to Optimization and Semidifferential Calculus. SIAM.\n\n\nDu, S., C. Jin, J. D. Lee, M. I. Jordan, B. Póczos, and A. Singh. 2017. “Gradient Descent Can Take Exponential Time to Escape Saddle Points.” In NIPS’17: Proceedings of the 31st International Conference on Neural Information Processing Systems, 1067—1077.\n\n\nGilbert, P., and R. Varadhan. 2019. numDeriv: Accurate Numerical Derivatives. https://CRAN.R-project.org/package=numDeriv.\n\n\nGower, J. C., and G. B. Dijksterhuis. 2004. Procrustus Problems. Oxford University Press.\n\n\nGroenen, P. J. F., R. Mathar, and W. J. Heiser. 1995. “The Majorization Approach to Multidimensional Scaling for Minkowski Distances.” Journal of Classification 12: 3–19.\n\n\nHeiser, W. J. 1991. “A Generalized Majorization Method for Least Squares Multidimensional Scaling of Pseudodistances that May Be Negative.” Psychometrika 56 (1): 7–27.\n\n\nHiriart-Urruty, J.-B. 1988. “Generalized Differentiability / Duality and Optimization for Problems Dealing with Differences of Convex Functions.” In Convexity and Duality in Optimization, edited by Ponstein. J., 37–70. Lecture Notes in Economics and Mathematical Systems 256. Springer.\n\n\nLee, J. D., M. Simchowitz, M. I. Jordan, and B. Recht. 2016. “Gradient Descent Converges to Minimizers.”\n\n\nOrtega, J. M., and W. C. Rheinboldt. 1970. Iterative Solution of Nonlinear Equations in Several Variables. New York, N.Y.: Academic Press.\n\n\nPliner, V. 1996. “Metric Unidimensional Scaling and Global Optimization.” Journal of Classification 13: 3–18.\n\n\nRockafellar, R. T. 1970. Convex Analysis. Princeton University Press.\n\n\nTrosset, M. W., and R. Mathar. 1997. “On the Existence on Nonglobal Minimizers of the STRESS Criterion for Metric Multidimensional Scaling.” In Proceedings of the Statistical Computing Section, 158–62. Alexandria, VA: American Statistical Association.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Properties of Stress</span>"
    ]
  },
  {
    "objectID": "spaces.html",
    "href": "spaces.html",
    "title": "3  Stress Spaces",
    "section": "",
    "text": "3.1 Configuration Space\nSo far we have defined stress on \\(\\mathbb{R}^{n\\times p}\\), the space of all matrices with \\(n\\) rows and \\(p\\) columns. We call this configuration space. The usual MDS representation of a dissimilarity matrix is a scatterplot of \\(n\\) points in \\(p\\) dimensions. Therefor there is little doubt that configuration space is the most natural MDS space, and most of our theorems and derivations use this space.\nNevertheless, configuration space has some disadvantages. Even for \\(n\\) as small as four and \\(p\\) as small as two the dimension of the space of configurations is eight, and there is no compelling way to visualize stress as a function of eight variables. Secondly, if we work in configuration space we have to keep the indeterminacies of the representation in mind. Because of translational indeterminacy, minimizing stress over \\(X\\in\\mathbb{R}^{n\\times p}\\) will give the same result as minimizing stress over \\(X\\in\\mathbb{R}_C^{n\\times p}\\), the \\(p(n-1)\\)-dimensional subspace of all column-centered matrices. Because of translational and rotational indeterminacy it will also give the same result as minimizing stress over \\(X\\in\\mathbb{R}_{CT}^{n\\times p}\\), the \\(np-\\frac12p(p+1)\\) dimensional subspace of all centered lower triangular configurat ions (which have \\(x_{ij}=0\\) for all \\(j&gt;i\\)). Or the same result as minimizing stress over the \\(np-\\frac12p(p+1)\\) dimensional nonlinear manifold \\(\\mathbb{R}_{CO}^{n\\times p}\\) of all centered orthogonal configurations. Especially rotational indeterminacy complicates our analysis of MDS algorithms that operate on configuration space.\nAnother disadvantage of configuration space is that it needlessly complicates some of our formulas and derivations. Not all pairs of coordinate in a configuration \\(X\\) have the same status. Some pairs belong to the same row of the matrix, and some pairs to different rows. Some pairs of coordinates are in the same column, and some are in different columns. This complicates formulas which depend on considering pairs of coordinates, such as second derivatives.\nOne way to make a picture in configuration space is to plot the \\(np\\) individual coordinates as functions of a one-dimensional perturbation. In figure @ref(fig:piccoorc) we illustrate this with an example. We have four objects, with all weights and dissimilarities equal, and the \\(4\\times 2\\) configuration is an equilateral triangle together with its centroid. Everything is suitably scaled, and we perturb each coordinate using 101 values equally spaced between -1 and +1.\nOne Coordinate at a Time\ndelta &lt;- wdef(4)\nw &lt;- wdef(4)\nw &lt;- w / sum(w)\ns &lt;- sum (w * delta ^ 2)\ndelta &lt;- delta / sqrt (s)\nx &lt;- matrix(c(0,0,1,0,.5,sqrt(3)/2),3,2,byrow = TRUE)\nx &lt;- rbind(x, apply(x, 2, mean))\nx &lt;- apply(x, 2, function(x) x - mean(x))\nd &lt;- as.matrix(dist(x))\ns &lt;- sum (w * delta * d) / sum (w * d * d)\nx &lt;- x * s\nd &lt;- d * s\nh &lt;- smacofR(w,\n            delta,\n            p=2,\n            xold = x,\n            eps=1e-15,\n            xstop=TRUE)\n\nitel      1  eiff    0.0000000000  sold    0.0334936491  snew    0.0334936491  \n\nprint(eigen(h$h)$values)\n\n[1]  3.333333e-01  2.559831e-01  2.559831e-01  2.224907e-16  2.146434e-16\n[6]  3.456430e-17 -2.151233e-17 -1.812014e-16\nx&lt;-matrix(c(0,2,1,1,0,0,sqrt(3),sqrt(3)/3),4,2)\nz&lt;-matrix(c(0,-2,sqrt(3),-2-sqrt(3)/3,0,0,sqrt(3),2+sqrt(3)),4,2)\neps &lt;- 0.00001\nd &lt;- as.matrix(dist(x + eps * z))\nsmacofLossR(d, w, delta) \n\n[1] 0.2559831",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stress Spaces</span>"
    ]
  },
  {
    "objectID": "spaces.html#propconfspace",
    "href": "spaces.html#propconfspace",
    "title": "3  Stress Spaces",
    "section": "",
    "text": "3.1.1 Zero Distance Subspaces\nA zero-distance subspace is a subspace of configuration space in which one or more distances are zero. That a zero distance indeed defines a subspace follows from the fact that the nonlinear equation \\(d_{ij}(X)=0\\) is equivalent to the homogeneous linear equation \\(x_i=x_j\\).\nThe number of zero-distance subspaces is the same as the number of set partitions of \\(n\\) objects, which is the Bell number \\(B_n\\). Bell numbers are defined by the recursion \\[\\begin{equation}\nB_{n+1}=\\sum_{k=0}^n\\binom{n}{k}B_k,\n(\\#eq:bellnums)\n\\end{equation}\\] with \\(B_0=1\\). The next ten Bell numbers \\(B_1,\\cdots,B_{10}\\) are 1, 2, 5, 15, 52, 203, 877, 4140, 21147, 11597. So there are lots of zero-distance subspaces.\nThe gradient in configuration space for all \\(X\\in\\mathbb{R}^{n\\times p}\\) is \\[\n\\nabla\\sigma(X)=(V-B(X))X.\n\\] \\[\n\\nabla\\tilde\\sigma(\\theta)=(\\tilde V\\theta-\\tilde B(\\theta)\\theta)\n\\] \\[\n\\frac12\\{\\nabla\\tilde\\sigma(\\theta)\\}_s=\\text{tr}\\ Y_s'(VX-B(X)X)=\\text{tr}\\ Y_s'\\nabla\\sigma(X),\n\\]\nwith \\(X=\\sum_{s=1}^r\\theta_sY_s\\).\nThus \\(\\nabla\\tilde\\sigma(\\theta)=0\\) if and only if \\(\\nabla\\sigma(X)\\in\\mathcal{Y}_\\perp\\), the subspace of \\(\\mathbb{R}^{n\\times p}\\) orthogonal to \\(\\mathcal{Y}\\). Specifically \\(\\nabla\\sigma(X)=0\\) implies \\(\\nabla\\tilde\\sigma(\\theta)=0\\). If the \\(Y_s\\) span all of \\(\\mathbb{R}^{n\\times p}\\) then \\(\\nabla\\tilde\\sigma(\\theta)=0\\) if and only if \\(\\nabla\\sigma(X)=0\\).\nFor the relationship between the minimization problems in coefficient space and configuration space we also study the relationship between the two Hessians.\nFor all \\(X\\) and \\(Z\\) in configuration space \\[\n\\frac12\\nabla^2\\sigma(X)(Z,Z)=\\text{tr}\\ Z'VZ-\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\frac{\\delta_{ij}}{d_{ij}(X)}\\left\\{\\text{tr}\\ Z'A_{ij}Z-\\frac{\\{\\text{tr}\\ Z'A_{ij}Y\\}^2}{d_{ij}^2(X)}\\right\\}\n\\]\nFor any \\(\\theta\\) in coefficient space \\[\n\\frac12\\nabla^2\\tilde\\sigma(\\theta)=\\tilde V-\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\frac{\\delta_{ij}}{d_{ij}(\\theta)}\\left\\{\\tilde A_{ij}-\\frac{\\tilde A_{ij}\\theta\\theta'\\tilde A_{ij}}{d_{ij}^2(\\theta)}\\right\\}\n\\] and thus\n\\[\n\\xi'\\nabla^2\\tilde\\sigma(\\theta)\\xi=\\nabla^2\\sigma(X)(Z,Z).\n\\] where \\(Z:=\\sum_{s=1}^r\\xi_s Y_s\\) and \\(X:=\\sum_{s=1}^r\\theta_s Y_s\\).\nThus if \\(\\theta\\) is a local minimum in coefficient space then \\(\\nabla^2\\sigma(X)\\) is positive semi-definite on the subspace \\(\\mathcal{Y}\\). If \\(X\\in\\mathcal{Y}\\) but \\(Z\\not\\in\\mathcal{Y}\\) it is perfectly possible that \\(\\nabla^2\\sigma(X)(Z,Z)&lt;0\\), and thus \\(X\\) can be a saddle point in configuration space. If \\(X\\in\\mathcal{Y}\\) is a local minimum in configuration space, then \\(\\theta\\) is a local minimum in coefficient space. Of course if \\(\\mathcal{Y}\\) is the whole space then \\(\\theta\\) is a local minimum in coefficient space if and only if \\(X\\) is a local minimum in configuration space.\n\\[\\begin{multline}\n\\sigma(X(\\theta+\\epsilon\\xi))=\\sigma(X(\\theta)+\\epsilon\\mathcal{D}X(\\theta)(\\xi)+\\frac12\\epsilon^2\\mathcal{D}^2X(\\theta)(\\xi,\\xi))=\\\\\n\\sigma(X(\\theta))+\\epsilon\\mathcal{D}\\sigma(X(\\theta))\\mathcal{D}X(\\theta)\\xi+\\frac12\\epsilon^2\\{\\mathcal{D}\\sigma(X(\\theta))\\mathcal{D}^2X(\\theta)(\\xi,\\xi)+\\mathcal{D}\\sigma(X(\\theta))\\mathcal{D}\\sigma(X(\\theta))\\}\n\\end{multline}\\]\n\\[\n\\{\\mathcal{D}^2X(\\theta)(\\xi,\\xi)\\}_{ip}=\\sum\\sum\\xi_s\\xi_t\\mathcal{D}_{st}x_{ip}(\\theta)=\\xi'H_{ip}(\\theta)\\xi\n\\] \\[\n\\{\\mathcal{D}X(\\theta)(\\xi)\\}_{ip}=\\sum\\xi_s\\mathcal{D}_sx_{ip}(\\theta)=G_{ip}(\\theta)\\xi\n\\]\n\\[\n\\mathcal{D}\\sigma(X(\\theta))=\\{V-B(X(\\theta))\\}X(\\theta)=F(\\theta)\n\\]\n\\[\n\\mathcal{D}\\sigma(\\theta)=\\mathcal{D}\\sigma(X(\\theta))\\mathcal{D}X(\\theta)\n\\]\n\\(\\sigma(x_{11}(\\theta),\\cdots,x_{np}(\\theta))\\)\n\\[\n\\mathcal{D}_s\\sigma(\\theta)=\\sum_{i=1}^n\\sum_{s=1}^p\\mathcal{D}_{ip}\\sigma(X(\\theta))\\mathcal{D}_sx_{ip}(\\theta)\n\\]\n\\[\n\\mathcal{D}_{st}\\sigma(\\theta)=\\sum_{i=1}^n\\sum_{s=1}^p\n\\sum_{j=1}^n\\sum_{r=1}^p\\mathcal{D}_{is,jr}\\sigma(X(\\theta))\\mathcal{D}_sx_{is}(\\theta)\\mathcal{D}_tx_{jr}(\\theta)\n+\\sum_{i=1}^n\\sum_{s=1}^p\\mathcal{D}_{is}\\sigma(X(\\theta))\\mathcal{D}_{st}x_{is}(\\theta)\n\\] Now let \\(Y_1,Y_2,\\cdots,Y_r\\) be linearly independent configurations in \\(\\mathbb{R}^{n\\times p}\\), and consider minimizing stress over all linear combinations \\(X\\) of the form \\(X=\\sum_{s=1}^r \\theta_sY_s\\).\nEach linear combination can be identified with a unique vector \\(\\theta\\in\\mathbb{R}^r\\), the coefficients of the linear combination. Thus we can also formulate our problem as minimizing stress over coefficient space, which is simply \\(\\mathbb{R}^r\\). We write \\(d_{ij}(\\theta)\\) for \\(d_{ij}(X)\\) and \\(\\sigma(\\theta)\\) for \\(\\sigma(X)\\). Note that \\(d_{ij}(\\theta)=\\sqrt{\\theta'C_{ij}\\theta}\\), where \\(C_{ij}\\) has elements \\(\\{C_{ij}\\}_{st}:=\\text{tr}\\ Y_s'A_{ij}Y_t\\).\nIf the \\(Y_t\\) are actually a basis for configuration space (i.e. if \\(r=np\\)) then minimizing over configuration space and coordinate space is the same thing. For the \\(Y_t\\) we could choose all rank one matrices, for example, of the form \\(a_i^{\\ }b_s'\\) where the \\(a_i\\) are a basis for \\(\\mathbb{R}^n\\) and the \\(b_s\\) are a basis for \\(\\mathbb{R}^p\\). And, in particular, the \\(a_i\\) and \\(b_s\\) can be chosen as unit vectors of length \\(n\\) and \\(p\\), respectively. That case we have \\(C_{ij}=I_p\\otimes A_{ij}\\), i.e. the direct sum of \\(p\\) copies of \\(A_{ij}\\). Also if \\(\\theta=\\text{vec}(X)\\) then \\(d_{ij}(X)=\\sqrt{\\theta'(I_p\\otimes A_{ij})\\theta}\\)\nIf \\(r&lt;np\\) then coefficient space defines a proper subspace of configuration space. If it happens to be the \\((n -1)p\\) dimensional subspace of all column-centered matrices, then the two approaches still define the same minimization problem. But in general \\(r&lt;(n-1)p\\) with the \\(Y_s\\) column-centered defines a constrained MDS problem, which we analyze in more detail in chapter @ref(cmds).\nCoefficient space is also a convenient place to deal with rotational indeterminacy in basic MDS. It follows from QR decomposition that any configuration matrix can be rotated in such a way that it upper diagonal elements (the \\(x_{ij}\\) with \\(i&lt;j\\)) are zero (define \\(X_p\\) to be the first \\(p\\) rows of \\(X\\), compute \\(X_p'=QR\\) with \\(Q\\) square orthonormal and \\(R\\) upper triangular, thus \\(X_p=R'Q'\\) and \\(X_pQ=R'\\), which is lower triangular). The column-centered upper triangular configurations are a subspace of dimension \\(p(n-1)-p(p-1)/2\\), and we can choose the \\(Y_s\\) as a basis for this subspace. In this way we eliminate rotational indeterminacy in a relatively inexpensive way.\nIf \\(X=\\sum_{s=1}^r \\theta_sY_s\\) then we define the symmetric positive definite matrix \\(B(\\theta)\\) of order \\(r\\) with elements\n\\[\\begin{equation}\nb_{st}(\\theta):=\\text{tr}\\ Y_s'B(X)Y_t,\n(\\#eq:propcoefb)\n\\end{equation}\\]\nwhere \\(B(X)\\) is the usual B-matrix of order \\(n\\) in configuration space, defined in equation @ref(eq:bdef). Also define \\(V\\) of order \\(r\\) by\n\\[\\begin{equation}\nv_{st}:=\\text{tr}\\ Y_s'VY_t,\n(\\#eq:propcoefv)\n\\end{equation}\\]\nwhere the second \\(V\\), of order \\(n\\), is given by equation @ref(eq:vdef). Then\n\\[\\begin{equation}\n\\sigma(\\theta)=1-2\\ \\theta'B(\\theta)\\theta+\\theta'V\\theta.\n(\\#eq:propcoefs)\n\\end{equation}\\]\nThe relationship between the stationary points in configuration space and coefficient space is fairly straightforward.\n\nSuppose \\(\\theta\\) is in coefficient space and \\(X=\\sum_{s=1}^r\\theta_s Y_s\\) is the corresponding point in configuration space.\n\nIf \\(X\\) is a stationary point in configuration space then \\(\\theta\\) is a stationary point in coefficient space.\nIf \\(\\theta\\) is a stationary point in coefficient space then \\(X\\) is a stationary point in configuration space if and only if \\(\\text{rank}(Y_1\\mid\\cdots\\mid Y_r)\\geq n-1\\). (THIS IS WRONG)\n\n\n\nProof. We have \\(B(X)X=VX\\), i.e.  \\[\\begin{equation}\n\\sum_{s=1}^r \\theta_s B(X)Y_s=\\sum_{s=1}^r \\theta_s VY_s.\n(\\#eq:propfitoff)\n\\end{equation}\\] Premultiplying both sides by \\(Y_t'\\) and taking the trace gives \\(B(\\theta)\\theta=V\\theta\\). This proves the first part.\nFor the second part, suppose \\(B(\\theta)\\theta=V\\theta\\) and define \\(X=\\sum_{s=1}^r\\theta_s Y_s\\). Then\n\\[\\begin{equation}\n\\sum_{t=1}^r\\text{tr}\\ Y_s'(B(X)-V)X=0.\n(\\#eq:propfftofi)\n\\end{equation}\\]\nThus \\(B(X)X=VX\\) if and only if \\(Y_s'(B(X)-V)X=0\\) for all \\(s\\), which translates to the rank condition in the theorem (this is WRONG, correct).\n\nThe advantage of working in coefficient space is that formulas tend to become more simple. Functions are defined on \\(\\mathbb{R}^r\\), and not a space of matrices, in which some coordinates belong to the same point (row) and others to other points (rows), and some are on the same dimension (column), while others are on different dimensions (columns).\nNote that expressions such as @ref(eq:propfitoff) and @ref(eq:propfitoff) simplify if the \\(Y_s\\) are \\(V\\)-orthonormal, i.e. if \\(\\text{tr}\\ Y_s'VY_t=\\delta^{st}\\) and thus \\(V=I\\). It is easy to generate such an orthonormal set from the original \\(Y_s\\) by using the Gram-Schmidt process. The R function gramy() in utilities.R does exactly that. Coefficient space, which is the span of the \\(Y_s\\), is not changed by the orthogonalization process.\nFor a \\(V\\)-orthonormal set \\(Y\\) we have the stationary equations \\(B(\\theta)\\theta=\\theta\\), which says that \\(\\theta\\) is an eigenvector of \\(B(\\theta)\\) with eigenvalue 1.\nThe Hessian is\n\\[\\begin{equation}\n\\mathcal{D}^2\\sigma(\\theta)=I-H(\\theta),\n(\\#eq:hessmat)\n\\end{equation}\\]\nwith\n\\[\\begin{equation}\nH(\\theta):=\\mathcal{D}^2\\rho(\\theta)=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\frac{\\delta_{ij}}{d_{ij}(\\theta)}\\left\\{C_{ij}-\\frac{C_{ij}\\theta\\theta'C_{ij}}{\\theta'C_{ij}\\theta}\\right\\}.\n(\\#eq:rhohessdef)\n\\end{equation}\\]\nWe have \\(0\\lesssim H(\\theta)\\lesssim B(\\theta)\\) and thus \\(I-B(\\theta)\\lesssim\\mathcal{D}^2\\sigma(\\theta)\\lesssim I\\).\nHessian in coef and conf space",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stress Spaces</span>"
    ]
  },
  {
    "objectID": "spaces.html#propspherespace",
    "href": "spaces.html#propspherespace",
    "title": "3  Stress Spaces",
    "section": "3.2 Spherical Space",
    "text": "3.2 Spherical Space\n\\[\\begin{equation}\n\\min_X\\sigma(X)=\\min_{\\lambda\\geq 0}\\min_{\\eta^2(X)=1}\\sigma(\\lambda X)=\\min_{\\eta^2(X)=1}\\min_{\\lambda\\geq 0}\\sigma(\\lambda X)=\n\\min_{\\eta^2(X)=1}1-\\rho^2(X).\n(\\#eq:homequ)\n\\end{equation}\\]\nWe see that basic MDS can also be formulated as maximization of \\(\\rho\\) over the ellipsoid \\(\\{X\\mid \\eta^2(X)=1\\}\\) or, equivalently, over the convex ellipsoidal disk \\(\\{X\\mid \\eta^2(X)\\leq 1\\}\\). A similar formulation is available in coefficient space.\nThis shows that the MDS problem can be seen as a rather special nonlinear eigenvalue problem. Guttman (1968) also discusses the similarities of MDS and eigenvalue problems, in particular as they relate to the power method. In linear eigenvalue problems we maximize a convex quadratic form, in the MDS problem we maximize the homogeneous convex function \\(\\rho\\), in both cases over an ellipsoidal disk. The sublevel sets of \\(\\rho\\) defined as \\(\\mathcal{L}_r:=\\{X\\mid \\rho(X)\\leq r\\}\\) are nested convex sets containing the origin. The largest of these sublevel sets that still intersects the ellipsoid \\(\\eta^2(X)=1\\) corresponds to the global minimum of stress.\nIn two and maybe three dimensions graphical method.\n\\(\\alpha X+\\beta Y\\) in sphere space one-dimensional",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stress Spaces</span>"
    ]
  },
  {
    "objectID": "spaces.html#distance-space",
    "href": "spaces.html#distance-space",
    "title": "3  Stress Spaces",
    "section": "3.3 Distance Space",
    "text": "3.3 Distance Space\n\\[\n\\sigma(D)=\\min_{D\\in\\mathbb{D}}\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n} w_{ij}(\\delta_{ij}-d_{ij})^2.\n\\] \\(\\mathbb{D}\\) is the set of \\(p\\)-dimensional Euclidean distance matrices, which is not convex.\n\\(\\mathbb{D}\\) is the set of Euclidean distance matrices, which is not convex.\nIf \\(D_1\\times D_2=0\\) then they span a convex cone.\n\\[\n\\tau(\\alpha D_1+(1-\\alpha)D_2)^{(2)})=\\alpha^2 \\tau(D_1^2)+(1-\\alpha)^2\\tau(D_2^2)+2\\alpha(1-\\alpha)\\tau(D_1\\times D_2)\n\\] So if \\(\\tau(D_1\\times D_2)\\gtrsim 0\\) convex.\n\\(\\mathbb{D}\\) is the set of distance matrices, which is convex.\nDistance matrices are defined by linear inequalities.\n\\(\\mathbb{D}\\) is the set of ultrametric matrices, \\(d_{ij}\\leq\\max(d_{ik},d_{jk})\\), which is not convex.\nIf \\(D\\in\\mathbb{D}\\) then \\(\\sqrt{D}\\in\\mathbb{D}\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stress Spaces</span>"
    ]
  },
  {
    "objectID": "spaces.html#squared-distance-space",
    "href": "spaces.html#squared-distance-space",
    "title": "3  Stress Spaces",
    "section": "3.4 Squared Distance Space",
    "text": "3.4 Squared Distance Space\n\\[\n\\min_{D\\in\\mathbb{E}}\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\delta_{ij}-d_{ij})^2.\n\\] \\(\\mathbb{E}\\) is the set of squared Euclidean distance matrices, which is convex.\nIf \\(E\\in\\mathbb{E}\\) then \\(\\sqrt{E}\\in\\mathbb{D}\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stress Spaces</span>"
    ]
  },
  {
    "objectID": "spaces.html#gramian-space",
    "href": "spaces.html#gramian-space",
    "title": "3  Stress Spaces",
    "section": "3.5 Gramian Space",
    "text": "3.5 Gramian Space\nWe can write \\(d_{ij}^2(X)=\\text{tr}\\ X'A_{ij}X = \\text{tr}\\ A_{ij}C,\\) with \\(C=XX'\\). This shows minimizing stress can also be formulated as minimizing\n\\[\\begin{equation}\n\\sigma(C)=1+\\text{tr}\\ VC-2\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\delta_{ij}\\sqrt{\\text{tr}\\ A_{ij}C}\n(\\#eq:stresspsd)\n\\end{equation}\\]\nover all \\(C\\gtrsim 0\\) of rank \\(r\\leq p\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stress Spaces</span>"
    ]
  },
  {
    "objectID": "spaces.html#picsstress",
    "href": "spaces.html#picsstress",
    "title": "3  Stress Spaces",
    "section": "3.6 Pictures of Stress",
    "text": "3.6 Pictures of Stress\nEven for \\(n\\) as small as four and \\(p\\) as small as two the dimension of the space of centered configurations is six, and there is no natural way to visualize a function of six variables. What we can do is plot stress on two-dimensional subspaces, either as a contour plot or as a perspective plot. Our two-dimensional subspaces are of the form \\(\\alpha X+\\beta Y\\), where \\(X\\) and \\(Y\\) are fixed configurations. Much of this chapter is a modified, and in some places expanded, version of De Leeuw (2016).\nThroughout we use a small example of order \\(n=4\\) which all dissimilarities equal. The same example has been analyzed by De Leeuw (1988), De Leeuw (1993), Trosset and Mathar (1997), and Zilinskas and Poslipskyte (2003). For this example a global minimum in two dimensions has its four points in the corners of a square. That is our \\(X\\), which has stress 0.0285955. Our \\(Y\\) is another stationary point, which has three points in the corners of an equilateral triangle and the fourth point in the center of the triangle. Its stress is 0.0669873. We column-center the configurations and scale them so that they are actually stationary points, i.e. so that \\(\\eta^2(X)=\\rho(X)\\) and \\(\\eta^2(Y)=\\rho(Y)\\). The example is chosen in such a way that there are non-zero \\(\\alpha\\) and \\(\\beta\\) such that \\(d_{12}(\\alpha X+\\beta Y)=0\\). In fact \\(d_{12}\\) is the only distance that can be made zero by a non-trivial linear combination.\nAnother way of looking at the two configurations is that \\(X\\) are four points equally spaced on a circle, and \\(Y\\) are three points equally spaced on a circle with the fourth point in the center of the circle. De Leeuw (1988) erroneously claims that \\(Y\\) is a non-isolated local minimum of stress, but Trosset and Mathar (1997) have shown there exists a descent direction at \\(Y\\), and thus \\(Y\\) is actually a saddle point. Of course the stationary points defined by \\(X\\) and \\(Y\\) are far from unique, because we can permute the four points over the corners of the square and the triangle in many ways.\n\n3.6.1 Coefficient Space\nConfigurations as a linear combination of a number of given configurations have already been discussed in general in chapter @ref(propchapter), section @ref(propspaces) as the transformation from configuration space to coefficient space. Since we are dealing here with the special case of linear combinations of only two configurations we specialize some of these general results.\nWe start with \\(d_{ij}^(\\theta)=\\theta'T_{ij}\\theta\\), where \\(\\theta\\) has elements \\(\\alpha\\) and \\(\\beta\\), and where \\(T\\) is the \\(2\\times 2\\) matrix with elements \\[\\begin{equation}\nt_{ij}:=\\begin{bmatrix}\n\\text{tr}\\ X'A_{ij}X&\\text{tr}\\ X'A_{ij}Y\\\\\n\\text{tr}\\ Y'A_{ij}X&\\text{tr}\\ Y'A_{ij}Y\n\\end{bmatrix}\n(\\#eq:pictbform)\n\\end{equation}\\]\nThen\n\\[\\begin{equation}\n\\tilde\\sigma(\\theta):=1-2\\ \\theta'C(\\theta)\\theta+\\theta'U\\theta,\n(\\#eq:picstress2)\n\\end{equation}\\]\nwhere, using \\(Z(\\theta)=\\alpha X+\\beta Y\\),\n\\[\\begin{equation}\nC(\\theta):=\n\\begin{bmatrix}\n\\text{tr}\\ X'B(Z(\\theta))X&\\text{tr}\\ X'B(Z(\\theta))Y\\\\\n\\text{tr}\\ Y'B(Z(\\theta))X&\\text{tr}\\ Y'B(Z(\\theta))Y\n\\end{bmatrix},\n(\\#eq:pictbformc)\n\\end{equation}\\]\nand\n\\[\\begin{equation}\nU:=\\begin{bmatrix}\n\\text{tr}\\ X'VX&\\text{tr}\\ X'VY\\\\\n\\text{tr}\\ Y'VX&\\text{tr}\\ Y'VY\n\\end{bmatrix}.\n(\\#eq:pictbformu)\n\\end{equation}\\]\nWe have used \\(\\tilde\\sigma\\) in equation @ref(eq:picstress2) to distinguish stress on the two-dimensional space of coefficients from stress on the eight-dimensional space of \\(4\\times 2\\) configurations. Thus \\(\\tilde\\sigma(\\alpha,\\beta)=\\sigma(\\alpha X + \\beta Z)\\).\nThe gradient at \\(\\theta\\) is\n\\[\\begin{equation}\n\\nabla\\tilde\\sigma(\\theta)=U\\theta-C(\\theta)\\theta,\n(\\#eq:picgrad2)\n\\end{equation}\\]\nand the Hessian is\n\\[\\begin{equation}\n\\nabla^2\\tilde\\sigma(\\theta)=U-\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}\nw_{ij}\\frac{\\delta_{ij}}{d_{ij}(\\theta)}\\left\\{T_{ij}-\\frac{T_{ij}\\theta\\theta'T_{ij}}{\\theta'T_{ij}\\theta}\\right\\}.\n(\\#eq:pichess2)\n\\end{equation}\\]\n\n\nIf \\(B(X)X=VX\\) and \\(\\theta=\\begin{bmatrix}1&0\\end{bmatrix}\\) then \\(C(\\theta)\\theta=U\\theta\\).\n\n\n\nProof. If \\(B(X)X=VX\\) and \\(\\theta=\\begin{bmatrix}1&0\\end{bmatrix}\\) then, by equations @ref(eq:pictbformc) and @ref(eq:pictbformu),\n\\[\\begin{equation}\nU-C(\\theta)=\\begin{bmatrix}0&0\\\\0&\\text{tr}\\ Y'(V-B(X))Y\\end{bmatrix}.\n(\\#eq:picuminc)\n\\end{equation}\\]\nThus \\((U-C(\\theta))\\theta=0\\).\n\n\nThus each stationary point of \\(\\sigma\\) gives a stationary point of \\(\\tilde\\sigma\\). The other way around, however, we are not so lucky.\n\n\nIf \\(C(\\theta)\\theta=U\\theta\\) and if the \\(n\\times 2p\\) matrix \\(\\begin{bmatrix}X&Y\\end{bmatrix}\\) has rank \\(n-1\\) then \\(B(Z)Z=VZ\\).\n\n\n\nProof. If \\(C(\\theta)\\theta=U\\theta\\) then both \\(\\text{tr}\\ X'(V-B(Z)Z)=0\\) and \\(\\text{tr}\\ Y'(V-B(Z))Z=0\\). If the \\(n\\times 2p\\) matrix \\(\\begin{bmatrix}X&Y\\end{bmatrix}\\) has rank \\(n-1\\) then this implies \\((V-B(Z))Z=0\\).\n\n\nIn our example the singular values of \\(\\begin{bmatrix}X&Y\\end{bmatrix}\\) are 0.4879059, 0.4333287, 0.2242285, 2.256597^{-17} and thus there is a one-one correspondence between stationary points of \\(\\sigma\\) and \\(\\tilde\\sigma\\).\n\n\nIf \\(B(X)X=VX\\) and \\(\\theta=\\begin{bmatrix}1&0\\end{bmatrix}\\) then\n\nIf \\(\\text{tr}\\ Y'(V-B(X))Y &gt; 0\\) then \\(\\sigma\\) has a local minimum at theta.\nIf \\(\\sigma\\) has a saddle point at \\(\\theta\\) then \\(\\text{tr}\\ Y'(V-B(X))Y &lt; 0\\).\n\n\n\n\nProof. Suppose \\(B(X)X=VX\\) and \\(\\theta=\\begin{bmatrix}1&0\\end{bmatrix}\\). Then, from @ref(eq:pichess2) and @ref(eq:picuminc), \\[\n\\nabla^2\\sigma(\\theta)=\\begin{bmatrix}0&0\\\\0&\\text{tr}\\ Y'(V-B(X))Y\\end{bmatrix}+\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}\nw_{ij}\\frac{\\delta_{ij}}{d_{ij}(\\theta)}\\frac{T_{ij}\\theta\\theta'T_{ij}}{\\theta'T_{ij}\\theta}.\n\\]\n\n\nIn our example \\(\\text{tr}\\ X'(V-B(Y))X\\) is -0.1502768 and \\(\\text{tr}\\ Y'(V-B(X))Y\\) is -0.0533599.\n\n\n3.6.2 Global Perspective\nWe first make a global perspective plot, over the range \\((-2.5,+2.5)\\).\n\n\n\n\n\nGlobal Perspective\n\n\n\n\nWe see the symmetry, following from the fact that stress is even. We also see the local maximum at the origin, where stress is not differentiable. Also note the ridge, where \\(d_{12}(\\theta)=0\\) and where stress is not differentiable either. The ridge shows nicely that on rays emanating from the origin stress is a convex quadratic. Also, far away from the origin, stress globally behaves very much like a convex quadratic (except for the ridge). Clearly local minima must be found in the valleys surrounding the small mountain at the origin, all within the sphere with radius \\(\\sqrt{2}\\).\n\n\n3.6.3 Global Contour\nFigure @ref(fig:globalperspective) is a contour plot of stress over \\((-2,+2)\\otimes(-2,+2)\\). The red line is \\(\\{\\theta\\mid d_{12}(\\theta) = 0\\}\\). The blue line has the minimum of the convex quadratic on each of the rays through the origin. Thus all local minima, and in fact all stationary points, are on the blue line. Note that the plot uses \\(\\theta\\) to define the coordinate axes, not \\(\\gamma=(\\alpha,\\beta)\\). Thus there are no stationary points at \\((0,1)\\) and \\((1,0)\\), but at the corresponding points (1.3938469, 0) and (1.0406404, 0.8849253) in the \\(\\theta\\) coordinates (and, of course, at their mirror images).\nBesides the single local maximum at the origin, it turns out that in this example there are five pairs of stationary points. Or, more precisely, I have not been able to find more than five. Each stationary point \\(\\theta\\) has a mirror image \\(-\\theta\\). Three of the five are local minima, two are saddle points. Local minima are plotted as blue points, saddle points as red points.\n\n\n\n\n\nGlobal Contour\n\n\n\n\n\n\n3.6.4 Stationary Points\n\n3.6.4.1 First Minimum\nWe zoom in on the first local minimum at (1.0406404, 0.8849253). Its stress is 0.0669873, and the corresponding configuration has three points in the corners of an equilateral triangle and the fourth point in its centroid. Note that this local minimum is a saddle point in configuration space \\(\\mathbb{R}^{4\\times 2}\\) (Trosset and Mathar (1997)). The eigenvalues of \\(B(\\theta)\\) are (1.3686346, 1) and those of the Hessian \\(I-H(\\theta)\\) are (1, 0.0817218). The area of the contour plot around the stationary value is in figure @ref(fig:contfirstmin).\n\n\n\n\n\nContour Plot First Minimum\n\n\n\n\n\n\n\n3.6.5 Second Minimum\nThe second local minimum (which is the global minimum) at (1.3938469, 0) has stress 0.0285955. The configuration are the four points at the corners of a square. The eigenvalues of \\(B(\\theta)\\) are (1.1362799, 1) and those of the Hessian \\(I-H(\\theta)\\) are (1, 0.3743105). The area of the contour plot around the stationary value is in figure @ref(fig:contsecmin).\n\n\n\n\n\nContour Plot Second Minimum\n\n\n\n\n\n\n3.6.6 Third Minimum\nThe third local minimum at (0.1096253, 1.3291942) has stress 0.1106125, and the corresponding configuration is in figure @ref(fig:confthirdmin).\n\n\n\n\n\n\nConfiguration Third Minimum\n\n\n\n\nThe eigenvalues of \\(B(\\theta)\\) are (1.5279386, 1) and those of the Hessian \\(I-H(\\theta)\\) are (1, 0.2362079). The area of the contour plot around the stationary value is in figure @ref(fig:contthirdmin)\n\n\n\n\n\n\nContour Plot Third Minimum\n\n\n\n\n\n\n3.6.7 First Saddle Point\nThe saddle point at (0.3253284, 1.2916758) has stress 0.1128675, and the corresponding configuration is in figure @ref(fig:conffirstsad).\n\n\n\n\n\n\nConfiguration First Saddlepoint\n\n\n\n\nThe eigenvalues of \\(B(\\theta)\\) are (1.7778549, 1) and those of the Hessian \\(I-H(\\theta)\\) are (1, -0.311088). The area of the contour plot around the stationary value is in figure @ref(fig:contfirstsad)\n\n\n\n\n\nContour First Saddlepoint\n\n\n\n\n\n\n3.6.8 Second Saddle Point\nThe saddle point at (1.1238371, 0.7762046) has stress 0.0672483 and the corresponding configuration is in figure @ref(fig:confsecsad)\n\n\n\n\n\nConfiguration Second Saddlepoint\n\n\n\n\nThe eigenvalues of \\(B(\\theta)\\) are (1.4111962, 1) and those of the Hessian \\(I-H(\\theta)\\) are (1, -0.0841169). The area of the contour plot around the stationary value is in figure @ref(fig:contsecsad)\n\n\n\n\n\nContour Plot Second Saddlepoint",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stress Spaces</span>"
    ]
  },
  {
    "objectID": "spaces.html#picsphere",
    "href": "spaces.html#picsphere",
    "title": "3  Stress Spaces",
    "section": "3.7 Another Look",
    "text": "3.7 Another Look\nRemember that \\(\\rho(\\theta)=\\theta'B(\\theta)\\theta\\). Thus \\(\\sigma(\\lambda\\theta)=1-\\lambda\\rho(\\theta)+\\frac12\\lambda^2\\theta'\\theta\\), and \\[\n\\min_\\lambda\\sigma(\\lambda\\theta)=1-\\frac12\\frac{\\rho^2(\\theta)}{\\theta'\\theta}.\n\\] Thus we can minimize \\(\\sigma\\) over \\(\\theta\\) by maximizing \\(\\rho\\) over the unit circle \\(\\mathcal{S}:=\\{\\theta\\mid\\theta'\\theta=1\\}\\). This is a nice formulation, because \\(\\rho\\) is norm, i.e. a homogeneous convex function of \\(\\theta\\). Consequently we have transformed the problem from unconstrained minimization of the DC function (i.e. difference of convex functions) stress to that of maximization of a ratio of norms. In turn this is equivalent to maximization of the convex function \\(\\rho\\) over the unit circle, or, again equivalently, over the unit ball, a compact convex set. This transform was first used in MDS by De Leeuw (1977), partly because it made the theory developed by Robert (1967) available.\nThe levels sets \\(\\{\\theta\\mid\\rho(\\theta)=\\kappa\\}\\) are the \\(\\rho\\)-circles defined by the norm \\(\\rho\\). The corresponding \\(\\rho\\)-balls \\(\\{\\theta\\mid\\rho(\\theta)\\leq\\kappa\\}\\) are closed and nested convex sets containing the origin. Thus we want to find the largest \\(\\rho\\)-circle that still intersects \\(\\mathcal{S}\\). The similarity with the geometry of eigenvalue problems is obvious.\nIn our example we know that the global optimum of stress is at (1.3938469, 0), and if we project that point on the circle it becomes (1, 0). The corresponding optimal \\(\\rho\\) is 1.3938469. Figure @ref(fig:rhocontour) gives the contourplot for \\(\\rho\\), with the outer \\(\\rho\\)-circle corresponding with the optimal value. The fact that the optimal value contour is disjoint from the interior of \\(\\mathcal{S}\\) is necessary and sufficient for global optimality (Dür, Horst, and Locatelli (1998)). Notice the sharp corners in the contour plot, showing the non-diffentiability of \\(\\rho\\) at the points where \\(d_{12}(\\theta)=0\\). We could also look for the minimum of \\(\\rho\\) on the unit circle, which means finding the largest \\(\\rho\\)-circle that touches \\(\\mathcal{S}\\) on the inside. Inspecting figure @ref(fig:rhocontour) shows that this will be a point where \\(\\rho\\) is not differentiable, i.e. a point with \\(d_{12}(\\theta)=0\\). This minimum \\(\\rho\\) problem does not make much sense in the context of multidimensional scaling, however, and it not related directly to the minimization of stress.\n\n\n\n\n\nContour Plot for Rho",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stress Spaces</span>"
    ]
  },
  {
    "objectID": "spaces.html#picline",
    "href": "spaces.html#picline",
    "title": "3  Stress Spaces",
    "section": "3.8 A Final Look",
    "text": "3.8 A Final Look\nNow that we know that the MDS problem is equivalent to maximizing \\(\\rho\\) on the unit circle, we can use nonlinear coordinates \\((\\theta_1,\\theta_2)=(\\sin\\xi,\\cos\\xi)\\) to reduce the problem to a one-dimensional unconstrained one in, say, ``circle space’’. Thus, with the same abuse of notation as for stress, \\(\\rho(\\xi):=\\rho(\\sin\\xi,\\cos\\xi)\\), and we have to maximize \\(\\rho\\) over \\(0\\leq\\xi\\leq\\pi\\).\nIn figure @ref(fig:rhononlinearplot) we have plotted \\(\\rho\\) as a function of \\(\\eta\\). There are blue vertical lines at the three local minima in coefficient space, red vertical lines at the stationary points, and a green vertical line where \\(d_{12}(\\xi)=0\\). Note that in circle space stress has both multiple local minima and multiple local maxima.\n\n\n\n\n\n\nOne-dimensional Rho\n\n\n\n\nFrom lemma xxx we see that the second derivative \\(\\mathcal{D}^2\\rho(\\xi)\\) is equal to \\(\\mathbf{tr}\\ H(\\xi)-\\rho(\\xi)\\). For the three local minima in coordinate space we find second derivatives 0, 0, 0 in circle space, i.e. they are properly converted to local maxima. The two stationary points in coordinate space have second derivatives 0, 0, and are turned into local minima.\nFor more general cases, with a basis of \\(n\\) configurations, we know from Lyusternik and Schnirelmann (1934) that a continuously differentiable even function on the unit sphere in \\(\\mathbb{R}^n\\) has at least \\(n\\) distinct pairs of stationary points.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stress Spaces</span>"
    ]
  },
  {
    "objectID": "spaces.html#discussion",
    "href": "spaces.html#discussion",
    "title": "3  Stress Spaces",
    "section": "3.9 Discussion",
    "text": "3.9 Discussion\nNote that we have used \\(\\sigma\\) for three different functions. The first one with argument \\(Z\\) is defined on configuration space, the second one with argument \\(\\gamma\\) on coefficient space, and the third one with argument \\(\\theta\\) also on coefficient space. This is a slight abuse of notation, rather innocuous, but we have to keep it in mind.\nFrom lemma xxx we see that \\(\\mathcal{D}\\sigma(X)=\\mathcal{D}\\sigma(Y)=0\\) then \\(\\mathcal{D}\\sigma(1,0)=\\mathcal{D}\\sigma(0,1)=0\\). Thus stationary points in configuration space are preserved as stationary points in coefficient space, but the reverse implication may not be true. If \\(\\mathcal{D}^2\\sigma(X)\\) and \\(\\mathcal{D}^2\\sigma(Y)\\) are positive semi-definite, then so are \\(\\mathcal{D}^2\\sigma(1,0)\\) and \\(\\mathcal{D}^2\\sigma(0,1)\\). Thus local minima are preserved. But it is entirely possible that \\(\\mathcal{D}^2\\sigma(X)\\) and/or \\(\\mathcal{D}^2\\sigma(Y)\\) are indefinite, and that \\(\\mathcal{D}^2\\sigma(1,0)\\) and/or \\(\\mathcal{D}^2\\sigma(0,1)\\) are positive semi-definite. Thus saddle points in configuration space can be mapped into local minima in coefficient space. As we will see this actually happens with \\(Y\\), the equilateral triangle with center, in our example.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stress Spaces</span>"
    ]
  },
  {
    "objectID": "spaces.html#coordinates",
    "href": "spaces.html#coordinates",
    "title": "3  Stress Spaces",
    "section": "3.10 Coordinates",
    "text": "3.10 Coordinates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s look at a small example with four points, all dissimilarities equal, and all weights equal to one. There is a local minimum with four points in the corners of a square, with stress equal to 0.0285955. And there is another local minimum with three points forming an equilateral triangle, and the fourth point in the center. This has stress 0.0669873. We can compute stress for all points of the form \\(\\alpha X+\\beta Y\\), where \\(X\\) and \\(Y\\) are the two local minima. Figure @ref(fig:equalcontourplot) has a contour plot of \\(\\sigma(\\alpha,\\beta)\\), showing the local minima at \\((1,0)\\) and \\((0,1)\\), and the local maximum at \\((0,0)\\).\n\n\n\n\n\nPlane spanned by two local minima, equal dissimilarities\n\n\n\n\nAlternatively, we can plot stress on the line connecting \\(X\\) and \\(Y\\). Note that although stress only has a local maximum at the origin in configuration space, it can very well have local maxima if restricted to lines. In fact on a line connecting two local minima there has to be at least one local maximum.\n\n\n\n\n\nLine connecting two local minima, equal dissimilarities\n\n\n\n\nA fundamental result, which forms the basis of chapter @ref(fullchapter) in this book, is that \\(\\sigma\\) is convex on Gramian Space, i.e. on the closed convex cone of all \\(C\\gtrsim 0\\).\n\n\n\n\nDe Leeuw, J. 1977. “Applications of Convex Analysis to Multidimensional Scaling.” In Recent Developments in Statistics, edited by J. R. Barra, F. Brodeau, G. Romier, and B. Van Cutsem, 133–45. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\n———. 1988. “Convergence of the Majorization Method for Multidimensional Scaling.” Journal of Classification 5: 163–80.\n\n\n———. 1993. “Fitting Distances by Least Squares.” Preprint Series 130. Los Angeles, CA: UCLA Department of Statistics. https://jansweb.netlify.app/publication/deleeuw-r-93-c/deleeuw-r-93-c.pdf.\n\n\n———. 2016. “Pictures of Stress.” 2016.\n\n\nDür, M., R. Horst, and M. Locatelli. 1998. “Necessary and Sufficient Global Optimality Conditions for Convex Maximization Revisited.” Journal of Mathematical Analysis and Applications 217: 637–49.\n\n\nGuttman, L. 1968. “A General Nonmetric Technique for Fitting the Smallest Coordinate Space for a Configuration of Points.” Psychometrika 33: 469–506.\n\n\nLyusternik, L., and L. Schnirelmann. 1934. Méthodes Topologiues Dans Les Problèmes Variationelle. Hermann.\n\n\nRobert, F. 1967. “Calcul du Rapport Maximal de Deux Normes sur \\(\\mathbb{R}^n\\).” Revue Francaise d’Automatique, d’Informatique Et De Recherche Operationelle 1: 97–118.\n\n\nTrosset, M. W., and R. Mathar. 1997. “On the Existence on Nonglobal Minimizers of the STRESS Criterion for Metric Multidimensional Scaling.” In Proceedings of the Statistical Computing Section, 158–62. Alexandria, VA: American Statistical Association.\n\n\nZilinskas, A., and A. Poslipskyte. 2003. “On Multimodality of the SSTRESS Criterion for Metric Multidimensional Scaling.” Informatica 14 (1): 121–30.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stress Spaces</span>"
    ]
  },
  {
    "objectID": "classical.html",
    "href": "classical.html",
    "title": "4  Classical Multidimensional Scaling",
    "section": "",
    "text": "4.1 Algebra",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "classical.html#algebra",
    "href": "classical.html#algebra",
    "title": "4  Classical Multidimensional Scaling",
    "section": "",
    "text": "4.1.1 Torgerson Transform\nThe Torgerson transform of a matrix is a linear function from the space of real symmetric hollow matrices to the space of doubly-centered real symmetric matrices defined as\n\\[\\begin{equation}\n\\tau(S):=-\\frac12 JSJ,\n(\\#eq:torgerson)\n\\end{equation}\\]\nwith \\(J\\) the centering matrix \\(I-\\frac{1}{n}ee'\\). For historical reasons @(eq:torgerson) may be more familiar in elementwise notation. Spelled out it is\n\\[\\begin{equation}\n\\tau_{ij}(S)=-\\frac12\\{s_{ij}-s_{i\\bullet}-s_{\\bullet j}+s_{\\bullet\\bullet}\\},\n(\\#eq:torgelem)\n\\end{equation}\\]\nwhere bullets are indices over which we average. The symbol \\(\\tau\\) was chosen by Critchley (1988) to honor Torgerson.\nSome simple calculation with @ref(eq:torgelem), using the hollowness of \\(S\\), gives\n\\[\\begin{equation}\n\\tau_{ii}(S)+\\tau_{jj}(S)-2\\tau_{ij}(S)=s_{ij}.\n(\\#eq:torginv)\n\\end{equation}\\]\nAccordingly, Critchley (1988) defines a linear operator \\(\\kappa\\) on the space of real symmetric matrices by \\(\\kappa(S):=s_{ii}+s_{jj}-2s_{ij}\\). From @ref(eq:torginv) we see that \\(\\kappa(\\tau(S))=S\\). Also for all double-centered \\(S\\) we have \\(\\tau(\\kappa(S))=S\\). Thus \\(\\tau\\) and \\(\\kappa\\) are mutually inverse (Critchley (1988), theorem 2.2).\n\n\n4.1.2 Schoenberg’s Theorem\n\nThere is an \\(X\\in\\mathbb{R}^{n\\times p}\\) such that \\(\\Delta=D(X)\\) if and only if \\(\\tau(\\Delta^{(2)})\\) is positive semi-definite of rank \\(r\\leq p\\).\n\n\nProof. If \\(\\Delta\\) are the distances of a column-centered \\(p\\)-dimensional configuration \\(X\\), then the squared dissimilarities \\(\\Delta^{(2)}\\) are of the form \\(ue'+eu'-2XX'\\), where \\(u_i=x_i'x_i^{\\ }\\) are the squared row lengths. This implies \\(\\tau(\\Delta^{(2)})=XX'\\), which is positive semi-definite of rank \\(r=\\text{rank}(X)\\leq p\\).\nConversely, suppose \\(\\tau(\\Delta^{(2)})=XX'\\). Applying \\(\\kappa\\) to both sides and using \\(\\kappa(XX')=D^{(2)}(X)\\) gives \\(\\Delta^{(2)}=D^{(2)}(X)\\).\n\nAccordingly, we call a dissimilarity matrix \\(\\Delta\\) Euclidean if it is symmetric, hollow, and non-negative and has \\(\\tau(\\Delta^{(2)})\\gtrsim 0\\). The dimension of a Euclidean dissimilarity matrix is the rank of \\(\\tau(\\Delta^{(2)})\\), which is always less than or equal to \\(n-1\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "classical.html#approximation",
    "href": "classical.html#approximation",
    "title": "4  Classical Multidimensional Scaling",
    "section": "4.2 Approximation",
    "text": "4.2 Approximation\n\n4.2.1 Low Rank Approximation of the Torgersen Transform\nIn classical MDS we minimize \\[\\begin{equation}\n\\sigma(X)=\\text{tr}\\ \\left\\{W(\\tau(\\Delta^{(2)})-XX')W\\right\\}^2\n(\\#eq:spapp)\n\\end{equation}\\] over \\(X\\in\\mathbb{R}^{n\\times p}\\), where \\(W\\) is a square non-singular matrix of weights. The minimizer \\(X\\) for the loss function in @ref(eq:spapp) is given by a slight variation of Eckart and Young (1936), discussed first by Keller (1962). See also De Leeuw (1974) and the generalizations to rectangular and singular weight matrices in De Leeuw (1984). The solution is \\(X=W^{-1}K\\Lambda\\), where \\(K\\) are the normalized eigenvectors corresponding with the \\(p\\) largest eigenvalues of the positive part of \\(W(\\tau(\\Delta^{(2)})W\\) (i.e. the matrix that results by replacing the negative eigenvalues with zeroes). Thus the rank of the solution \\(X\\) is equal to the minimum of \\(p\\) and the number of positive eigenvalues of \\(\\tau(\\Delta^{(2)}\\).\nIn the original versions of classical scaling (W. S. Torgerson (1952), W. S. Torgerson (1958)) there are no weights and the problem that the Torgerson transform may be indefinite is ignored. In fact, going from \\(\\tau(\\Delta^{(2)}\\) to \\(X\\) is done by “any method of factoring”, including the centroid method, so no specific loss function is minimized.\nThe loss function @ref(eq:spapp) has been called, somewhat jokingly, strain by Takane, Young, and De Leeuw (1977), mainly because stress and sstress had already been taken. Stress is a weighted least squares loss function defined on the distances, sstress on the squared distances, and strain on the inner products.\n\n\n4.2.2 Minimization of Strain\nIt may be considered a disadvantage of the classical approach, even if it is described as the minimization of strain, that the MDS equations are \\(\\Delta^{(2)}= D^{(2)(X)}\\) while loss is measured on the scalar products and not the distances or the squared distances.\nIt was first pointed out by De Leeuw and Heiser (1982) that strain can also be written as a loss function defined on the squared distances. In fact strain is equal to \\[\\begin{equation}\n\\sigma(X)=\\frac14\\text{tr}\\ \\left\\{WJ(\\Delta^{(2)}-D^{(2)}(X))JW\\right\\}^2,\n(\\#eq:strainer)\n\\end{equation}\\] i.e. to an appropriately weighted version of sstress.\nAn advantage of the classical scaling approach via minimizing strain is that there is no local minimum problem. Finding the optimal configuration is an eigenvalue problem, which allows us to find the global minimum. This has not been emphasized enough, so I’ll emphasize it here once again. If iterative basic MDS algorithms use the classical minimum strain solution as their starting point, then they start at the global minimum of a related loss function. Since they are descent algorithms they will improve their own loss functions, but having such an excellent starting point means they avoid many local minima.\nAnother advantage of the loss function formulation in @ref{eq:strainer} is that it is immediately obvious how to generalize classical scaling when there are missing data or when there is only rank order information. As with stress and sstress we minimize strain over both the configuration \\(X\\) and the missing information.\n\n\n4.2.3 Approximation from Below\nSuppose the Torgerson transform \\(\\tau(\\Delta^{(2)}\\) is PSD of rank \\(r\\), with eigen decomposition \\(K\\Lambda^2 K'\\), and, using the largest \\(p\\) eigenvalues with corresponding eigenvectors, define \\(C_p:=K_p\\Lambda_p^2 K_p'\\). Then, in the Loewner order, \\[\nC_1\\lesssim C_2\\lesssim\\cdots\\lesssim C_r=\\tau(\\Delta^{(2)}).\n\\] Define \\(X_p=K_p\\Lambda_p\\). Then applying Critchley’s inverse Torgerson transform \\(\\kappa\\) it follows from … that elementwise \\[\nD^{(2)}(X_1)\\leq D^{(2)}(X_2)\\leq\\cdots\\leq D^{(2)}(X_r)=\\Delta^{(2)},\n\\] and thus also \\[\nD(X_1)\\leq D(X_2)\\leq\\cdots\\leq D(X_r)=\\Delta.\n\\] Thus in the PSD case classical scaling we approximate the dissimilarities from below. This result is implicit in Gower (1966) and explicit in De Leeuw and Meulman (1986) and Meulman (1986).\nIf \\(\\tau(\\Delta^{(2)}\\) is not psd then\n\\[\nD(X_1)\\leq D(X_2)\\leq\\cdots\\leq D(X_p)=\\cdots=D(X_{n-1})\\geq\\Delta.\n\\]\nBenzecri plot\n\n\n\n\nBlumenthal, L. M. 1953. Theory and Applications of Distance Geometry. Oxford University Press.\n\n\nCritchley, F. 1988. “On Certain Linear Mappings Between Inner Product and Squared Distance Matrices.” Linear Algebra and Its Applications 105: 91–107.\n\n\nDe Leeuw, J. 1974. “Approximation of a Real Symmetric Matrix by a Positive Semidefinite Matrix of Rank r.” Technical Memorandum TM-74-1229-10. Murray Hill, N.J.: Bell Telephone Laboratories.\n\n\n———. 1984. “Fixed-Rank Approximation with Singular Weight Matrices.” Computational Statistics Quarterly 1: 3–12.\n\n\nDe Leeuw, J., and W. J. Heiser. 1982. “Theory of Multidimensional Scaling.” In Handbook of Statistics, Volume II, edited by P. R. Krishnaiah and L. Kanal. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\nDe Leeuw, J., and J. J. Meulman. 1986. “Principal Component Analysis and Restricted Multidimensional Scaling.” In Classification as a Tool of Research, edited by W. Gaul and M. Schader, 83–96. Amsterdam, London, New York, Tokyo: North-Holland.\n\n\nEckart, C., and G. Young. 1936. “The Approximation of One Matrix by Another of Lower Rank.” Psychometrika 1 (3): 211–18.\n\n\nGower, J. C. 1966. “Some Distance Properties of Latent Root and Vector Methods Used in Multivariate Analysis.” Biometrika 53: 325–38.\n\n\nKeller, J. B. 1962. “Factorization of Matrices by Least Squares.” Biometrika 49: 239–42.\n\n\nMessick, S. J., and R. P. Abelson. 1956. “The Additive Constant Problem in Multidimensional Scaling.” Psychometrika 21 (1–17).\n\n\nMeulman, J. J. 1986. “A Distance Approach to Nonlinear Multivariate Analysis.” PhD thesis, Leiden University.\n\n\nSchoenberg, I. J. 1935. “Remarks to Maurice Frechet’s article: Sur la Definition Axiomatique d’une Classe d’Espaces Vectoriels Distancies Applicables Vectoriellement sur l’Espace de Hllbert.” Annals of Mathematics 36: 724–32.\n\n\nTakane, Y., F. W. Young, and J. De Leeuw. 1977. “Nonmetric Individual Differences in Multidimensional Scaling: An Alternating Least Squares Method with Optimal Scaling Features.” Psychometrika 42: 7–67.\n\n\nTorgerson, W. S. 1958. Theory and Methods of Scaling. New York: Wiley.\n\n\nTorgerson, W. S. 1952. “Multidimensional Scaling: I. Theory and Method.” Psychometrika 17 (4): 401–19.\n\n\nYoung, G., and A. S. Householder. 1938. “Discussion of a Set of Points in Terms of Their Mutual Distances.” Psychometrika 3 (19-22).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classical Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "minimization.html",
    "href": "minimization.html",
    "title": "5  Minimization of Basic Stress",
    "section": "",
    "text": "5.1 Gradient Methods\nThe initial algorithms for nonmetric MDS Kruskal (1964) and Guttman (1968) were gradient methods. Thus the gradient, or vector of partial derivatives, was computed in each iteration step, and a step was taken in the direction of the negative gradient (which is also known as the direction of steepest descent).\nInformally, if \\(f\\) is differentiable at \\(x\\) then \\(f(x+h)\\approx f(x)+h'\\mathcal{D}f(x)\\) and the direction \\(h\\) that minimizes the diferential (the second term) is \\(-\\mathcal{D}f(x)/\\|\\mathcal{D}f(x)\\|\\), the normalized negative gradient. Although psychometricians had been in the business of minimizing least squares loss functions in the linear and bilinear case, this result for general nonlinear functions was new to them. And I, and probably many others, hungrily devoured the main optimization reference in Kruskal (1964), which was the excellent early review by Spang (1962).\nKruskal’s paper also presents an elaborate step-size procedure, to determine how far we go in the negative gradient direction. In the long and convoluted paper Guttman (1968) there is an important contribution to gradient methods in basic MDS. Let’s ignore the complications arising from zero distances, which is after all what both Kruskal and Guttman do as well, and assume all distances at configuration \\(X\\) are positive. Then stress is differentiable at \\(X\\), with gradient\n\\[\n\\nabla\\sigma(X)=\n-\\sum_{i=1}^n\\sum_{j=1}^nw_{ij}(\\delta_{ij}-d_{ij}(X))\\frac{1}{d_{ij}(X)}\n(e_i-e_j)(x_i-x_j)'\n\\]\nGeometrical interpretation - Gleason, Borg-Groenen p 20\nGuttman C-matrix\nRamsay C-matrix",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Minimization of Basic Stress</span>"
    ]
  },
  {
    "objectID": "minimization.html#gradient-methods",
    "href": "minimization.html#gradient-methods",
    "title": "5  Minimization of Basic Stress",
    "section": "",
    "text": "5.1.1 Step Size\n\n5.1.1.1 Kruskal Step Size\nelaborate\n\n\n5.1.1.2 Guttman Step Size\nconstant\n\n\n5.1.1.3 Cauchy Step Size\nIn the classical version of the steepest descent method we choose the step-size \\(\\alpha\\) by minimizing \\(h(\\alpha)=f(x+\\alpha y)\\) over \\(\\alpha\\)\n\\[\nd_+h(\\alpha;\\beta)=\\lim_{\\epsilon\\downarrow 0}\\frac{f(x+(\\alpha+\\epsilon\\beta)y)-f(x+\\alpha y)}{\\epsilon}=\\beta\\ d_+f(x+\\alpha y;y)\n\\] or local minimum closest to zero\nNewton version\n\\[\nd_+^2h(\\alpha;\\beta,\\gamma)=\\beta\\gamma d_+^2f(x+\\alpha y;y,y)\n\\]\n\n\n5.1.1.4 Majorization Step Size\nLagrange form of the remainder\n\\(e(\\epsilon)=\\eta^2(X+\\epsilon Y)\\) \\(r(\\epsilon)=\\rho(X+\\epsilon Y)\\) \\(s(\\epsilon)=\\sigma(X+\\epsilon Y)=1-r(\\epsilon)+\\frac12e(\\epsilon)\\)\n\\[\nr(\\epsilon)\\geq r(0)+r'(0)\\epsilon\n\\] \\[\ne(\\epsilon)=e(0)+e'(0)\\epsilon+\\frac12 e''(0)\\epsilon^2\n\\] \\[\ns(\\epsilon)\\leq s(0)+s'(0)\\epsilon+\\frac12 e''(0)\\epsilon^2\n\\] \\[\n\\hat\\epsilon=\\frac{s'(0)}{e''(0)}\n\\] underestimates newton step\n\\[\nr(\\epsilon)\\geq r(0)+\\epsilon\\ r'(0)+\\frac12\\epsilon^2\\min_{0\\leq \\xi\\leq\\epsilon}r''(\\xi)\n\\] \\[\n\\hat\\epsilon=\\frac{s'(0)}{e''(0)-\\min_{0\\leq \\xi\\leq\\epsilon}r''(\\xi)}\n\\]\n\\[\nr''(\\xi)=\\mathcal{D}^2\\sigma(X+\\xi Y;Y,Y)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Minimization of Basic Stress</span>"
    ]
  },
  {
    "objectID": "minimization.html#initial-configurations",
    "href": "minimization.html#initial-configurations",
    "title": "5  Minimization of Basic Stress",
    "section": "5.2 Initial Configurations",
    "text": "5.2 Initial Configurations\nRandom\nL\nTorgerson\nGuttman",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Minimization of Basic Stress</span>"
    ]
  },
  {
    "objectID": "minimization.html#apmajmin",
    "href": "minimization.html#apmajmin",
    "title": "5  Minimization of Basic Stress",
    "section": "5.3 On MM Algorithms",
    "text": "5.3 On MM Algorithms\nThe term majorization is used in mathematics in many different ways. In this book we use it as a general technique for the construction of stable iterative minimization algorithms. An iterative minimization algorithm is stable if it decreases the objective function in each iteration.\nOrtega, Rheinboldt Weber Bohning, Lindsay Vosz, Eckart\nSpecial cases of majorization had been around earlier, most notably the smacof algorithm for MDS and the EM algorithm for maximum likelihood with missing data, but in full generality majorization was first discussed in De Leeuw (1994) and Heiser (1995).\nMajorization can be used to construct minimization methods, while minorization can construct maximization methods. This cleverly suggests to use the acronym MM algorithms for this class of techniques. An excellent comprehensive account of MM algorithms is Lange (2016). Another such account is slowly growing in one of the companion volumes in this series of personal research histories ((deleeuw_B_21b?)).\nHere we just give a quick introduction to majorization. Suppose \\(f\\) is a real-valued function on \\(X\\subseteq\\mathbb{R}^n\\). Then a real-valued function \\(g\\) on \\(X\\otimes X\\) is said to majorize \\(f\\) on \\(X\\) if\n\\[\\begin{equation}\ng(x,y)\\geq f(x)\\qquad\\forall (x,y)\\in X\\otimes X,\n(\\#eq:majorineq)\n\\end{equation}\\]\nand\n\\[\\begin{equation}\ng(y,y)=f(y)\\qquad\\forall y\\in X.\n(\\#eq:majoreq)\n\\end{equation}\\]\nThus for each \\(y\\in X\\) the function \\(g(\\bullet,y)\\) lies above \\(f\\), and it touches \\(f\\) from above at \\(x=y\\). Majorization is strict if \\(g(x,y)=f(x)\\) if and only if \\(x=y\\), i.e. if \\(y\\) is the only point in \\(X\\) where \\(g(\\bullet,y)\\) and \\(f\\) touch.\nA majorization algorithm to minimize \\(f\\) on \\(X\\) starts with an initial estimate, and then updates the estimate in iteration \\(k\\) by\n\\[\\begin{equation}\nx^{(k+1)}\\in\\mathop{\\text{argmin}}_{x\\in X}g(x,x^{(k)}),\n(\\#eq:majoralg)\n\\end{equation}\\]\nwith the understanding that the algorithms stops when \\(x^{(k)}\\in\\mathop{\\text{argmin}}_{x\\in X}g(x,x^{(k)})\\).\nIf we do not stop we have an infinite sequence satisfying the sandwich inequality\n\\[\\begin{equation}\nf(x^{(k+1)})\\leq g(x^{(k+1)},x^{(k)})\\leq g(x^{(k)},x^{(k)})=f(x^{(k)}).\n(\\#eq:sandwich)\n\\end{equation}\\]\nThe first inequality in this chain comes from @ref(eq:majorineq). It is strict when majorization is strict. The second inequality comes from @ref(eq:majoralg), and it is strict if \\(g(\\bullet,y)\\) has a unique minimum on \\(X\\) for each \\(y\\).\nNecessary conditions through MM",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Minimization of Basic Stress</span>"
    ]
  },
  {
    "objectID": "minimization.html#smacof-algorithm",
    "href": "minimization.html#smacof-algorithm",
    "title": "5  Minimization of Basic Stress",
    "section": "5.4 Smacof Algorithm",
    "text": "5.4 Smacof Algorithm\n\n5.4.1 Guttman Transform\nThe Guttman Transform is named to honor the contribution of Louis Guttman to non-metric MDS (mainly, but by no means exclusively, in Guttman (1968)). Guttman introduced the transform in a slightly different way, and partly for different reasons. In chapter @ref(minstr) we shall see that the Guttman Transform plays a major role in defining and understanding the \\(\\textrm{smacof}\\) algorithm.\nThe Guttman Transform of a configuration \\(X\\) is defined as \\[\\begin{equation}\n\\Gamma(X):=V^+B(X)X,\n(\\#eq:guttrans)\n\\end{equation}\\] which is simply equal to \\(\\Gamma(X)=n^{-1}B(X)X\\) if all weights are equal. For some purposes it is useful to think of \\(V^+B(X)X\\) as a function of the weights, the dissimilarities, and the configuration (see, for exam0ple, chapter @ref(chinverse)), but we reserve the name Guttman transform for a map from \\(\\mathbb{R}^{n\\times p}\\) into \\(\\mathbb{R}^{n\\times p}\\) .\nWhat we have called \\(B(X)\\) is what Guttman calls the correction matrix or C-matrix (see De Leeuw and Heiser (1977) for a comparison).\nCompleting the square in equation @ref(eq:propmatexp) gives \\[\\begin{equation}\n\\sigma(X)=1+\\eta^2(X-\\Gamma(X))-\\eta^2(\\Gamma(X)),\n(\\#eq:gutsquare)\n\\end{equation}\\] which shows that \\[\\begin{equation}\n1-\\eta^2(\\Gamma(X))\\leq\\sigma(X)\\leq 1+\\eta^2(X-\\Gamma(X)).\n(\\#eq:propsbounds)\n\\end{equation}\\]\nNote that it follows from @ref(eq:propsbounds) that \\(\\sigma(X)\\geq 1-\\eta^2(\\Gamma(X))\\), with equality if and only if \\(X=\\Gamma(X)\\).\n\nThe Guttman transform is\n\nself-scaling (a.k.a. homogeneous of degree zero) because \\(\\Gamma(\\alpha X)=\\Gamma(X)\\) for all \\(-\\infty&lt;\\alpha&lt;+\\infty\\). With our definition @ref(eq:bdef) of \\(B(X)\\) we also have \\(\\Gamma(0)=0\\).\nself-centering, because \\(\\Gamma(X+e\\mu')=\\Gamma(X)\\) for all \\(\\mu\\in\\mathbb{R}^p\\).\nBounded\nLipschitz\n\n\n::: { ,proof} We already know, from the CS inequality, that \\[\\begin{equation}\n\\rho(X)\\leq\\eta(X).\n(\\#eq:csagain)\n\\end{equation}\\]\nWith the Guttman transform in hand we can apply CS once more, and find\n\\[\\begin{equation}\n\\rho(X)=\\text{tr}\\ X'B(X)X=\\text{tr}\\ X'V\\Gamma(X)\\leq\\eta(X)\\eta(\\Gamma(X))\n(\\#eq:csagainagain)\n\\end{equation}\\]\nNote that the Guttman transform is bounded. In fact, using the Euclidean norm throughout, \\[\n\\Gamma(X)\\leq\\|V^+\\|\\|B(X)X\\|\n\\] Now \\[\nB(X)X=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\delta_{ij}\\frac{x_i-x_j}{d_{ij}(X)}(e_i-e_j),\n\\] and thus\n\\[\n\\|B(X)X\\|\\leq\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\delta_{ij}\\left\\|\\frac{x_i-x_j}{d_{ij}(X)}\\right\\|\\|e_i-e_j\\|=\\sqrt{2}\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\delta_{ij},\n\\] and \\[\n\\|\\Gamma(X)\\|\\leq\\sqrt{2}\\|V^+\\|\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\delta_{ij}.\n\\] In fact \\[\nB(X)X-B(Y)Y=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\delta_{ij}\\left\\{\\frac{x_i-x_j}{d_{ij}(X)}-\\frac{y_i-y_j}{d_{ij}(Y)}\\right\\}(e_i-e_j),\n\\] and thus \\[\n\\|\\Gamma(X)-\\Gamma(Y)\\|\\leq 2\\|V^+\\|\n\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\delta_{ij},\n\\] and thus the Guttman transform is Lipschitz. ::: ;’’’’ytyh ### Subdifferentials\n\n\n5.4.2 Derivative\nThe basic smacof algorithm, which is the main building block for most of the MDS techniques discussed in this book, updates the configuration \\(X^{(k)}\\) in iteration \\(k\\) by\n\\[\\begin{equation}\nX^{(k+1)}=\\Gamma(X^{(k)})=V^+B(X^{(k)})X^{(k)}.\n(\\#eq:smacofupdate)\n\\end{equation}\\]\nso that first \\(X^{(1)}=\\Gamma(X^{(0)})\\), then \\(X^{(2)}=\\Gamma(X^{(1)})=\\Gamma(\\Gamma(X^{(0)}))\\), and generally \\(X^{(k)}=\\Gamma^k(X^{0}),\\) where \\(\\Gamma^k\\) is the k-times composition (or iteration) of \\(\\Gamma.\\)\nWe shall show in this chapter that as \\(k\\rightarrow\\infty\\) both\n\\(\\sigma(X^{(k+1)})-\\sigma(X^{(k)})\\rightarrow 0\\), and \\(\\eta^2(X^{(k)}-X^{(k+1)})=(X^{(k+1)}-X^{(k)})'V(X^{(k+1)}-X^{(k)})\\rightarrow 0\\). The iterations stop either if \\(\\sigma(X^{(k)})-\\sigma(X^{(k+1)})&lt;\\epsilon\\) or if \\(\\eta^2(X^{(k)}-X^{(k+1)})&lt;\\epsilon\\), where the \\(\\epsilon\\) are small cut-off numbers chosen by the user, or if we reach a user-defined maximum number of iterations, and we give up on convergence. The user also chooses if the stop criterion is based on function value changes or configuration changes.\nSome quick remarks on implementation. We only have to compute \\(V^+\\) once, but premultiplying by a full symmetric matrix in each iteration does add quite a few multiplications to the algorithm. If all \\(w_{ij}\\) are one, then \\(V^+=\\frac{1}{n}J\\) and thus \\(\\Gamma(X^{(k)})=\\frac{1}{n}B(X^{(k)})X^{(k)}\\). In fact we do not even have to carry out this division by \\(n\\), because the basic algorithm is self scaling. which means in this context that \\(\\Gamma(\\alpha X)=\\Gamma(X)\\) for all \\(\\alpha\\geq 0\\).\n\n\n5.4.3 Global Convergence\nThe iterations in @ref(eq:smacofupdate) start at some \\(X^{(0)}\\) and then generate five sequences of non-negative numbers. Define \\(\\lambda(X):=\\rho(X)/\\eta(X)\\) and \\(\\Gamma(X):=\\eta^2(X-\\Gamma(X))\\). The five sequences we will look at are\n\\[\\begin{align}\n\\begin{split}\n\\sigma_k&:=\\sigma(X^{(k)}),\\\\\n\\rho_k&:=\\rho(X^{(k)}),\\\\\n\\eta_k&:=\\eta(X^{(k)}),\\\\\n\\lambda_k&:=\\lambda(X^{(k)}),\\\\\n\\Gamma_k&:=\\Gamma(X^{(k)}),\n\\end{split}\n(\\#eq:smacofseq)\n\\end{align}\\]\nDepend on \\(X^{(0)}\\)\nZangwill\nArgyros\n\n5.4.3.1 From the CS Inequality\n\n \n\n\\(\\sigma_k\\) is a decreasing sequence, bounded below by 0.\n\\(\\rho_k\\), \\(\\eta_k\\), amd \\(\\lambda_k\\) are increasing sequences, bounded above by 1.\n\\(\\Gamma_k\\) is a null-sequence.\n\n\nTo prove convergence of these sequences we slightly modify and extend the proofs in De Leeuw (1977) and De Leeuw and Heiser (1977) (while I say to myself: that’s 44 years ago).\n\nProof. \n\nFor each \\(X\\in\\mathbb{R}^{n\\times p}\\) we have \\(\\rho(X)\\leq\\eta(X)\\) and thus \\(\\lambda(X)\\leq 1\\).\nFor each \\(X,Y\\in\\mathbb{R}^{n\\times p}\\) we have \\(\\rho(X)\\geq\\text{tr}\\ X'B(Y)Y\\) and thus \\(\\rho(X)\\geq\\text{tr}\\ X'V\\Gamma(Y)\\). Taking \\(X=\\Gamma(Y)\\) we see that \\(\\rho(X)\\geq\\eta^2(\\Gamma(Y))\\). Now \\(\\sigma(\\Gamma(Y))=1-2\\rho(\\Gamma(Y))+\\eta^2(\\Gamma(Y))\\leq 1-\\eta^2(\\Gamma(Y))\\) and thus for all \\(X\\) \\(\\eta^2(\\Gamma(X)) \\leq 1\\).\nFor each \\(X\\in\\mathbb{R}^{n\\times p}\\) we have \\(\\rho(X)=\\text{tr}\\ X'B(X)X\\) and thus \\(\\rho(X)\\leq\\eta(X)\\eta(\\Gamma(X))\\) and thus \\(\\lambda(X)\\leq\\eta(\\Gamma(X))\\)\n\n\\[\n\\rho(X^{(k)})=\\text{tr}\\ \\{X^{(k)}\\}'VX^{(k+1)}\\leq\\eta(X^{(k)})\\eta(X^{(k+1)}),\n\\]\n\\[\n\\rho(X^{(k+1)})\\geq\\text{tr}\\ \\{X^{(k+1)}\\}'B(X^{(k)})X^{(k)}=\\eta^{2}(X^{(k+1)}),\n\\]\n\\[\n\\eta(X^{(k)})\\leq\\lambda(X^{(k)})\\leq\\eta(X^{(k+1)})\n\\]\nand\n\\[\n\\rho(X^{(k)})\\leq\\frac{\\eta(X^{(k)}}{X^{(k+1)}}\\rho(X^{(k+1)})\\leq\\rho(X^{(k+1)})\n\\]\n\n\n\n5.4.3.2 From Majorization\nSmacof is based on the majorization, valid for all configurations \\(X\\) and \\(Y\\),\n\\[\\begin{equation}\n\\sigma(X)\\leq 1+\\eta^2(X-\\Gamma(Y))-\\eta^2(\\Gamma(Y)),\n(\\#eq:upbmajineq)\n\\end{equation}\\]\nwith equality if and only if \\(X\\propto Y\\). If \\(Y=\\alpha X\\) for some \\(\\alpha\\) then \\[\\begin{equation}\n\\sigma(X)=1+\\eta^2(X-\\Gamma(Y))-\\eta^2(\\Gamma(Y)),\n(\\#eq:upbmajeq)\n\\end{equation}\\] and specifically we have @ref(eq:upbmajeq) if \\(Y=X\\).\nNow suppose we have an \\(Y\\) with \\(Y\\not=\\Gamma(Y)\\). If \\(\\eta^2(X-\\Gamma(Y))\\leq\\eta^2(Y-\\Gamma(Y))\\) then\n\\[\\begin{align}\n\\begin{split}\n\\sigma(X)%\\leq 1+\\eta^2(X-\\Gamma(Y))-\\eta^2(\\Gamma(Y))\\leq\\\\\n&\\leq 1+\\eta^2(Y-\\Gamma(Y))-\\eta^2(\\Gamma(Y))=\\sigma(Y)\n\\end{split}\n(\\#eq:upbmajimp)\n\\end{align}\\]\nThe obvious choice for \\(X\\) is \\(X=\\Gamma(Y)\\), which makes \\(\\eta^2(X-\\Gamma(Y))=0\\), and thus\n\\[\\begin{equation}\n\\sigma(X)\\leq 1-\\eta^2(\\Gamma(Y))&lt;\n1+\\eta^2(Y-\\Gamma(Y))-\\eta^2(\\Gamma(Y))=\\sigma(Y)\n(\\#eq:upbmajmin)\n\\end{equation}\\]\n\n\n5.4.3.3 From Ratio of Norms\nDe Leeuw (1977)\nDC Algorithm\nRobert\n\n\n\n5.4.4 Component Rotated Smacof\nConsider the modified smacof iterations \\(\\tilde X^{(k+1)}=X^{(k+1)}L^{(k+1)}\\), where \\(L^{(k+1)}\\) are the normalized eigenvectors of \\(\\{X^{(k+1)}\\}^TVX^{(k+1)}\\). Then\n\\[\n\\sigma(\\tilde X^{(k)})=\\sigma(X^{(k)})\n\\] Thus the modified update produces the same sequence of stress values as the basic update. Also \\[\n\\Gamma(\\tilde X^{(k)})=\\Gamma(X^{(k)})L^{(k)}\n\\] Thus \\(\\tilde X^{(k)}\\) and \\(X^{(k)}\\) differ by a rotation for each \\(k\\). It follows that we can actually compute \\(\\tilde X^{(k)}\\) from the basic sequence \\(X^{(k)}\\) by rotating the \\(X^{(k)}\\) to principal components. Specifically if \\(X_\\infty\\) is a subsequential limit of \\(X^{(k)}\\) then the corresponding limit of \\(\\tilde X^{(k)}\\) is \\(X_\\infty\\) rotated to principal components. Modifying the intermediate updates is just a waste of time, we can simply rotate the final smacof solution. And we should, as we explain in the next section, @ref(minlocconv).\n\n\n5.4.5 Local Convergence\n\\[\n\\mathcal{D}\\Gamma(X)(Y)=V^+\\left\\{B(X)Y-\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n} w_{ij}\\frac{\\delta_{ij}}{d_{ij}(X)}\\left(\\frac{\\text{tr}\\ Y'A_{ij}X}{d_{ij}^2(X)}\\right)A_{ij}\\right\\}\n\\]\nFor any X: one zero eigenvalue \\(\\mathcal{D}\\Gamma(X)(X)=0\\)\nIf on \\(\\mathbb{R}^{n\\times p}\\) then an additional \\(p\\) zero eigenvalues \\(\\mathcal{D}\\Gamma(X)(e\\mu^T)=0\\)\nFor \\(X=\\Gamma(X)\\) and \\(M\\) anti-symmetric: \\(\\frac12 p(p-1)E\\) unit eigenvalues \\(\\mathcal{D}\\Gamma(X)(XM)=\\Gamma(X)M=XM\\)\n\n5.4.5.1 Cross Product Iterations\nMap of C into C. No rotational indeterminacy. Same stress sequence.\n\\[\nC^{(k+1)}=V^+B(C^{(k)})C^{(k)}B(C^{(k)})V^+\n\\] Map of \\(D\\) into \\(D\\). Guttman transform as function of distances. Not very nice.\n\\[\nD^{(k+1)}:=D(X^{(k+1)})=D(\\Gamma(X^{(k)}))\n\\]\n\n\n5.4.5.2 Rotation to PC\nWe suppose the configfuration \\(X\\) is \\(n\\times p\\), with rank \\(p\\). If the singular value decomposition is \\(X=K\\Lambda L'\\) then the rotation to principle components is \\(\\Gamma(X):=K\\Lambda=XL\\). Thus \\(\\mathcal{D}\\Gamma(X)(Y)=YL+X\\mathcal{D}L(X)(Y)\\). So we need to compute \\(\\mathcal{D}L(X)\\), with \\(L\\) the right singular vectors of \\(X\\), i.e. the eigenvectors of \\(X^TX\\). We use the methods and results from De Leeuw (2007), which were applied to similar problems in De Leeuw (2008), De Leeuw and Sorenson (2012), and De Leeuw (2016).\n\nIf the \\(n\\times p\\) matrix has rank \\(p\\), singular value decomposition \\(X=K\\Lambda L^T\\), with all singular values different, then \\(\\Gamma(X+\\Delta)=\\Gamma(X)+\\Delta L+XLM+o(\\|\\Delta\\|)\\), where \\(M\\) is antisymmetric with off-diagonal elements\n\\[\\begin{equation}\nm_{ij}=\\frac{\\lambda_is_{ij}+\\lambda_js_{ji}}{\\lambda_i^2-\\lambda_j^2}.\n(\\#eq:minsvdmsolve)\n\\end{equation}\\]\n\n\nProof. The proof involves computing the derivatives of the singular value decomposition of \\(X\\), which is defined by the equations\n\\[\\begin{align}\nXL&=K\\Lambda,(\\#eq:minsvd1)\\\\\nX^TK&=L\\Lambda,(\\#eq:minsvd2)\\\\\nK^TK&=I,(\\#eq:minsvd3)\\\\\nL^TL&=LL^T=I.(\\#eq:minsvd4)\n\\end{align}\\]\nWe now perturb \\(X\\) to \\(X+\\Delta\\), which perturbs \\(L\\) to \\(L+L_\\Delta+o(\\|\\Delta\\|)\\), \\(K\\) to \\(K+K_\\Delta+o(\\|\\Delta\\|)\\), and \\(\\Lambda\\) to \\(\\Lambda+\\Lambda_\\Delta+o(\\|\\Delta\\|)\\). Substutute this into the four SVD equations for \\(X+\\Delta\\) and keep the linear terms.\n\\[\\begin{align}\nXL_\\Delta+\\Delta L&=K_\\Delta\\Lambda+K\\Lambda_\\Delta,(\\#eq:minsvdperb1)\\\\\nX^TK_\\Delta+\\Delta^TK&=L_\\Delta\\Lambda+L\\Lambda_\\Delta,(\\#eq:minsvdperb2)\\\\\nL_\\Delta^TL+L^TL_\\Delta&=0,(\\#eq:minsvdperb3)\\\\\nK_\\Delta^TK+K^TK_\\Delta&=0.(\\#eq:minsvdperb4)\n\\end{align}\\]\nDefine \\(M:=L^TL_\\Delta\\) and \\(N:=K^TK_\\Delta\\). Then equations @ref(eq:minsvdperb3) and @ref(eq:minsvdperb4) say that \\(M\\) and \\(N\\) are antisymmetric. Also define \\(S:=K^T\\Delta L\\). Premultiplying equation @ref(eq:minsvdperb1) by \\(K^T\\) and @ref(eq:minsvdperb2) by \\(L^T\\) gives\n\\[\\begin{align}\n\\Lambda M+S&=N\\Lambda+\\Lambda_\\Delta,(\\#eq:minsvdred1)\\\\\n\\Lambda N+S^T&=M\\Lambda+\\Lambda_\\Delta.(\\#eq:minsvdred2)\n\\end{align}\\]\nEither of these two equations gives, using the antisymmetry, and thus hollowness, of \\(M\\) and \\(N\\), that \\(\\Lambda_\\Delta=\\text{diag}(S)\\). Define the hollow matrix \\(U:=S-\\text{diag}(S)\\). Then\n\\[\\begin{align}\n\\Lambda M-N\\Lambda&=U,(\\#eq:minsvdu1)\\\\\n\\Lambda N-M\\Lambda&=U^T.(\\#eq:minsvdu2)\n\\end{align}\\]\nPremultiply @ref(eq:minsvdu1) and postmultiply @ref(eq:minsvdu2) by \\(\\Lambda\\).\n\\[\\begin{align}\n\\Lambda^2 M-\\Lambda N\\Lambda&=\\Lambda U,(\\#eq:minsvdu3)\\\\\n\\Lambda N\\Lambda-M\\Lambda^2&=U^T\\Lambda.(\\#eq:minsvdu4)\n\\end{align}\\]\nIf we add these two equations we can solve for the off-diagonal elements of \\(M\\) and find the expression in the theorem. Since \\(L_\\Delta=LM\\) this completes the proof.\n\n\n\n\n5.4.6 Data Asymmetry\nThe non-basic situation in which there are asymmetric weights and/or asymmetric dissimilarities in basic MDS is analyzed in De Leeuw (1977), although it is just standard linear least squares projection theory. We give a slightly different partitionng ofd the sum of squares here. Note that it is not even necessary that the weights and dissimilarities are hollow and/or non-negative.\nWe decompose the weights and dissimilarities additively into a symmetric and anti-symmetric part. Thus \\(w_{ij}=w_{ij}^S+w_{ij}^A\\) and \\(\\delta_{ij}=\\delta_{ij}^S+\\delta_{ij}^A\\). Now in general if \\(A\\) is anti-symmetric and \\(B\\) is symmetric, then \\(\\text{tr}\\ AB=0\\). Also their Hadamard (element-wise) product \\(A*B\\) is anti-symmetric, and the Hadamard product of two anti-symmetric matrices is symmetric. Using these rules gives after some calculation \\[\\begin{align}\n\\begin{split}\n\\sigma(X)&=\\frac14\\sum_{i=1}^n\\sum_{j=1}^nw_{ij}(\\delta_{ij}-d_{ij}(X))^2=\\\\&=\\frac14\\sum_{i=1}^n w_{ii}^{\\ }\\delta_{ii}^2+\n\\frac12\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}^S\\left\\{\\underline{\\delta}_{ij}-d_{ij}(X)\\right\\}^2+\\frac12\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}\\frac{w_{ij}w_{ji}}{w_{ij}^S}\\{\\delta_{ij}^A\\}^2,\n\\end{split}(\\#eq:partiunsym)\n\\end{align}\\] where \\[\\begin{equation}\n\\underline{\\delta}_{ij}:=\\delta_{ij}^S+\\frac{w_{ij}^A\\delta_{ij}^A}{w_{ij}^S}.\n(\\#eq:defofunddelta)\n\\end{equation}\\] Thus minimizing stress in the case of asymmetric weights and dissimilarities, which even can be non-hollow and non-positive, reduces to a symmmetric basic MDS problem for adjusted dissimilarities defined by equation @ref(eq:defofunddelta). If the original weights and dissimilarities are non-negative, then so are the weights \\(w_{ij}^S\\) and the dissimilarities \\(\\underline{\\delta}_{ij}\\).\n\n\n5.4.7 Replications\nIf there are replications in basic MDS we can use a simple partitioning of stress to reduce the problem to standard form. We start with \\[\\begin{equation}\n\\sigma(X)=\\frac12\\sum_{k=1}^m\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ijk}(\\delta_{ijk}-d_{ij}(X))^2.\n\\#(eq:inddiffstress)\n\\end{equation}\\] Let \\[\\begin{align}\nw_{ij\\bullet}&=\\sum_{k=1}^m w_{ijk}(\\#eq:wbul1)\\\\\n\\delta_{ij\\bullet}&=\\frac{\\sum_{k=1}^m w_{ijk}\\delta_{ijk}}{w_{ij\\bullet}}(\\#eq:wbul2).\n\\end{align}\\] Then \\[\\begin{equation}\n\\sigma(X)=\\frac12\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n} w_{ij\\bullet}(\\delta_{ij\\bullet}-d_{ij}(X))^2+\\frac12\\sum_{k=1}^m\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n} w_{ijk}(\\delta_{ijk}-\\delta_{ij\\bullet})^2,\n(\\#eq:partinddiff)\n\\end{equation}\\] and it suffices to minimize the first term, which is a standard basic MDS problem.\nIn the nonmetric case, in which in principle each of the \\(\\Delta_k\\) can be transformed, we must alternate minimization of #ref(eq:inddiffstress) over the \\(\\Delta_k\\) and minimization of @ref(eq:partinddiff) over \\(X\\). In the case in which \\(X_k\\) is different pover replications we use the methods of chapter @ref(chindif).\n\n\n5.4.8 Negative Dissimilarities\n\\[\\begin{equation}\n\\sigma(X)=1-\\sum_{k\\in\\mathcal{K}_{1+}} w_k\\delta_kd_k(X)\n+\\sum_{k\\in\\mathcal{K}_{1-}} w_k|\\delta_k|d_k(X)+\\frac12\\sum_{k\\in\\mathcal{K}} w_kd_k^2(X)).\n(\\#eq:disneg)\n\\end{equation}\\]\nSplit rho\nHeiser (1991)\n\n\n5.4.9 Normalization\nIn actual computer output using the scaling in formula @ref(eq:scaldiss1) and @ref(eq:scaldiss1) has some disadvantages. There are, say, \\(M\\) non-zero weights. The summation in #ref(eq:stressall) is really over \\(M\\) terms only. If \\(n\\) is at all large the scaled dissimilarities, and consequently the distances and the configuration, will become very small. Thus, in actual computation, or at least in the computer output, we scale our dissimilarities as \\(\\frac12\\mathop{\\sum\\sum}_{1\\leq j&lt;i\\leq n} w_{ij}^{\\ }\\delta_{ij}^2=M\\). So, we scale our dissimilarities to one in formulas and to \\(M\\) in computations. Thus the computed stress will b\nIn fact, we do not even use it in our computer programs, except at the very last moment when we return the final stress after the algorithm has completed.\n\n\n5.4.10 Unweighting\nConsider the general problem of minimizing a least squares loss function, defined as \\(f(x):=(x-y)'W(x-y)\\) over \\(x\\) in some set \\(X\\), where \\(W\\) is a symmetric weight matrix. Sometimes \\(W\\) complicates the problem, maybe because it is too big, too full, too singular, or even indefinite. We will use iterative majorization to give \\(W\\) a more subordinate role. See also Kiers (1997) and Groenen, Giaquinto, and Kiers (2003).\nSuppose \\(z\\) is another element of \\(X\\). Think of it as the current best approximation to \\(y\\) that we have, which we want to improve. Then\n\\[\\begin{align}\n\\begin{split}\nf(x)&=(x-y)'W(x-y)\\\\\n&=((x-z)+(z-y))'W((x-z)+(z-y))\\\\\n&=f(z)+2(x-z)'W(z-y)+(x-z)'W(x-z)\n\\end{split}\n(\\#eq:unwgth)\n\\end{align}\\]\nNow choose a non-singular \\(V\\) such that \\(W\\lesssim V\\) and define \\(u:=V^{-1}W(z-y)\\). Then we have the majorization\n\\[\\begin{equation}\nf(x)\\leq f(z)+2(x-z)'W(z-y)+(x-z)'V(x-z)=\\\\\nf(z)+2(x-z)'Vu+(x-z)'V(x-z)=\\\\\nf(z)+(x-(z-u))'V(x-(z-u))-u'Vu.\n(\\#eq:compsq)\n\\end{equation}\\]\nHere are some ways to choose \\(V\\). We use \\(\\lambda_{\\text{max}}(W)\\) and \\(\\lambda_{\\text{min}}(W)\\) for the largest and smallest eigenvalues of the symmetric matrix \\(W\\).\nFor any \\(W\\) we can choose \\(V=\\lambda_{\\text{max}}(W)I\\). Or, more generally, \\(V=\\lambda_{\\text{max}}(D^{-1}W)D\\) for any positive definite \\(D\\). If \\(W\\) is singular we can choose \\(V=W+\\epsilon D\\) for any positive definite \\(D\\). And in the unlikely case that \\(W\\) is indefinite we can choose \\(V=W+(\\epsilon-\\lambda_{\\text{min}}(W))I\\). But if \\(W\\) is indefinite we have more serious problems.\nIn appendix @ref(apcodemathadd) the R function lsuw(), implements the iterative majorization algorithm minimizing \\((x-y)'W(x-y)\\) over \\(x\\) in some set \\(X\\). One of the parameters of lsuw() is a function proj(), which projects a vector on \\(X\\) in the metric define by \\(V\\). The projection could be on the positive orthant, on a cone with isotone vectors, on a linear subspace, on a sphere, on a set of low-rank matrices, and so on.\nAs an example choose \\(W\\) as a banded matrix of order 10 with \\(w_{ij}=1\\) if \\(|i-j|\\leq 3\\) and \\(i\\not= j\\), \\(w_{ij}=i\\) if \\(i=j\\), and \\(w_{ij}=0\\) otherwise. We require all 10 elements of \\(x\\) to be the same, and we use \\(V=\\lambda_{\\text{max}}(W)I\\) (the default).\nThe iterations are\n\nw&lt;-ifelse(outer(1:10,1:10,function(x,y) abs(x-y) &lt;= 3),1,0)\nw &lt;- w + diag(0:9)\nh1 &lt;- lsuw(1:10, w, projeq)\n\nIf we use \\(\\lambda_{\\text{max}}(D^{-1}W)D\\) with \\(D=\\text{diag}(W)\\) for \\(V\\) we see the following majorization iterations.\n\nd &lt;- diag(w)\nv &lt;- max(eigen((1 / d) * w)$values) * diag(d)\nh2 &lt;- lsuw(1:10, w, v = v, projeq)\n\nSo the second method of choosing \\(V\\) is a tiny bit less efficient in this case, but it really does not make much of a difference. In both cases \\(x\\) is 6.3009558, 6.3009558, 6.3009558, 6.3009558, 6.3009558, 6.3009558, 6.3009558, 6.3009558, 6.3009558, 6.3009558 with function value 595.6699029.\nApply to stress and to\nInner iterations, use one.\n\\[\n\\sigma_c(X):=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}\\mathop{\\sum\\sum}_{1\\leq k&lt;l\\leq n}w_{ijkl}(\\delta_{ij}-d_{ij}(X))(\\delta_{kl}-d_{kl}(X))\n\\] If \\(A\\leq B\\) (elementwise) then \\(\\sum\\sum(b_{ij}-a_{ij})(x_i-x_j)^2\\geq 0\\) and thus \\(V(A)\\lesssim V(B)\\).\n\n5.4.10.1 Symmetric non-negative matrix factorization\n\\(w_{ij}=\\sum_{s=1}^rv_{is}^2v_{js}^2\\) for all \\(i\\not= j\\). Then \\[\n\\sigma(X)=\\sum_{s=1}^p\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}(\n\\delta_{ijs}-d_{ijs}(X))^2\n\\] with \\(\\delta_{ijs}:=v_{is}v_{js}\\delta_{ij}\\) and \\(d_{ijs}(X):=v_{is}v_{js}d_{ij}(X)\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Minimization of Basic Stress</span>"
    ]
  },
  {
    "objectID": "minimization.html#propenvelopes",
    "href": "minimization.html#propenvelopes",
    "title": "5  Minimization of Basic Stress",
    "section": "5.5 Stress Envelopes",
    "text": "5.5 Stress Envelopes\nintro\n\n5.5.1 CS Majorization\n\n\\(\\sigma\\) is the lower envelop of an infinite number of convex quadratics.\n\n\nProof. By the CS inequality\n\\[\\begin{equation}\nd_{ij}(X)=\\max_Y \\frac{\\text{tr}\\ X^TA_{ij}Y}{d_{ij}(Y)},\n(\\#eq:dasmax)\n\\end{equation}\\]\nwhich implies\n\\[\\begin{equation}\n\\sigma(X)=\\min_Y\\left(1-\\text{tr}\\ X^TB(Y)Y+\\frac12\\text{tr}\\ X^TVX\\right),\n(\\#eq:sigasmin)\n\\end{equation}\\]\nwhich is what we set out to prove.\n\nWe can use the lower envelop of a finite number of the quadratics from theorem @ref(thm:proplowenv) to approximate stress. This is illustrated graphically, using a small example in which the configuration is a convex combination of two fixed configurations. Thus in the example stress is a function of the single parameter \\(0\\leq\\lambda\\leq 1\\) defining the convex combination. In figure @ref(fig:upperfig) stress is in red, and we have used the three quadratics corresponding with \\(\\lambda\\) equal to 0.25, 0.5, 0.75. The maximum of the three quadratics is in blue, and the approximation is really good, in fact almost perfect in the areas where the blue is not even visible. As an aside, we also see three points in the figure where stress is not differentiable. The minimum of the three quadratics is also not differentiable at a point, but that point is different from the points where stress is non-smooth.\nNote that by definition stress and the lower envelop of the quadratics are equal at the three points where \\(\\lambda\\) is 0.25, 0.5, 0.75, i.e at the three vertical lines in the plot.\n\n\n\n\n\nPiecewise Quadratic Upper Approximation\n\n\n\n\n\n\n5.5.2 AM/GM Minorization\nInstead of approximating stress from above, we can also approximate it from below.\n\n\\(\\sigma\\) is the upper envelop of an infinite number of quadratics.\n\n\nProof. By AM/GM\n\\[\\begin{equation}\nd_{ij}(X)\\leq\\min\n\\frac12\\frac{1}{d_{ij}(Y)}\\{d_{ij}^2(X)+d_{ij}^2(Y)\\}\n(\\#eq:dasmin)\n\\end{equation}\\]\nThus\n\\[\\begin{equation}\n\\sigma(X)=\\max_Y \\left(1-\\frac12\\rho(Y)+\\frac12\\text{tr}\\ X'(V-B(Y))X\\right)\n(\\#eq:sigmax)\n\\end{equation}\\]\n\n\n\n\n\n\nPiecewise Quadratic Lower Approximation\n\n\n\n\nAgain we illustrate this result using a finite number of quadratics. In figure @ref(fig:lowerfig) we choose \\(\\lambda\\) equal to 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. Although we now use 11 quadratics, and thus force the envelop to be equal to the function at the 11 points on the vertical lines in the plot, the approximation is poor. This seems to be mainly because the convex-like function stress must be approximated from below by quadratics which are often concave.\n\n\n5.5.3 Dualities\n\\[\\begin{multline}\n\\min_X\\sigma(X)=\\min_Y \\left(1 - \\frac12\\text{tr}\\ Y'B(Y)V^+B(Y)Y\\right)=\\\\1-\\frac12\\max_Y\\text{tr}\\ Y'B(Y)V^+B(Y)Y.\n\\end{multline}\\]\nThus minimizing stress is equivalent to maximizing \\(\\eta^2(V^+B(X)X)\\).\n\\[\n\\min_X\\sigma(X)\\geq\\max_{B(Y)\\lesssim V}(1-\\rho(Y))\n\\]\nBy the minimax inequality \\(\\min_X\\sigma(X)=\\min_X\\max_Y\\theta(X,Y)\\geq\\max_Y\\min_X\\theta(X,Y).\\) Now \\(\\min_X\\theta(X,Y)\\) is \\(-\\infty\\), unless \\(B(Y)\\lesssim V\\), in which case \\(\\min_X\\theta(X,Y)=0\\). Thus \\[\n\\max_Y\\min_X\\theta(X,Y)=\\max_{B(Y)\\lesssim V}(1-\\rho(Y))=1-\\min_{B(Y)\\lesssim V}\\ \\rho(Y)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Minimization of Basic Stress</span>"
    ]
  },
  {
    "objectID": "minimization.html#smacofcoef",
    "href": "minimization.html#smacofcoef",
    "title": "5  Minimization of Basic Stress",
    "section": "5.6 Smacof in Coefficient Space",
    "text": "5.6 Smacof in Coefficient Space",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Minimization of Basic Stress</span>"
    ]
  },
  {
    "objectID": "minimization.html#smacofnewton",
    "href": "minimization.html#smacofnewton",
    "title": "5  Minimization of Basic Stress",
    "section": "5.7 Newton in MDS",
    "text": "5.7 Newton in MDS\n\n5.7.1 Regions of Attraction\n\ndelta &lt;- as.matrix (dist (diag (4)))\ndelta &lt;- delta * sqrt (2 / sum (delta ^ 2))\n\n\n5.7.1.1 Smacof\nWe use the smacof() function from the code in the appendix with 100 different starting points of \\(\\theta\\), equally spaced on the circle. Figure @ref(fig:histsmacof) is a histogram of the number of smacof iterations to convergence within 1e-15. In all cases smacof converges to a local minimum in coefficient space, never to a saddle point. Figure @ref(fig:pathsmacof) shows which local minima are reached from the different starting points. This shows, more or less contrary to what Trosset and Mathar (1997) suggests, that non-global minima can indeed be points of attraction for smacof iterations.\n\n\n\n\n\nHistogram Number of Smacof Iterations\n\n\n\n\n\n\n\n\n\nPath Endpoints of Smacof Iterations\n\n\n\n\n\n\n5.7.1.2 Newton\nWe repeat the same exercise with Newton’s method, which also converges from all 100 starting points in our example. In higher dimensions we may not be so lucky.\nThe histogram of iteration counts is in figure @ref(fig:histnewton). It shows in this example that smacof needs about 10 times the number of iterations that Newton needs. Because smacof iterations are much less expensive than Newton ones, this does not really say much about computing times. If we look at figure @ref(fig:pathnewton) we see the problem with non-safeguarded Newton. Although we have fast convergence from all 100 starting points, Newton converges to a saddle point in 45 cases.\n\n\n\n\n\nHistogram Number of Newton Iterations\n\n\n\n\n\n\n\n\n\nPath Endpoints of Newton Iterations",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Minimization of Basic Stress</span>"
    ]
  },
  {
    "objectID": "minimization.html#propdistsmo",
    "href": "minimization.html#propdistsmo",
    "title": "5  Minimization of Basic Stress",
    "section": "5.8 Distance Smoothing",
    "text": "5.8 Distance Smoothing\nIn sections @ref(propconvex) and @ref(propstationary) we show the lack of differentiability in basic MDS is not a serious problem in the actual computation of local minima.\nThere is another rather straightforward way to circumvent the differentiabily issue, which actually may have additional benefits. The idea is to use an approximation of the Euclidean distance that is as close as possible on the positive real axis, but smooth at zero. This was first applied in unidimensional MDS by Pliner (Pliner (1986), Pliner (1996)) and later taken up and generalized to pMDS for arbitrary \\(p\\), and even for arbitrary Minkovski metrics, by Groenen, Heiser, and Meulman (1998) and Groenen, Heiser, and Meulman (1999). They coined the term distance smoothing for this variation of the \\(\\textrm{smacof}\\) framework for MDS.\nPliner (1986) uses a smooth approximation of the sign function, while Groenen, Heiser, and Meulman (1998) borrow the smooth Huber approximation of the absolute value function from robust regression. We use another classical and efficient approximation \\(|x|\\approx\\sqrt{x^2+\\epsilon^2}\\) to the absolute value function, used in image analysis, location analysis, and computational geometry (De Leeuw (2018), Ramirez et al. (2014)). In our context that becomes \\(d_{ij}(X)\\approx d_{ij}(X,\\epsilon):=\\sqrt{d_{ij}^2(X)+\\epsilon^2}\\). Note that on the non-negative reals \\[\\begin{equation}\n\\max(\\epsilon,d_{ij}(X))\\leq d_{ij}(X,\\epsilon)\\leq d_{ij}(X)+\\epsilon.\n(\\#eq:smoothineq)\n\\end{equation}\\] Figures @ref(fig:dfsmoother) and @ref(fig:ddsmoother) show the absolute value function and its derivative are approximated for \\(\\epsilon\\) equal to 0, 0.01, 0.05, 0.1, 0.5.\n\n\n\n\n\nFunction for Various Epsilon\n\n\n\n\n\n\n\n\n\nDerivative for Various Epsilon\n\n\n\n\nThe distance smoother we use fits nicely into \\(\\textrm{smacof}\\). Define \\(X_\\epsilon:=\\begin{bmatrix}X&\\mid&\\epsilon I\\end{bmatrix}\\). Then \\(d_{ij}(X_\\epsilon)=\\sqrt{d_{ij}^2(X)+\\epsilon^2}\\). Thus we can define \\[\\begin{equation}\n\\sigma_\\epsilon(X):=\\sigma(X_\\epsilon)=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\delta_{ij}- d_{ij}(X_\\epsilon))^2,\n(\\#eq:sigmaepsilon)\n\\end{equation}\\] with \\(\\rho_\\epsilon\\) and \\(\\eta^2_\\epsilon\\) defined in the same way.\nFor a fixed \\(\\epsilon&gt;0\\) now \\(d_{ij}(X_\\epsilon)\\), and thus stress, is (infinitely many times) differentiable on all of \\(\\mathbb{R}^{n\\times p}\\). Moreover \\(d_{ij}(X,\\epsilon)\\) is convex in \\(X\\) for fixed \\(\\epsilon\\) and jointly convex in \\(X\\) and \\(\\epsilon\\), and as a consequence so are \\(\\rho_\\epsilon\\) and \\(\\eta^2_\\epsilon\\).\n\n\n\n\n\nJan de Leeuw, Gilbert Saporta, Yutaka Kanaka in Kolkata, December 1985\n\n\n\n\n\n\n\n\nDe Leeuw, J. 1977. “Applications of Convex Analysis to Multidimensional Scaling.” In Recent Developments in Statistics, edited by J. R. Barra, F. Brodeau, G. Romier, and B. Van Cutsem, 133–45. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\n———. 1994. “Block Relaxation Algorithms in Statistics.” In Information Systems and Data Analysis, edited by H. H. Bock, W. Lenski, and M. M. Richter, 308–24. Berlin: Springer Verlag. https://jansweb.netlify.app/publication/deleeuw-c-94-c/deleeuw-c-94-c.pdf.\n\n\n———. 2007. “Derivatives of Generalized Eigen Systems with Applications.” Preprint Series 528. Los Angeles, CA: UCLA Department of Statistics.\n\n\n———. 2008. “Derivatives of Fixed-Rank Approximations.” Preprint Series 547. Los Angeles, CA: UCLA Department of Statistics.\n\n\n———. 2016. “Derivatives of Low Rank PSD Approximation.” 2016.\n\n\n———. 2018. “MM Algorithms for Smoothed Absolute Values.” 2018.\n\n\nDe Leeuw, J., and W. J. Heiser. 1977. “Convergence of Correction Matrix Algorithms for Multidimensional Scaling.” In Geometric Representations of Relational Data, edited by J. C. Lingoes, 735–53. Ann Arbor, Michigan: Mathesis Press.\n\n\nDe Leeuw, J., and K. Sorenson. 2012. “Derivatives of the Procrustus Transformation with Applications.”\n\n\nGroenen, P. J. F., P. Giaquinto, and H. A. L Kiers. 2003. “Weighted Majorization Algorithms for Weighted Least Squares Decomposition Models.” Econometric Institute Report EI 2003-09. Econometric Institute, Erasmus University Rotterdam. https://repub.eur.nl/pub/1700.\n\n\nGroenen, P. J. F., W. J. Heiser, and J. J. Meulman. 1998. “City-Block Scaling: Smoothing Strategies for Avoiding Local Minima.” In Classification, Data Analysis, and Data Highways, edited by I. Balderjahn, R. Mathar, and M. Schader. Springer.\n\n\n———. 1999. “Global Optimization in Least-Squares Multidimensional Scaling by Distance Smoothing.” Journal of Classification 16: 225–54.\n\n\nGuttman, L. 1968. “A General Nonmetric Technique for Fitting the Smallest Coordinate Space for a Configuration of Points.” Psychometrika 33: 469–506.\n\n\nHeiser, W. J. 1991. “A Generalized Majorization Method for Least Squares Multidimensional Scaling of Pseudodistances that May Be Negative.” Psychometrika 56 (1): 7–27.\n\n\n———. 1995. “Convergent Computing by Iterative Majorization: Theory and Applications in Multidimensional Data Analysis.” In Recent Advantages in Descriptive Multivariate Analysis, edited by W. J. Krzanowski, 157–89. Oxford: Clarendon Press.\n\n\nKiers, H. A. L. 1997. “Weighted Least Squares Fitting Using Iterative Ordinary Least Squares Algorithms.” Psychometrika 62: 251–66.\n\n\nKruskal, J. B. 1964. “Nonmetric Multidimensional Scaling: a Numerical Method.” Psychometrika 29: 115–29.\n\n\nLange, K. 2016. MM Optimization Algorithms. SIAM.\n\n\nPliner, V. 1986. “The Problem of Multidimensional Metric Scaling.” Automation and Remote Control 47: 560–67.\n\n\n———. 1996. “Metric Unidimensional Scaling and Global Optimization.” Journal of Classification 13: 3–18.\n\n\nRamirez, C., R. Sanchez, V. Kreinovich, and M. Argaez. 2014. “\\(\\sqrt{x^2+\\mu}\\) is the Most Computationally Efficient Smooth Approximation to x.” Journal of Uncertain Systems 8: 205–10.\n\n\nSpang, H. A. 1962. “A Review of Minimization Techniques for Nonlinear Functions.” SIAM Review 4 (4): 343–65.\n\n\nTrosset, M. W., and R. Mathar. 1997. “On the Existence on Nonglobal Minimizers of the STRESS Criterion for Metric Multidimensional Scaling.” In Proceedings of the Statistical Computing Section, 158–62. Alexandria, VA: American Statistical Association.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Minimization of Basic Stress</span>"
    ]
  },
  {
    "objectID": "acceleration.html",
    "href": "acceleration.html",
    "title": "6  Acceleration of Convergence",
    "section": "",
    "text": "6.1 Simple Acceleration\nA simple and inexpensive way to accelerate smacof iterations was proposed by\nDe Leeuw and Heiser (1980).\nOn the other hand, if we choose \\(X=2\\Gamma(Y)-Y\\) then again \\(X\\not= Y\\), but \\(\\eta^2(X-\\Gamma(Y))=\\eta^2(Y-\\Gamma(Y))\\). Thus \\[\\begin{equation}\n\\sigma(X)\\leq 1+\\eta^2(X-\\Gamma(Y))-\\eta^2(\\Gamma(Y))=\n1+\\eta^2(Y-\\Gamma(Y))-\\eta^2(\\Gamma(Y))=\\sigma(Y).\n(\\#eq:upbmajupb)\n\\end{equation}\\] Let’s define the two update rules \\(\\text{up}_A(X):=\\Gamma(X)\\) and \\(\\text{up}_B(X)=2\\Gamma(X)-X\\).\nThis is illustrated in figure …. We want to locate a local minimum of \\(f\\), in red, in the interval \\((-4,2)\\). In this case we happen to know that \\(f\\) is a quartic polynomial, with minimum -0.8894476 at -1.8044048. In the interval we are looking at we have \\(f''(x)\\leq 8\\). Suppose our initial guess for the location of the minimum is \\(x=-3\\), the first vertical line from the left, with \\(f(-3)\\) equal to 1.51. The upper bound on the second derivative allows us to construct a quadratic majorizer \\(g\\), in blue, touching \\(f\\) at \\(-3\\). Update rule \\(\\text{up}_A\\) tells us to go to the minimum of \\(g\\), which is at -2.4275, the second vertical line. Here \\(g\\) is equal to 0.198975 and \\(f\\) is -0.3245082.\nRule \\(\\text{up}_B\\) “overrelaxes” and goes all the way to -1.855, the third vertical line from the left, where \\(g\\) is equal to both \\(g(-3)\\) and \\(f(-3)\\), and where \\(f\\) is -0.8862781, indeed much closer to the minimum. Examples such as this make \\(\\text{up}_B\\) look good.\nDe Leeuw and Heiser give a rather informal theoretical justification of \\(\\text{up}_B\\) as well. Suppose the sequence \\(X^+=\\Gamma(X)\\) generated by \\(\\text{up}_A\\) has slow linear convergence with ACR \\(1-\\epsilon\\), where \\(\\epsilon\\) is positive and small. Then choosing the \\(\\text{up}_B\\) will change the ACR of \\(1-\\epsilon\\) to \\(2(1-\\epsilon)-1=1-2\\epsilon\\approx(1-\\epsilon)^2\\), and will approximately halve the number of iterations to convergence. This argument is supported by numerical experiments which seem to show that indeed about half the number of iterations are needed. It seems that \\(\\text{up}_B\\) will get you something for almost nothing, and thus it has been implemented in various versions of the smacof programs as the default update. Unfortunately this may mean that many users have obtained, and presumably reported, MDS results that are incorrect.\nWhat is ignored in De Leeuw and Heiser (1980) is that majorization only guarantees that the sequence of loss function values converges for both update methods. The general convergence theory discussed earlier in this chapter shows that for both \\(\\text{up}_A\\) and \\(\\text{up}_B\\) the sequence \\(\\{X^{(k)}\\}\\) has at least one accumulation point, and that the accumulation points of the sequence \\(\\{X^{(k)}\\}\\) are fixed points of the update rule, which means for both \\(\\text{up}_A\\) and \\(\\text{up}_B\\) that at accumulation points \\(X\\) we have \\(X=\\Gamma(X)\\). But it does not say that \\(\\{X^{(k)}\\}\\) converges.\nThe argument also ignores that at any \\(X\\) the derivative of \\(\\text{up}_A\\) has a zero eigenvalue, with eigenvector \\(X\\). For \\(\\text{up}_B\\) the eigenvector \\(X\\) has eigenvalue equal to \\(-1\\), which is the largest one in modulus near any local minimum. And so …\nSuppose we have a configuration of the form \\(\\alpha X\\) with \\(X=\\Gamma(X)\\). Then \\(\\text{up}_B(\\alpha X)=2\\Gamma(\\alpha X)-\\alpha X=(2-\\alpha)X\\) and \\(\\text{up}_B((2-\\alpha)X)=\\alpha X\\). Thus starting with \\(X^{(1)}=\\alpha X\\) \\(\\text{up}_B\\) generates a sequence with even members \\((2-\\alpha)X\\) and odd members \\(\\alpha X\\). Thus there are two convergent subsequences with accumulation points \\(\\alpha X\\) and \\((2-\\alpha)X\\). And never the twain shall meet.\nAs far as stress is concerned, note that if \\(X=\\Gamma(X)\\) then \\(\\sigma(\\alpha X)=\\sigma((2-\\alpha)X)\\). Thus the stress values never change, and consequently form a convergent sequence.\nWe also see that \\(\\text{up}^{(2)}_B(\\alpha X):=\\text{up}_B(\\text{up}_B(\\alpha X))=\\alpha X\\), which means that \\(\\alpha X\\) is a fixed point of \\(\\text{up}_B^{(2)}\\) for any fixed point \\(X\\) of \\(\\text{up}_A\\) and any \\(\\alpha\\).\nAnother way to express the difference between the two update rule is that \\(\\text{up}_A\\) is self-scaling, i.e. \\(\\Gamma(\\alpha X)=\\Gamma(X)\\), while \\(\\text{up}_B\\) is not. Self-scaling implies \\(\\mathcal{D}\\Gamma(X)(X)=0\\), while for \\(\\text{up}_B\\) \\(\\mathcal{D}(2\\Gamma(X)-X)(X)=-X\\).\nLet’s now look at a real example. We use the Ekman color similarity data again, this time transformed by \\(\\delta_{ij}=(1-s_{ij})^3\\), The analysis is in two dimensions, with no weights. We run four analyses, by crossing update rules \\(\\text{up}_A\\) and \\(\\text{up}_B\\) with stopping criteria \\(\\sigma(X^{(k)})-\\sigma(X^{(k+1)})&lt;\\epsilon\\) and \\(\\max_{i,s}|x^{(k)}_{is}-x^{(k+1)}_{is}|&lt;\\epsilon\\). Let’s call these stopping criteria stop_s and stop_x. In all cases we allow a maximum of 1000 iterations and we set \\(\\epsilon\\) to 1e-10.\nThe results are in table … The first subtable gives the number of iterations, the second the final stress value. We see that generally stop_x requires more iterations than stop_s, because it is a stricter criterion. If we use stop_x then \\(\\text{up}_B\\) does not converge at all. Both with stop_s and stop_x \\(\\text{up}_B\\) gves a higher stress value than \\(\\text{up}_A\\). And yes, with stop_s (which is the default stop criterion in the smacof programs so far) \\(\\text{up}_B\\) use fewer iterations than \\(\\text{up}_A\\).\nTo verify that something is seriously wrong with running \\(\\text{up}_B\\), we compute the maximum absolute value of the gradient at convergence for both rules and stop_s. For \\(\\text{up}_A\\) it is 0.0000000010 and for \\(\\text{up}_B\\) it is 0.7834335001. Once again, with \\(\\text{up}_B\\) both loss function and configuration converge to an incorrect value.\nThis can also be illustrated graphically. We see from table … that \\(\\text{up}_B\\) with stop_x ends after 1000 iteration. We perform an extra iteration, number 1001, and see how the configuration changes. In figure … iteration 1000 is in black, iteration 1001 in red with slightly bigger characters. Except for a scaling factor the two configurations are the same. Elementwise dividing the \\(\\text{up}_B\\) by the \\(\\text{up}_A\\) final configuration gives a shrinkage factor \\(\\alpha\\) of 1.0595315. This shrinkage factor can also be computed from the final stress values. Using \\(\\rho(X)=\\eta^2(X)\\) and \\(\\sigma(X)=1-\\eta^2(X)\\) we find \\(\\sigma(\\alpha X)-\\sigma(X)=(\\alpha-1)\\eta^2(X)\\), and thus \\[\\begin{equation}\n\\alpha=1\\pm\\sqrt{\\frac{\\sigma(\\alpha X)-\\sigma(X)}{1-\\sigma(X))}}.\n(\\#eq:minshrink)\n\\end{equation}\\]\nThere are two values \\(\\alpha\\) and \\(2-\\alpha\\), equal to 0.9595728 and 1.0404272, because the sequence has two accumulation points.\nThings do not look good for \\(\\text{up}_B\\) but simple remedies are available. The first one is renormalization. After the iterations, with say stop_s, have converged, we scale the configuration such that \\(\\rho(X)=\\eta^2(X)\\) and recompute stress. This corrects both stress and the confguration to the correct outcome. Another way to normalize is to do another single \\(\\text{up}_A\\) step after convergence of \\(\\text{up}_B\\). This has the same effect. We tried \\(\\text{up}_B\\) with both renormalization approaches and both stop_s and stop_b. The number of \\(\\text{up}_B\\) iterations is still the same as in table … because we just compute something additional at the end. All stress values for the four combinations are now the correct 0.4696867. It seems that using \\(\\text{up}_B\\) with stop_s and renormalization at the end gives us the best of both worlds. It accelerates convergence and it gives the correct loss function values.\nOf course \\(\\text{up}_B\\) with stop_x still does not converge, and probably the best way to deal with that unfortunate fact is to avoid the combination alltogether.\nWe can still use stop_x and get acceleration by define a single interation as \\(\\text{up}_{AB}(x):=\\text{up}_A(\\text{up}_B(X))\\). For comparison purposes we also run \\(\\text{up}_{AA}(x):=\\text{up}_A(\\text{up}_A(X))\\). Both converge to the correct values, \\(\\text{up}_{AA}\\) in 17 and \\(\\text{up}_{AB}(x)\\) in 10 iterations.\nAgain \\(\\text{up}_{AB}\\) is an attractive strategy. It works with both stop_s and stop_x and it accelerates. Less so than \\(\\text{up}_B\\), however. If the ACR of \\(\\text{up}_A\\) is \\(1-\\epsilon\\), then, by the same reasoning as before, the ACR of \\(\\text{up}_{AB}\\) is \\((1-\\epsilon)^\\frac32\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Acceleration of Convergence</span>"
    ]
  },
  {
    "objectID": "acceleration.html#accelsimple",
    "href": "acceleration.html#accelsimple",
    "title": "6  Acceleration of Convergence",
    "section": "",
    "text": "stop_f\nstop_x\n\n\n\n\nrule A\n17\n32\n\n\nrule B\n15\n1000\n\n\n\n\n\n\n\n\n\nstop_f\nstop_x\n\n\n\n\nrule A\n0.4696867\n0.4696867\n\n\nrule B\n0.6176456\n0.6176456",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Acceleration of Convergence</span>"
    ]
  },
  {
    "objectID": "acceleration.html#one-parameter-methods",
    "href": "acceleration.html#one-parameter-methods",
    "title": "6  Acceleration of Convergence",
    "section": "6.2 One-Parameter Methods",
    "text": "6.2 One-Parameter Methods\nIn psychometrics, and perhaps in multivariate analysis, Ramsay (1975) was the first to apply a general acceleration methods to sequences in \\(\\mathbb{R}^n\\) of the form \\(x^{(k+1)}=f(x^{(k)})\\).\nDe Leeuw (2006)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Acceleration of Convergence</span>"
    ]
  },
  {
    "objectID": "acceleration.html#squarem",
    "href": "acceleration.html#squarem",
    "title": "6  Acceleration of Convergence",
    "section": "6.3 SQUAREM",
    "text": "6.3 SQUAREM",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Acceleration of Convergence</span>"
    ]
  },
  {
    "objectID": "acceleration.html#vector-extrapolation-methods",
    "href": "acceleration.html#vector-extrapolation-methods",
    "title": "6  Acceleration of Convergence",
    "section": "6.4 Vector Extrapolation Methods",
    "text": "6.4 Vector Extrapolation Methods\nDe Leeuw (2008a)\nDe Leeuw (2008b)\nSidi (2017)\n\n\n\n\nDe Leeuw, J. 2006. “Accelerated Least Squares Multidimensional Scaling.” Preprint Series 493. Los Angeles, CA: UCLA Department of Statistics. https://jansweb.netlify.app/publication/deleeuw-r-06-b/deleeuw-r-06-b.pdf.\n\n\n———. 2008a. “Accelerating Majorization Algorithms.” Preprint Series 543. Los Angeles, CA: UCLA Department of Statistics.\n\n\n———. 2008b. “Polynomial Extrapolation to Accelerate Fixed Point Algorithms.” Preprint Series 542. Los Angeles, CA: UCLA Department of Statistics.\n\n\nDe Leeuw, J., and W. J. Heiser. 1980. “Multidimensional Scaling with Restrictions on the Configuration.” In Multivariate Analysis, Volume V, edited by P. R. Krishnaiah, 501–22. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\nRamsay, J. O. 1975. “Solving Implicit Equations in Psychometric Data Analysis.” Psychometrika 40: 337–60.\n\n\nSidi, A. 2017. Vector Extrapolation Methods with Applicatons. SIAM.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Acceleration of Convergence</span>"
    ]
  },
  {
    "objectID": "nonmetric.html",
    "href": "nonmetric.html",
    "title": "7  Nonmetric MDS",
    "section": "",
    "text": "7.1 Generalities\nIn non-metric MDS the dissimilarities are not a vector of known non-negative numbers, but they are only known up to a transformation or quantification. Ever since Kruskal (1964a) the approach for dealing with this aspect of the MDS problem is to define stress as a function of both \\(X\\) and \\(\\Delta\\), and to minimize \\[\\begin{equation}\n\\sigma(X,\\Delta):=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\delta_{ij}-d_{ij}(X))^2\n(\\#eq:nmstress)\n\\end{equation}\\] over both configurations \\(X\\) and feasible disparities (i.e. transformed dissimilarities). The name disparities was coined, as far as I know, by Forrest Young and used in our joint ALS work from the seventies (Takane, Young, and De Leeuw (1977)). Kruskal’s name for the transformed or quantified dissimilarities is pseudo-distances.\nTo work with a general notion of the feasability of a matrix of disparities we use the notation \\(\\Delta\\in\\mathfrak{D}\\). Typically, although not necessarily, \\(\\mathfrak{D}\\) is a convex set in disparity space. In interval, polynomial, splinical, and ordinal MDS it usually is a convex cone with apex at the origin. This implies that \\(0\\in\\mathfrak{D}\\), and consequently that \\[\\begin{equation}\n\\min_{X\\in\\mathbb{R}^{n\\times p}}\\min_{\\Delta\\in\\mathfrak{D}}=0,\n(\\#eq:nmtrivial)\n\\end{equation}\\] with the minimum attained at \\(X=0\\) and \\(\\Delta=0\\). Of course this is a trivial solution, which is completely independent of the data. Thus we cannot formulate the NMDS problem as the minimization of stress from equation @ref(eq:nmstress) over unconstrained \\(X\\) and over \\(\\Delta\\) in its cone. We need some way to exclude either \\(X=0\\) or \\(\\Delta=0\\), or both, from the feasible solutions. This we can do either by normalization of the loss function, or by using constraints that explicitly exclude one or both zero solutions. The commonly used options will be discussed in section @ref(nmdsnorm) of this chapter.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonmetric MDS</span>"
    ]
  },
  {
    "objectID": "nonmetric.html#generalities",
    "href": "nonmetric.html#generalities",
    "title": "7  Nonmetric MDS",
    "section": "",
    "text": "7.1.1 Kruskal’s Stress\n… we shall find ourselves doing arithmetic with dissimilarities. This we must not do, because we are committed to using only the rank ordering of the dissimilarities. (Kruskal (1964a), p 6-7)\nsection @ref(nmdsnorm)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonmetric MDS</span>"
    ]
  },
  {
    "objectID": "nonmetric.html#nmssingledouble",
    "href": "nonmetric.html#nmssingledouble",
    "title": "7  Nonmetric MDS",
    "section": "7.2 Single and Double Phase",
    "text": "7.2 Single and Double Phase\nThe distinction between single phase and double phase NMDS algorithms, introduced by Guttman (1968), has caused a great deal of confusion in the early stages of non-metric MDS (say between 1960 and 1970).\n\nThis beguiling complex distinction has given rise to an almost endless debate (among &gt; Guttman, Kruskal, Lingoes, Roskam, and Shepard – for all permutations of five things taken two at a time) and has caused anguish and despair (accompanied by an imprecation or two by at least four of the five) extending over a three year period – only occasionally alleviated by evanescent flashes of partial insight (Lingoes and Roskam (1973))\n\nI was an active, although late-arriving, participant in discussing, and perhaps perpetuating, this confusion (De Leeuw (1973)). For raking up this debate at this late stage I was sternly spoken to by Jim Lingoes, who pointed me to the discussion in Lingoes and Roskam (1973).\n\nI would hate to believe that after this heroic attempt on our part that “we all” would once more be engaged in a “correspondence musical chairs” on these issues. (Lingoes in De Leeuw (1973)).\n\nNevertheless, even in later discussions of the distinction between single-phase and double-phase (such as (roskam_79?)) I still get the feeling that there are some unresolved misunderstandings. Thus I will pay some more attention to the musical chairs here.\nBy the very definition of the minimum of a function we have the mathematical truism \\[\\begin{equation}\n\\min_{(X,\\Delta)\\in\\mathfrak{X}\\otimes\\mathfrak{D}}\\sigma(X,\\Delta)=\n\\min_{X\\in\\mathfrak{X}}\\min_{\\Delta\\in\\mathfrak{D}}\\sigma(X,\\Delta)=\\min_{X\\in\\mathfrak{X}}\\left\\{\\min_{\\Delta\\in\\mathfrak{D}}\\sigma(X,\\Delta)\\right\\}=\\min_{\\Delta\\in\\mathfrak{D}}\\left\\{\\min_{X\\in\\mathfrak{X}}\\sigma(X,\\Delta)\\right\\},\n(\\#eq:nmsminmin)\n\\end{equation}\\] provided all minima exist. This is true no matter what the subsets \\(\\mathfrak{X}\\) of configuration space and \\(\\mathfrak{D}\\) of disparity space are.\n\n7.2.1 Double Phase\nIn a double phase algorithm we alternate the minimization of stress over \\(X\\) and \\(\\Delta\\). Thus \\[\\begin{align}\nX^{(k+1)}&=\\mathop{\\text{argmin}}_{X\\in\\mathfrak{X}}\\sigma(X,\\Delta^{(k)}),(\\#eq:nmsals1)\\\\\n\\Delta^{(k+1)}&=\\mathop{\\text{argmin}}_{\\Delta\\in\\mathfrak{D}}\\sigma(X^{(k+1)},\\Delta).(\\#eq:nmsals2).\n\\end{align}\\] Thus double phase algorithms are alternating least squares or ALS algorithms. The designation “alternating least squares” was first used, AFAIK, by De Leeuw (1968), and of course it was widely disseminated by the series of ALS algorithms of Young, Takane, and De Leeuw in the seventies (see Young (1981) for a retrospective summary).\nThere are some possible variations in the ALS scheme. In equation @ref(eq:nmsals1) we update \\(X\\) first, and then in equation @ref(eq:nmsals1) we update \\(\\Delta\\). That order can be reversed without any essential changes. More importantly, we have to realize that minimizing over \\(X\\) in equation @ref(eq:nmsals1) is a basic metric MDS problem, which will generally take an infinite number of iterations for an exact solution. This means we have to truncate the minimization, and stop at some point. And, in addition, equation @ref(eq:nmsals1) implies we have to find the global minimum over \\(X\\), which is generally infeasible as well.Thus the ALS scheme as defined cannot really be implemented.\nWe remedy this situations by switching from minimization in each substep to a decrease, or, notationwise, from \\(\\text{argmin}\\) to \\(\\text{arglower}\\). The resulting update sequence \\[\\begin{align}\nX^{(k+1)}&=\\mathop{\\text{arglower}}_{X\\in\\mathfrak{X}}\\sigma(X,\\Delta^{(k)}),(\\#eq:nmslte1)\\\\\n\\Delta^{(k+1)}&=\\mathop{\\text{arglower}}_{\\Delta\\in\\mathfrak{D}}\\sigma(X^{(k+1)},\\Delta).(\\#eq:nmslte2).\n\\end{align}\\] is much more loosely defined than the previous one, because arglower can be implemented in many different ways. More about that later. But at least the new scheme can actually be implemented.\nAlgorithm #ref(eq:nmslte1) and #ref(eq:nmslte2) is still considered to be ALS, but it is also firmly in the class of block relaxation algorithms. General block relaxation, which has alternating least squares, coordinate relaxation, augmentation, EM, and majorization as special cases, was used to describe many different data analysis algorithms in De Leeuw (1994). As with ALS, special cases of block relaxation have been around for a long time.\n\n\n7.2.2 Single Phase\nFrom equation @ref(eq:nmsminmin) \\[\\begin{equation}\n\\min_{X\\in\\mathfrak{X}}\\min_{\\Delta\\in\\mathfrak{D}}\\sigma(X,\\Delta)=\\min_{X\\in\\mathfrak{X}}\\left\\{\\min_{\\Delta\\in\\mathfrak{D}}\\sigma(X,\\Delta)\\right\\}.\n(\\#eq:nmsminsin)\n\\end{equation}\\] So if we define \\[\\begin{equation}\n\\sigma_\\star(X):=\\min_{\\Delta\\in\\mathfrak{D}}\\sigma(X,\\Delta),\n(\\#eq:nmssingle)\n\\end{equation}\\] the NMDS problem is to minimize \\(\\sigma_\\star\\) from @ref(eq:nmssingle) over \\(X\\). Note there is a \\(\\sigma\\) defined by equation @ref(eq:nmstress) on \\(\\mathfrak{X}\\otimes\\mathfrak{D}\\), and a \\(\\sigma_\\star\\), defined by equation @ref(eq:nmssingle), which is a function only of \\(X\\). It is sometimes said that that \\(\\Delta\\) is projected when going from @ref(eq:nmstress) to @ref(eq:nmssingle), or that \\(\\sigma_\\star\\) is a marginal function.\nOnce more with feeling. The two-phase \\(\\sigma\\) is a function of two matrix variables \\(X\\) and \\(\\Delta\\), the one-phase \\(\\sigma_\\star\\)is a function of the single matrix variable \\(X\\). To make this even more clear we can write \\(\\sigma_\\star(X)=\\sigma(X,\\Delta(X))\\), where \\[\\begin{equation}\n\\Delta(X):=\\mathop{\\text{argmin}}_{\\Delta\\in\\mathfrak{D}}\\sigma(X,\\Delta).\n(\\#eq:nmsdeltasingle)\n\\end{equation}\\]\nOf course by projecting out \\(X\\) instead of \\(\\Delta\\) we could also have defined a loss function which is a function of \\(\\Delta\\) only, but typically we do not use the alternative projection because it is complicated and heavily nonlinear. Projecting out \\(X\\) is, in fact, solving a standard basic MDS problem. Projecting out \\(\\Delta\\) is usually much simpler. In most applications \\(\\mathfrak{D}\\) is convex, so computing \\(\\Delta(X)\\) is computing the projection on a convex set, and projections on convex sets always exist and are unique and continuous.\nAs an aside, projection creates a function of one variable out of a function of two variables. The inverse of projection is called augmentation, which starts with a function \\(f\\) of one variable on \\(\\mathfrak{X}\\) and tries to find a function of two variables \\(g\\) on \\(\\mathfrak{X}\\otimes\\mathfrak{Y}\\) such that \\(f(x)=\\min_{y\\in\\mathfrak{Y}} g(x,y)\\). If we have found such a \\(g\\) then we can minimize \\(f\\) over \\(\\mathfrak{X}\\) by minimizing \\(g\\) over \\(\\mathfrak{X}\\otimes\\mathfrak{Y}\\), for example by block relaxation (De Leeuw (1994)).\nOne reason there was some confusion, and some disagreement between Kruskal and Guttman, was a result on differentiation of the minimum function, which was not known in the psychometric community at the time. Guttman thought that \\(\\sigma_\\star\\) was not differentiable at \\(X\\), because \\(\\Delta\\) from @ref(eq:nmsdeltasingle) is a step function. Kruskal proved in Kruskal (1971) that \\(\\sigma_\\star\\) is differentiable, and saw that the result is basically one in convex analysis, not in classical linear analysis. The result follows easily from directional differentiability in Danskin’s theorem (Danskin (1967)) or from the minimax theorems of, for example, Demyanov and Malozemov (1990), using the fact that the projection is unique. More directly, deleeuw_R_73g refers to discussion on page 255 of Rockafellar (1970), following his corollary 26.3.2. We will go into more detail about differentiability, and the differences between Kruskal’s and Guttman’s loss functions, in the next chapter @ref(chapordinal). For now it suffices to note that \\[\\begin{equation}\n\\mathcal{D}\\sigma_\\star(X)=\\mathcal{D}_1\\sigma(X,\\Delta(X)),\n(\\#eq:nmsdanskin)\n\\end{equation}\\] or, in words, that the derivative of \\(\\sigma_\\star\\) at \\(X\\) is the partial derivative of \\(\\sigma\\) at \\((X,\\Delta(X))\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonmetric MDS</span>"
    ]
  },
  {
    "objectID": "nonmetric.html#affine-nmds",
    "href": "nonmetric.html#affine-nmds",
    "title": "7  Nonmetric MDS",
    "section": "7.3 Affine NMDS",
    "text": "7.3 Affine NMDS\nBasic MDS can now be interpreted as the special case of NMDS in which \\(\\mathfrak{D}=\\{\\Delta\\}\\) is a singleton, a set with only one element. Thus \\(0\\not\\in\\mathfrak{D}\\) and we do not have to worry about trivial zero solutions for \\(X\\).\nThis extends to basic MDS with missing data. We have so far dealt with missing data by setting the corresponding \\(w_{ij}\\) equal to zero. But for the non-missing part we still have fixed numbers in \\(\\Delta\\), and thus again \\(0\\not\\in\\mathfrak{D}\\) (unless all dissimilarities are missing). In a sense missing data are our first example of non-metric MDS, because \\(\\mathfrak{D}\\) can also be defined as the set \\[\\begin{equation}\n\\mathfrak{D}=\n\\left\\{\\Delta\\mid\\Delta_0+\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}\\{\\alpha_{ij}(E_{ij}+E_{ji})\\mid \\delta_{ij}\\text{ is missing}\\}\\right\\},\n(\\#eq:nmsmissing)\n\\end{equation}\\] where the \\(E_{ij}\\) are the unit matrices defined in section @ref(#propmatrix) and \\(\\Delta_0\\) is the non-missing part (which has zeroes for the missing elements).\nSingle/double phase\nAnother example in which \\(0\\not\\in\\mathfrak{D}\\) is the additive constant problem, which we will discuss in detail in section @ref(intadditive). Here \\(\\mathfrak{D}\\) is the set of all hollow and symmetric matrices of the form \\(\\Delta+\\alpha(E-I)\\), where the dissimilarities in \\(\\Delta_0\\) are known real numbers and where \\(\\alpha\\) is the unknown additive constant.\nAffine MDS problems also have single phase and double phase algorithms. For missing data single phase stress is \\[\\begin{equation}\n\\sigma_\\star(X)=\\min_{\\Delta\\in\\mathfrak{D}}\\ \\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\delta_{ij}-d_{ij}(X))^2=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}\\tilde w_{ij}(\\delta_{ij}-d_{ij}(X))^2,\n(\\#eq:nmssinglemis)\n\\end{equation}\\] where \\(\\tilde w_{ij}=0\\) if \\(\\delta_{ij}\\) is missing, and \\(\\tilde w_{ij}=w_{ij}\\) otherwise. In this case \\(\\sigma_\\star(X)=\\sigma(X)\\), the sigma of basic MDS with zero weights for missing data.\nFor the additive constant problem single phase stress is \\[\\begin{equation}\n\\sigma_\\star(X)=\\min_{\\alpha}\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\delta_{ij}+\\alpha-d_{ij}(X))^2=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n} w_{ij}(\\delta_{ij}-d_{ij}(X))^2-(\\overline\\delta-\\overline d(X))^2\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij},\n(\\#eq:nmssingleadd)\n\\end{equation}\\] where \\(\\overline\\delta\\) and \\(\\overline d(X)\\) are the weighted means of the dissimilarities and distances.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonmetric MDS</span>"
    ]
  },
  {
    "objectID": "nonmetric.html#nmdsconic",
    "href": "nonmetric.html#nmdsconic",
    "title": "7  Nonmetric MDS",
    "section": "7.4 Conic NMDS",
    "text": "7.4 Conic NMDS\n\n7.4.1 Normalization\nIn “wide-sense” non-metric MDS \\(\\mathfrak{D}\\) can be any set of hollow, non-negative and symmetric matrices. In “narrow-sense” non-metric MDS \\(\\mathfrak{D}\\) is defined by homogeneous linear inequality constraints of the form \\(\\delta_{ij}\\leq\\delta_{kl}\\) (in addition to hollow, non-negative, and symmetric). These constraints, taken together, define a polyhedral convex cone in disparity space. This just means that if \\(\\Delta_1\\) and \\(\\Delta_2\\) are in \\(\\mathfrak{D}\\) then so is \\(\\alpha\\Delta_1+\\beta\\Delta_2\\) for all non-negative \\(\\alpha\\) and \\(\\beta\\).\nThe disparities define a cone, and thus \\(0\\in\\mathfrak{D}\\). This implies that always \\(\\min_X\\min_{\\Delta\\in\\mathfrak{D}}\\sigma(X,\\Delta)=0,\\) independently of the data. This is our first example of a trivial solution, which have plagued non-metric scaling from the start. Note that \\(\\mathfrak{D}\\) for missing data and for the additive constant are not convex cones, and do not contain the zero matrix.\nIn our versions of non-metric MDS we actually require that the transformed dissimilarities satisfy \\(\\eta_\\delta=1\\), so that formula @ref(eq:expand) is still valid. We call this explicit normalization of the dissimilarities.\nTo explain the different forms of normalization of stress that are needed whenever \\(\\mathfrak{D}\\) is a cone we look at some general properties of least squares loss functions. More details are in Kruskal and Carroll (1969) and in De Leeuw (1975), De Leeuw (2019).\nSuppose \\(K\\) and \\(L\\) are cones in \\(\\mathbb{R}^n\\), nor necessarily convex. Our problem is to minimize \\(\\|x-y\\|^2\\) over both \\(x\\in K\\) and \\(y\\in L\\). Here \\(\\|x\\|^2=x'Wx\\) for some positive definite \\(W\\). In the MDS context, for \\(x\\) think disparities, for \\(y\\) think distances.\nOf course minimizing \\(\\|x-y\\|^2\\) is too easy, because \\(x=y=0\\) is the (trivial, and useless) solution. So we need some form of normalization. We distinguish six different ones.\n\nimplicit x-normalization \\[\n\\min_{x\\in K}\\min_{y\\in L}\\frac{\\|x-y\\|^2}{\\|x\\|^2}\n\\]\nimplicit y-normalization \\[\n\\min_{x\\in K}\\min_{y\\in L}\\frac{\\|x-y\\|^2}{\\|y\\|^2}\n\\]\nimplicit xy-normalization \\[\n\\min_{x\\in K}\\min_{y\\in L}\\frac{\\|x-y\\|^2}{\\|x\\|^2\\|y\\|^2}\n\\]\nexplicit x-normalization \\[\n\\min_{x\\in K\\cap S}\\min_{y\\in L}\\|x-y\\|^2\n\\]\nexplicit y-normalization \\[\n\\min_{x\\in K}\\min_{y\\in L\\cap S}\\|x-y\\|^2\n\\]\nexplicit xy-normalization \\[\n\\min_{x\\in K\\cap S}\\min_{y\\in L\\cap S}\\|x-y\\|^2\n\\] If we use a positive definite \\(W\\) to define our inner products and norms, then implicit normalization of \\(x\\) means \\[\n\\min_{x\\in X}\\min_{y\\in Y}\\frac{(x-y)'W(x-y)}{x'Wx}.\n\\] Let \\(\\mathcal{S}_x\\) and \\(\\mathcal{S}_y\\) be the ellipsoids of all \\(x\\) with \\(x'Wx=1\\) and of all \\(y\\) with \\(y'Wy=1\\). Then our implicit normalization problem is equivalent to \\[\n\\min_{\\alpha\\geq 0}\\min_{\\beta\\geq 0}\\min_{x\\in X\\cap\\mathcal{S}_x}\\min_{y\\in Y\\cap\\mathcal{S}_y}\\frac{(\\alpha x-\\beta y)'W(\\alpha x-\\beta y)}{\\alpha^2 x'Wx}=\\\\\\min_{x\\in X\\cap\\mathcal{S}_x}\\min_{y\\in Y\\cap\\mathcal{S}_y}\\min_{\\alpha\\geq 0}\\min_{\\beta\\geq 0}\\frac{\\alpha^2+\\beta^2-2\\alpha\\beta x'Wy}{\\alpha^2}=\\\\\n\\min_{x\\in X\\cap\\mathcal{S}_x}\\min_{y\\in Y\\cap\\mathcal{S}_y}\\ \\{1-(x'Wy)^2\\}.\n\\] Thus implicit normalization of \\(x\\) means maximizing \\((x'Wy)^2\\) over \\(x\\in X\\cap\\mathcal{S}_x\\) and \\(y\\in Y\\cap\\mathcal{S}_y.\\)\n\nIn the same way implicit normalization of \\(y\\) minimizes \\[\n\\min_{x\\in X}\\min_{y\\in Y}\\frac{(x-y)'W(x-y)}{y'Wy},\n\\] and in the same way it also leads to maximization of \\((x'Wy)^2\\) over \\(x\\in X\\cap\\mathcal{S}_x\\) and \\(y\\in Y\\cap\\mathcal{S}_y.\\) In terms of normalized stress it does not matter if we use the distances or the dissimilarities in the denominator for implicit normalization.\nIn explicit normalization of \\(x\\) we solve \\[\n\\min_{x\\in X\\cap\\mathcal{S}_x}\\min_{y\\in Y}\\ \\{1+y'Wy-2y'Wx\\}=\\\\\n\\min_{\\beta\\geq 0}\\min_{x\\in X\\cap\\mathcal{S}_x}\\min_{y\\in Y\\cap\\mathcal{S}_y}\\{1+\\beta^2-2\\beta x'Wy\\}\n=\\\\\\min_{x\\in X\\cap\\mathcal{S}_x}\\min_{y\\in Y\\cap\\mathcal{S}_y}\\ \\{1-(x'Wy)^2\\},\n\\] and the same thing is true for explicit normalization of \\(y\\), which is \\[\n\\min_{x\\in X}\\min_{y\\in Y\\cap\\mathcal{S}_y}\\ \\{1+x'Wx-2y'Wx\\}\n\\] So, again, it does not matter which one of the four normalizations we use, explicit/implicit on disparities/distances, the solutions will all be proportional to each other, i.e. the same except for scale factors.\n\n\n7.4.2 Normalized Cone Regression\n\n\n7.4.3 Hard Squeeze and Soft Squeeze\n\n\n7.4.4 Inner Iterations\n\n\n7.4.5 Stress-1 and Stress-2\nIn his original papers Kruskal (1964a) and Kruskal (1964b) defined two versions of normalized stress for nonmetric MDS. The first was \\[\n\\sigma_{JBK1}(X):=\\sqrt{\\frac{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}(\\hat d_{ij}-d_{ij}(X))^2}{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}d_{ij}^2(X)}}\n\\] \\[\n\\sigma_{JBK2}(X):=\\sqrt{\\frac{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}(\\hat d_{ij}-d_{ij}(X))^2}{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}(d_{ij}(X)-\\overline{d}(X))^2}}\n\\] where the \\(hat d_{ij}\\) (the d-hats) are the pseudo-distances obtained by projecting the \\(d_{ij}(X)\\) on the isocone defined by the order of the dissimilarities, i.e. by monotone regression (see section @ref(mathsimpiso)). The \\(\\overline{d}(X)\\) in the denominator of \\(\\sigma_{JBK2}\\) is the average of the distances.\nThere are some differences with the definition of stress in this book.\n\nWe do not use the square root.\nWe use explicit and not implicit normalization.\nIn NMDS we think of stress as a function of both \\(X\\) and \\(\\Delta\\), not of \\(X\\) only (see section @ref(nmdskruskal)).\n\n\n\n\n\nDanskin, J. M. 1967. The Theory of Max-Min and Its Application to Weapons Allocation Problems. Springer.\n\n\nDe Leeuw, J. 1968. “Nonmetric Discriminant Analysis.” Research Note 06-68. Department of Data Theory, University of Leiden.\n\n\n———. 1973. “Smoothness Properties of Nonmetric Loss Functions.” Technical Memorandum. Murray Hill, N.J.: Bell Telephone Laboratories.\n\n\n———. 1975. “A Normalized Cone Regression Approach to Alternating Least Squares Algorithms.” Department of Data Theory FSW/RUL.\n\n\n———. 1994. “Block Relaxation Algorithms in Statistics.” In Information Systems and Data Analysis, edited by H. H. Bock, W. Lenski, and M. M. Richter, 308–24. Berlin: Springer Verlag. https://jansweb.netlify.app/publication/deleeuw-c-94-c/deleeuw-c-94-c.pdf.\n\n\n———. 2019. “Normalized Cone Regression.” 2019. https://jansweb.netlify.app/publication/deleeuw-e-19-d/deleeuw-e-19-d.pdf.\n\n\nDemyanov, V. F., and V. N. Malozemov. 1990. Introduction to Minimax. Dover.\n\n\nGuttman, L. 1968. “A General Nonmetric Technique for Fitting the Smallest Coordinate Space for a Configuration of Points.” Psychometrika 33: 469–506.\n\n\nKruskal, J. B. 1964a. “Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesis.” Psychometrika 29: 1–27.\n\n\n———. 1964b. “Nonmetric Multidimensional Scaling: a Numerical Method.” Psychometrika 29: 115–29.\n\n\n———. 1971. “Monotone Regression: Continuity and Differentiability Properties.” Psychometrika 36 (1): 57–62.\n\n\nKruskal, J. B., and J. D. Carroll. 1969. “Geometrical Models and Badness of Fit Functions.” In Multivariate Analysis, Volume II, edited by P. R. Krishnaiah, 639–71. North Holland Publishing Company.\n\n\nLingoes, J. C., and E. E. Roskam. 1973. “A Mathematical and Empirical Analysis of Two Multidimensional Scaling Algorithms.” Psychometrika 38: Monograph Supplement.\n\n\nRockafellar, R. T. 1970. Convex Analysis. Princeton University Press.\n\n\nTakane, Y., F. W. Young, and J. De Leeuw. 1977. “Nonmetric Individual Differences in Multidimensional Scaling: An Alternating Least Squares Method with Optimal Scaling Features.” Psychometrika 42: 7–67.\n\n\nYoung, F. W. 1981. “Quantitative Analysis of Qualitative Data.” Psychometrika 46: 357–88.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonmetric MDS</span>"
    ]
  },
  {
    "objectID": "interval.html",
    "href": "interval.html",
    "title": "8  Interval MDS",
    "section": "",
    "text": "8.1 The Additive Constant",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interval MDS</span>"
    ]
  },
  {
    "objectID": "interval.html#intadditive",
    "href": "interval.html#intadditive",
    "title": "8  Interval MDS",
    "section": "",
    "text": "8.1.1 Early\nIn the early history of MDS dissimilarities were computed from comparative judgments in the Thurstonian tradition.\ntriads paired comparisons etc positive orthant\nThese early techniques only gave numbers on an interval scale, i.e. dissimilarities known only up to a linear transformation. In order to get positive dissimilarities a rational origin needed to be found in some way. This is the additive constant problem. It can be seen as the first example of nonmetric MDS, in which we have only partially known dissimilarities (up to an additive constant).\n\\[\\begin{align}\n\\begin{split}\n(\\delta_{ij}+\\alpha)&\\approx d_{ij}(X),\\\\\n\\delta_{ij}&\\approx d_{ij}(X)+\\alpha.\n\\end{split}\n(\\#eq:twoadd)\n\\end{align}\\]\nThe additive constant techniques were more important in the fifties and sixties than they are these days, because they have largely been replaced by iterative nonmetric MDS techniques.\nAn early algorithm to fit the additive constant based on Schoenberg’s theorem was given by Messick and Abelson (1956). Ii was Torgerson based, i.e. it used the eigenvalues of \\(\\tau(\\Delta^{(2)})\\). It was a somewhat hopeful iterative technique, without a convergence proof, designed to make the sum of the \\(n-p\\) smallest eigenvalues equal to zero. This is of course only a necessary condition for best approximation, not a sufficient one.\nIn addition, the Messick-Abelson algorithm sometimes yielded solutionsin which the Torgerson transform of the squared dissimilarities had negative eigenvalues, which could even be quite large. That is also somewhat of a problem.\n\n\n8.1.2 Cooper\nConsequently Cooper (1972) proposed an alternative additive constant algorithm, taking his clue from the work of Kruskal.\nThe solution was to redefine stress as a function of both the configuration and the additive constant. Thus\n\\[\\begin{equation}\n\\sigma(X,\\alpha):=\\mathop{\\sum\\sum}_{1\\leq j&lt;i\\leq n}w_{ij}(\\delta_{ij}+\\alpha-d_{ij}(X))^2,\n(\\#eq:nmcooper1)\n\\end{equation}\\]\nand we minimize this stress over both \\(X\\) and \\(\\alpha\\).\nDouble phase (ALS)\n\\(\\delta_{ij}+\\alpha\\geq 0\\)\nSingle Phase (Cooper)\n\\[\\begin{equation}\n\\sigma(X):=\\min_\\alpha\\mathop{\\sum\\sum}_{1\\leq j&lt;i\\leq n}w_{ij}(\\delta_{ij}+\\alpha-d_{ij}(X))^2,\n(\\#eq:nmcooper2)\n\\end{equation}\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interval MDS</span>"
    ]
  },
  {
    "objectID": "interval.html#exactad",
    "href": "interval.html#exactad",
    "title": "8  Interval MDS",
    "section": "8.2 Algebra",
    "text": "8.2 Algebra\nThe additive constant problem is to find \\(X\\in\\mathbb{R}^{n\\times p}\\) and \\(\\alpha\\) such that \\(\\Delta+\\alpha(E-I)\\approx D(X)\\). In this section we look for all \\(\\alpha\\) such that \\(\\Delta+\\alpha(E-I)\\) is Euclidean, i.e. such that there is a configuration \\(X\\) with \\(\\Delta+\\alpha(E-I)=D(X)\\). This is a one-parameter generalization of Schoenberg’s theorem.\nIt makes sense to require \\(\\alpha\\geq 0\\), because a negative \\(\\alpha\\) would more appropriately be called a subtractive constant. Also, we may want to make sure that the off-diagonal elements of \\(\\Delta+\\alpha(E-I)\\) are non-negative, i.e. that \\(\\alpha\\geq-\\delta_{ij}\\) for all \\(i&gt;j\\). Note that if we allow a negative \\(\\alpha\\) then if all off-diagonal \\(\\delta_{ij}\\) are equal, say to \\(\\delta&gt;0\\), we have the trivial solution \\(\\alpha=-\\delta\\) and \\(X=0\\).\n\n8.2.1 Existence\nWe start with a simple construction.\n\nFor all $\\Delta$ there is an $\\alpha_0\\geq 0$ such that for all $\\alpha\\geq\\alpha_0$ we have $\\Delta+\\alpha(E-I))$ Euclidean of dimension $r\\leq n-1$.\n\n\nProof. We have, using \\(\\Delta\\times(E-I)=\\Delta\\) and \\((E-I)\\times(E-I)=E-I\\),\n\\[\\begin{equation}\n  \\tau((\\Delta+\\alpha(E-I))\\times(\\Delta+\\alpha(E-I)))=\n  \\tau(\\Delta\\times\\Delta)+2\\alpha\\tau(\\Delta)+\\frac12\\alpha^2J.\n(\\#eq:tau1)\n\\end{equation}\\]\nThus each off-diagonal element is a concave quadratic in \\(\\alpha\\), which is negative for \\(\\alpha\\) big enough. Choose \\(\\alpha_0\\geq 0\\) to make all off-diagonal elements negative (and all dissimilarities non-negative). A doubly-centered matrix with all off-diagonal elements negative is positive semi-definite of rank \\(n-1\\) (Taussky (1949)).\n\nNote that by the same argument we can also find a negative \\(\\alpha_0\\) that makes all off-diagonal elements negative and thus \\(\\Delta+\\alpha(E-I))\\) is again Euclidean of dimension \\(r\\leq n-1\\). But this \\(\\alpha_0\\) will usually result in negative dissimilarities.\nTheorem @ref(thm:nmn1) can be sharpened for non-Euclidean \\(\\Delta\\). Define the following function of \\(\\alpha\\):\n\\[\\begin{equation}\n\\lambda_\\star(\\alpha):=\\min_{x'x=1, x'e=0}x'\\{\\tau(\\Delta\\times\\Delta)+2\\alpha\\tau(\\Delta)+\\frac12\\alpha^2J\\}x.\n(\\#eq:lambdas)\n\\end{equation}\\]\nThis is the smallest non-trivial eigenvalue of the Torgerson transform in @ref(eq:tau1). The matrix \\(\\Delta+\\alpha(E-I)\\) is Euclidean if and only if \\(\\lambda_\\star(\\alpha)\\geq 0\\). Note that \\(\\lambda_\\star\\) is continuous, by a simple special case of the Maximum Theorem (Berge (1963), Chapter VI, section 3), and coercive, i.e. \\(\\lambda_\\star(\\alpha)\\rightarrow +\\infty\\) if \\(|\\alpha|\\rightarrow +\\infty\\).\n\nFor all non-Euclidean $\\Delta$ there is an $\\alpha_1&gt;0$ such that for all $\\alpha\\geq\\alpha_1$ we have that $\\Delta+\\alpha(E-I))$ Euclidean of dimension $r\\leq n-2$.\n\n\nProof. Because \\(\\Delta\\) is non-Euclidean we have \\(\\lambda_\\star(0)&lt;0\\). By the construction in theorem @ref(thm:nmn1) there is an \\(\\alpha_0\\) such that \\(\\lambda_\\star(\\alpha)&gt;0\\) for all \\(\\alpha&gt;\\alpha_0\\). By the Maximum Theorem the function \\(\\lambda_\\star\\) is continuous, and thus, by Bolzano’s theorem, there is an \\(\\alpha_1\\) between \\(0\\) and \\(\\alpha_0\\) such that \\(\\lambda_\\star(\\alpha_1)=0\\). If there is more than one zero between \\(0\\) and \\(\\alpha_0\\) we take the largest one as \\(\\alpha_1\\).\n\nThe problem with extending theorem @ref(thm:nmn2) to Euclidean \\(\\Delta\\) is that the equation \\(\\lambda_\\star(\\alpha)=0\\) may have only negative roots, or, even more seriously, no roots at all. This may not be too important from the practical point of view, because observed dissimilarities will usually not be exactly Euclidean. Nevertheless I feel compelled to address it.\n\nIf $\\Delta$ is Euclidean then $\\lambda_\\star(\\alpha)$ is non-negative and non-decreasing on $[0,+\\infty)$.\n\n\nProof. If \\(\\Delta\\) is Euclidean, then \\(\\sqrt{\\Delta}\\), which is short for the matrix with the square roots of the dissimilarities, is Euclidean as well. This follows because the square root is a Schoenberg transform (Schoenberg (1937), Bavaud (2011)), and it implies that \\(\\tau(\\Delta)=\\tau(\\sqrt{\\Delta}\\times\\sqrt{\\Delta})\\) is positive semi-definite. Thus the matrix @ref(eq:tau1) is positive semi-definite for all \\(\\alpha\\geq 0\\). By Danskin’s Theorem the one-sided directional derivative of \\(\\lambda_\\star\\) at \\(\\alpha\\) is \\(2x(\\alpha)'\\tau(\\Delta)x(\\alpha)+\\alpha\\), where \\(x(\\alpha)\\) is one of the minimizing eigenvectors. Because the one-sided derivative is non-negative, the function is non-decreasing (in fact increasing if \\(\\alpha&gt;0\\)).\n\nOf course \\(\\lambda_\\star(\\alpha)=0\\) can still have negative solutions, and in particular it will have at least one negative solution if \\(\\lambda_\\star(\\alpha)\\leq 0\\) for any \\(\\alpha\\). There can even be negative solutions with \\(\\Delta+\\alpha(E-I)\\) non-negative.\n\n\n8.2.2 Solution\nThe solutions of \\(\\lambda_\\star(\\alpha)=0\\) can be computed and studied in more detail, using results first presented in the psychometric literature by Cailliez (1983). We reproduce his analysis here, with a somewhat different discussion that relies more on existing mathematical results.\nIn order to find the smallest \\(\\alpha\\) we solve the quadratic eigenvalue problem (Tisseur and Meerbergen (2001)). WHY ??\n\\[\\begin{equation}\n\\{\\tau(\\Delta\\times\\Delta)+2\\alpha\\tau(\\Delta)+\\frac12\\alpha^2J\\}y=0.\n(\\#eq:qep1)\n\\end{equation}\\]\nA solution \\((y,\\alpha)\\) of #ref(eq:qep1) is an eigen pair, in which \\(y\\) is an eigenvector, and \\(\\alpha\\) the corresponding eigenvalue. The trivial solution \\(y=e\\) satisfies #ref(eq:qep1) for any \\(\\alpha\\). We are not really interested in the non-trivial eigenvectors here, but we will look at the relationship between the eigenvalues and the solutions of \\(\\lambda_\\star(\\alpha)=0\\).\nThe eigenvalues can be complex, in which case they do not interest us. If \\(\\alpha\\) is a non-trivial real eigenvalue, then the rank of the Torgerson transform of the matrix in #ref(eq:qep1) is \\(n-2\\), but\nTo get rid of the annoying trivial solution \\(y=e\\) we use a square orthonormal matrix whose first column is proportional to \\(e\\). Suppose \\(L\\) contains the remaining \\(n-1\\) columns. Now solve\n\\[\\begin{equation}\n\\{L'\\tau(\\Delta\\times\\Delta)L+2\\alpha L'\\tau(\\Delta)L+\\frac12\\alpha^2I\\}y=0.\n(\\#eq:qep2)\n\\end{equation}\\]\nNote that the determinant of the polynomial matrix in @ref(eq:qep2) is a polynomial of degree \\(2(n-1)\\) in \\(\\alpha\\), which has \\(2(n-1)\\) real or complex roots.\nThe next step is linearization (Gohberg, Lancaster, and Rodman (2009), chapter 1), which means finding a linear or generalized linear eigen problem with the same roots as @ref(eq:qep2). In our case this is the eigenvalue problem for the matrix\n\\[\\begin{equation}\n\\begin{bmatrix}\n\\hfill 0&\\hfill I\\\\\n-2L'\\tau(\\Delta\\times\\Delta)L&-4L'\\tau(\\Delta)L\n\\end{bmatrix}\n(\\#eq:qep3)\n\\end{equation}\\]\n\n\n8.2.3 Examples\n\n8.2.3.1 Small Example\nHere is a small artificial dissimilarity matrix.\n\n\n  1  2  3  4 \n1 +0 +1 +2 +5\n2 +1 +0 +4 +2\n3 +2 +4 +0 +1\n4 +5 +2 +1 +0\n\n\nIt is constructed such that \\(\\delta_{14}&gt;\\delta_{12}+\\delta_{24}\\) and that \\(\\delta_{23}&gt;\\delta_{21}+\\delta_{13}\\). Because the triangle inequality is violated the dissimilarities are not distances in any metric space, and certainly not in a Euclidean one. Because the minimum dissimilarity is \\(+1\\), we require that the additive constant \\(\\alpha\\) is at least \\(-1\\).\nThe R function treq() in appendix @ref(apcodeclass) finds the smallest additive constant such that all triangle inequalities are satisfied. For this example it is \\(\\alpha=2\\).\nThe Torgerson transform of \\(\\Delta\\times\\Delta\\) is\n\n\n  1      2      3      4     \n1 +4.312 +2.688 +1.188 -8.188\n2 +2.688 +2.062 -5.938 +1.188\n3 +1.188 -5.938 +2.062 +2.688\n4 -8.188 +1.188 +2.688 +4.312\n\n\nwith eigenvalues\n\n\n[1] +12.954 +7.546  +0.000  -7.750 \n\n\nThe smallest eigenvalue -7.75 is appropriately negative, and theorem @ref(thm:nmn2) shows that \\(\\Delta\\times\\Delta+7.75(E-I)\\) are squared distances between four points in the plane.\nThe upper bound for the smallest \\(\\alpha\\) from theorem @ref(thm:nmn1), computed by the R function acbound(), is 9.309475.\nIt is useful to look at a graphical representation of the minimum non-trivial eigenvalue of \\(\\tau((\\Delta+\\alpha(E-I))\\times(\\Delta+\\alpha(E-I)))\\) as a function of \\(\\alpha\\). The R function aceval() generates the data for the plot.\n\n\n\n\n\n\n\n\n\nWe see that the minimum non-trivial eigenvalue is a continuous function of \\(\\alpha\\),but one which certainly is not convex or concave or differentiable. The graph crosses the horizontal axes near -8, -3, and +6.\nTo make this precise we apply the theory of section xxx. The R function acqep() finds the six non-trivial eigenvalues\n\n\n[1] -8.192582+0.000000i  5.713075+0.000000i -3.500000+2.179449i\n[4] -3.500000-2.179449i -2.807418+0.000000i -2.713075+0.000000i\n\n\nTwo of the eigenvalues are complex conjugates, four are real. Of the real eigenvalues three are negative, and only one is positive, equal to +5.713. The table above gives the eigenvalues of the Torgerson transform, using all four real eigenvalues for \\(\\alpha\\). The three negative ones do result in a positive semi-definite matrix with rank equal to \\(n-2\\), but they also create negative dissimilarities.\n\n\n -8.193  ******  +38.098 +13.885  +0.000  +0.000 \n +5.713  ******  +61.116 +43.441  +0.000  -0.000 \n -2.807  ******   +3.115  +0.402  +0.000  -0.000 \n -2.713  ******   +3.228  +0.215  +0.000  -0.000 \n\n\n\n\n8.2.3.2 De Gruijter Example\n\n\n\n\n\n\n\n\n\n [1] -20.527411+0.0000000i -10.174103+0.0000000i  -9.472504+0.0000000i\n [4]  -6.622263+0.3526193i  -6.622263-0.3526193i  -5.885691+0.2875441i\n [7]  -5.885691-0.2875441i  -5.640580+0.3668888i  -5.640580-0.3668888i\n[10]  -4.391289+0.2532477i  -4.391289-0.2532477i  -3.708911+0.3868444i\n[13]  -3.708911-0.3868444i  -3.238930+0.0000000i  -2.311379+0.0000000i\n[16]  -1.369315+0.0000000i\n\n\n\n\n8.2.3.3 Ekman Example\n\n\n\n\n\n\n\n\n\n [1] -5.713009655+0.00000000i -3.782729083+0.00000000i -1.791313475+0.00000000i\n [4] -1.628964140+0.00000000i -0.976213035+0.00000000i -0.744289350+0.04959388i\n [7] -0.744289350-0.04959388i -0.682321433+0.00000000i -0.534849034+0.00000000i\n[10] -0.513033529+0.00000000i -0.497908376+0.02481447i -0.497908376-0.02481447i\n[13] -0.372321687+0.13138923i -0.372321687-0.13138923i -0.388308013+0.00000000i\n[16] -0.229813135+0.18259852i -0.229813135-0.18259852i -0.286712033+0.00000000i\n[19] -0.212601059+0.11851989i -0.212601059-0.11851989i  0.206312577+0.00000000i\n[22] -0.194299448+0.00000000i  0.132767430+0.00000000i -0.079646956+0.00000000i\n[25] -0.024193535+0.00000000i -0.006762279+0.00000000i\n\n\n\n\n\n\n\n\n\n\n\n [1] -7.974065161+0.00000000i -4.867929358+0.00000000i -1.224234244+0.00000000i\n [4] -0.982237601+0.00000000i  0.785644808+0.00000000i  0.648677529+0.00000000i\n [7] -0.554033177+0.00000000i -0.542600618+0.01297027i -0.542600618-0.01297027i\n[10]  0.486418243+0.00000000i -0.111892110+0.39235860i -0.111892110-0.39235860i\n[13]  0.382974612+0.00000000i  0.353089664+0.00000000i -0.351610318+0.00000000i\n[16] -0.307360690+0.00000000i -0.126594060+0.26307892i -0.126594060-0.26307892i\n[19] -0.073544792+0.26986305i -0.073544792-0.26986305i -0.233302697+0.00000000i\n[22] -0.008137562+0.19480062i -0.008137562-0.19480062i -0.138025415+0.11750706i\n[25] -0.138025415-0.11750706i  0.120502647+0.00000000i\n\n\n\n\n\n8.2.4 A Variation\nAlternatively, we could define our approximation problem as finding \\(X\\in\\mathbb{R}^{n\\times p}\\) and \\(\\alpha\\) such that \\(\\sqrt{\\delta_{ij}^2+\\alpha}\\approx d_{ij}(X)\\), or, equivalently, \\(\\Delta\\times\\Delta+\\alpha(E-I)\\approx D(X)\\times D(X)\\).\n\nFor any $X\\in\\mathbb{R}^{n\\times p}$ with $p=n-2$ there is an $\\alpha$\nsuch that $\\sqrt{\\delta_{ij}^2+\\alpha}= d_{ij}(X)$.\n\n\nProof. Now we have\n\\[\\begin{equation}\n\\tau(\\Delta\\times\\Delta+\\alpha(E-I)))=\n  \\tau(\\Delta\\times\\Delta)+\\frac12\\alpha J.\n(\\#eq:tau2)\n\\end{equation}\\]\nThe eigenvalues of \\(\\tau(\\Delta\\times\\Delta)+\\frac12\\alpha J\\) are zero and \\(\\lambda_s+\\frac12\\alpha\\), where the \\(\\lambda_s\\) are the \\(n-1\\) non-trivial eigenvalues of \\(\\tau(\\Delta\\times\\Delta)\\). If \\(\\underline{\\lambda}\\) is smallest eigenvalue we choose \\(\\alpha=-2\\underline{\\lambda}\\), and \\(\\tau(\\Delta\\times\\Delta)+\\frac12\\alpha J\\) is positive semi-definite of rank \\(r\\leq n-2\\).\n\nNote that theorem @ref(thm:nmn2) implies that for any \\(\\Delta\\) there is a strictly increasing differentiable transformation to the space of Euclidean distance matrices in \\(n-2\\) dimensions. This is a version of what is sometimes described as Guttman’s n-2 theorem (Lingoes (1971)). The proof we have given is that from De Leeuw (1970), Appendix B.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interval MDS</span>"
    ]
  },
  {
    "objectID": "interval.html#interval-smacof",
    "href": "interval.html#interval-smacof",
    "title": "8  Interval MDS",
    "section": "8.3 Interval smacof",
    "text": "8.3 Interval smacof\nIn this section we introduce a double-phase alternating least squares algorithm that fits better into the smacof framework than the single-phase method proposed by Cooper (1972). We also restrict our linear transformations to be to be increasing and non-negative on the positive real axes.\nTo avoid various kinds of trivialities, assume not all \\(d_{ij}(X)\\) are zero.\nIn the optimal scaling phase we must minimize\n\\[\\begin{equation}\n\\sigma(X,\\alpha,\\beta)=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq  n}w_{ij}(\\alpha\\delta_{ij}+\\beta-d_{ij}(X))^2\n(\\#eq:intloss)\n\\end{equation}\\]\nThe constraints are \\(\\alpha\\delta_{ij}+\\beta\\geq 0\\) and \\(\\alpha\\delta_{ij}+\\beta\\geq\\alpha\\delta_{kl}+\\beta\\) if \\(\\delta_{ij}\\geq\\delta_{kl}\\). These define pointed convex cone in the space of disparities. We need to project \\(D(X)\\) on that cone, in the metric defined by \\(W\\). But it is easy to see that and equivalent set of constraints in \\(\\mathbb{R}^2\\) is \\(\\alpha\\geq 0\\) and \\(\\alpha\\delta_\\text{min}+\\beta\\geq 0\\). Again these two constraints define a pointed cone in two-dimensional \\((\\alpha,\\beta)\\) space, where proje ction is much easier to handle thanin the generally much larger disparity space. Of course the projection metric in \\((\\alpha,\\beta)\\) is different from the one in disparity space.\nIn addition to the inequality constraints we have the normalization constraint \\[\\begin{equation}\n\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq  n}w_{ij}(\\alpha\\delta_{ij}+\\beta)^2=1,\n(\\#eq:intnorm)\n\\end{equation}\\] but as we have seen in chapter @ref(nonmtrmds) we can initially ignore that constraint, project on the cone, and then normalize the projection.\nIn order to simplify the notation we collect the \\(d_{ij}(X)\\) in a vector \\(d\\), the \\(\\delta_{ij}\\) in a vector \\(\\delta\\) and the \\(w_{ij}\\) in a diagonal matrix \\(W\\).\nLet’s first get the trivial case where all \\(\\delta_{ij}\\) are equal out of the way. In that case the linear regression is singular, and we simply choose all \\(\\alpha\\delta+\\beta\\) equal to the constant \\(e'Wd\\), for example by setting \\(\\alpha=0\\) and \\(\\beta=e'Wd\\). Applying the normalization condition @ref(eq:intnorm) then sets \\(\\beta=1\\). From now on we assume in this section that not all \\(\\delta_{ij}\\) are equal.\nProjecting on the cone gives us four possibilities. We can have \\(\\alpha=0\\) or \\(\\alpha\\delta_\\text{min}+\\beta=0\\), or both, or neither. We first analyze the case in which the unconstrained minimum of @ref(eq:intloss) is in the cone, which will be the most common case, especially in later smacof iterations. Using the fact that \\(\\delta'W\\delta=e'We=1\\) we find that \\[\\begin{equation}\n\\begin{bmatrix}\\tilde\\alpha\\\\\\tilde\\beta\\end{bmatrix}=\n\\frac{1}{1-(e'W\\delta)^2}\\begin{bmatrix}\\delta'(W-We e'W)d\\\\e'(W-W\\delta\\delta'W)d\\end{bmatrix}.\n(\\#eq:intunc)\n\\end{equation}\\] If \\(\\tilde\\alpha\\geq 0\\) and \\(\\tilde\\beta\\geq-\\alpha\\delta_\\text{min}\\) we are done. If not, we know the projection is on the line \\(\\alpha=0\\) or on the line \\(\\tilde\\beta=-\\alpha\\delta_\\text{min}\\), or on their intersection, which is the origin.\nFirst suppose the projection is on \\(\\alpha=0\\). We find the minimizing \\(\\beta\\) equal to \\(\\overline{\\beta}:=e'Wd\\), which strictly satisfies the second constraint because \\(\\overline\\beta&gt;-\\alpha\\delta_\\text{min}=0\\), and thus \\((0,e'Wd)\\) is on the boundary of the cone. This also show that the origin, which has \\(\\sigma(X,0,0)=d'Wd\\), can never be the projection. The minimum at \\((0,e'Wd)\\) is \\[\\begin{equation}\n\\sigma(X,0,e'Wd)=d'Wd-(d'We)^2\n(\\#eq:intvertexloss1)\n\\end{equation}\\] Or, alternatively, we can assume that the projection is on the vertex \\(\\beta=-\\alpha\\delta_\\text{min}\\), in which case the minimizing \\(\\alpha\\) is \\[\\begin{equation}\n\\overline{\\alpha}:=\\frac{(\\delta-\\delta_\\text{min})'Wd}{(\\delta-\\delta_\\text{min})'W(\\delta-\\delta_\\text{min})},\n(\\#eq:intalpha)\n\\end{equation}\\] which is always positive, and thus \\((\\overline{\\alpha},-\\overline{\\alpha}\\delta_\\text{min})\\) is on the boundary of the cone. The minimum is \\[\\begin{equation}\n\\sigma(X,\\overline{\\alpha},-\\overline{\\alpha}\\delta_\\text{min})=d'Wd-\\frac{((\\delta-\\delta_\\text{min})'Wd)^2}{(\\delta-\\delta_\\text{min})'W(\\delta-\\delta_\\text{min})}\n(\\#eq:intvertexloss2)\n\\end{equation}\\] If the unconstrained solution is not in the cone, then we choose the projection as the solution corresponding with the smallest of @ref(eq:intvertexloss1) and (@ref(eq:intvertexloss2).\n\n8.3.1 Example\nWe illustrate finding the optimal linear transformation with a small example. We choose some arbitrary \\(w\\), \\(\\delta\\), and \\(d\\) and normalize them in the usual way.\n\nw &lt;- c(rep(1,5),rep(2,5))\nw &lt;- w / sum(w)\ndelta &lt;- 1:10\ns &lt;- sum(w * delta ^ 2)\ndelta &lt;- delta / sqrt (s)\nd &lt;- c(1, 2, 3, 4, 4, 3, 3, 3, 1, 1)\ns &lt;- sum (w * d ^ 2)\nt &lt;- sum (w * d * delta)\nd &lt;- d * (t / s)\n\nAfter normalization the \\(\\delta_{\\text{min}}\\) is 0.1448414. The pink region in figure @ref(fig:intconex) is the cone formed by the intersection of the half-spaces \\(\\alpha\\geq 0\\) and \\(\\alpha\\delta_{\\text{min}}+\\beta\\geq 0\\).\n\n\n\n\n\nCone Projection\n\n\n\n\nThe unconstrained minimum is attained at -0.2541855, 0.9484652, the red point in figure @ref(fig:intconex), with stress equal to 0.0939827. That is clearly outside the cone, so we now consider projection on the two one-dimensional boundary rays. The blue point is 0, 0.7152935, the projection on \\(\\alpha\\geq 0\\). It is fairly close to the unconstrained minimum, with stress 0.1042239. The green point 0.6782764, -0.0982425is the projection on \\(\\beta=-\\alpha\\delta_{\\text{min}}\\), which has stress 0.2684117. Thus the blue point 0, 0.7152935is the actual projection on the cone in \\((\\alpha,\\beta)\\) space, and the best fitting line has slope zero (which, in smacof, would make all disparities equal for the next iteration).\nThis is illustrated in a different way (with Shepard plots) in figure @ref(fig:intlineex), where we see the red, blue, and green lines corresponding with the red, blue, and green points in figure @ref(fig:intconex). Note that the green line goes through the point \\((\\delta_{\\text{min}},0)\\). The horizontal blue line is the best fitting one under the constraints.\n\n\n\n\n\nFitted Lines\n\n\n\n\n\n\n\n\nBavaud, F. 2011. “On the Schoenberg Transformations in Data Analysis: Theory and Illustrations.” Journal of Classification 28: 297–314.\n\n\nBerge, C. 1963. Topological Spaces. Oliver & Boyd.\n\n\nCailliez, F. 1983. “The Analytical Solution to the Additive Constant Problem.” Psychometrika 48 (2): 305–8.\n\n\nCooper, L. G. 1972. “A New Solution to the Additive Constant Problem in Metric Multidimensional Scaling.” Psychometrika 37 (3): 311–22.\n\n\nDe Leeuw, J. 1970. “The Euclidean Distance Model.” Research Note 002-70. Department of Data Theory FSW/RUL.\n\n\nGohberg, I., P. Lancaster, and L. Rodman. 2009. Matrix Polynomials. Classic in Applied Mathematics. SIAM.\n\n\nLingoes, J. C. 1971. “Some Boundary Conditions for a Monotone Analysis of Symmetric Matrices.” Psychometrika 36 (2): 195–203.\n\n\nMessick, S. J., and R. P. Abelson. 1956. “The Additive Constant Problem in Multidimensional Scaling.” Psychometrika 21 (1–17).\n\n\nSchoenberg, I. J. 1937. “On Certain Metric Spaces Arising From Euclidean Spaces by a Change of Metric and Their Imbedding in Hilbert Space.” Annals of Mathematics 38 (4): 787–93.\n\n\nTaussky, O. 1949. “A Recurring Theorem on Determinants.” American Mathematical Monthly 56: 672–76.\n\n\nTisseur, F., and K. Meerbergen. 2001. “The Quadratic Eigenvalue Problem.” SIAM Review 43 (2): 235–86.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interval MDS</span>"
    ]
  },
  {
    "objectID": "polynomial.html",
    "href": "polynomial.html",
    "title": "9  Polynomial MDS",
    "section": "",
    "text": "9.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Polynomial MDS</span>"
    ]
  },
  {
    "objectID": "polynomial.html#fitting-polynomials",
    "href": "polynomial.html#fitting-polynomials",
    "title": "9  Polynomial MDS",
    "section": "9.2 Fitting Polynomials",
    "text": "9.2 Fitting Polynomials\n\\[\n\\sigma(X)=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(P_r(\\delta_{ij})-d_{ij}(X))^2\n\\]\n\\[\nP_r(\\delta_{ij}):=\\sum_{s=0}^r\\alpha_s^{\\ }\\delta_{ij}^s.\n\\]\nThe polynomial \\(P_r\\) is tied down if \\(\\alpha_0=0\\), and thus \\(P_r(0)=0\\).\nVandermonde matrix",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Polynomial MDS</span>"
    ]
  },
  {
    "objectID": "polynomial.html#positive-and-convex-monotone-polynomials",
    "href": "polynomial.html#positive-and-convex-monotone-polynomials",
    "title": "9  Polynomial MDS",
    "section": "9.3 Positive and Convex, Monotone Polynomials",
    "text": "9.3 Positive and Convex, Monotone Polynomials\n\n9.3.1 Introduction\nConstraints on values, constraints on coefficients\n\nf &lt;- function(x) return(x * (x - 1) * (x - 2))\nx &lt;- seq(0, 3, length = 100)\ny &lt;- f(x)\nplot(x, y, type=\"l\", lwd = 3, col = \"RED\")\nx &lt;- c(.10, .45, 2.25, 2.75)\ny &lt;- f(x)\nabline(h = y[1])\nabline(h = y[2])\nabline(h = y[3])\nabline(h = y[4])\nabline(v = x[1])\nabline(v = x[2])\nabline(v = x[3])\nabline(v = x[4])\n\n\n\n\n\n\n\n\nTied-down increasing non-negative cubic.\nWrite the tied-down cubic in the form \\(f(x)=ax(x^2+2bx+c)\\). Since we must have \\(f(x)\\rightarrow+\\infty\\) of \\(x\\rightarrow+\\infty\\) we have \\(a&gt;0\\). Since \\(f\\) must be increasing at zero, we must have \\(f'(0)=c&gt;0\\). No real roots on the positive reals. Case 1: no real roots at all \\(b^2-c&lt;0\\). Case 2: two real roots, both negative \\(b^2-c\\geq 0\\) and \\(b\\geq 0\\). Since \\(c&gt;0\\) the product of roots is positive. If the sum is negative, i.e. if \\(b&gt;0\\) both roots are negative. Since \\(f''(x)=6x+b\\) we see that \\(f\\) is convex on the positive real axis if \\(b\\geq 0\\).\n\n\n9.3.2 A QP Algorithm\nIn this section we construct an algorithm for a general weighted linear least squares projection problem with equality and/or inequality constraints. It uses duality and unweighting majorization. The section takes the form of a small diversion, with examples. This may seem somewhat excessive, but it provides an easy reference for both you and me and it serves as a manual for the corresponding R code.\nWe start with the primal problem, say problem \\(\\mathcal{P}\\), which is minimizing\n\\[\\begin{equation}\nf(x)=\\frac12(Hx-z)'V(Hx-z)\n(\\#eq:qpbase)\n\\end{equation}\\]\nover all \\(x\\) satisfying equalities \\(Ax\\geq b\\) and equations \\(Cx=d\\). We suppose the Slater condition is satisfied, i.e. there is an \\(x\\) such that \\(Ax&gt;b\\). And, in addition, we suppose the system of inequalities and equations is consistent, i.e. has at least one solution.\nWe first reduce the primal problem to a simpler, and usually smaller, one by partitioning the loss function. Define\n\\[\\begin{align}\n\\begin{split}\nW&:=H'VH,\\\\\ny&:=W^{-1}H'Vz,\\\\\nQ&:=(I-H(H'VH)^{-1}H'V).\\\\\n\\end{split}\n(\\#eq:qpfirstpart)\n\\end{align}\\]\nThen\n\\[\\begin{equation}\n(Hx-y)'V(Hx-y)=(x-y)'W(x-y)+y'Q'VQy,\n(\\#eq:qpsimple)\n\\end{equation}\\]\nThe simplified primal problem \\(\\mathcal{P}'\\) is to minimize \\((x-y)'W(x-y)\\) over \\(Ax\\geq b\\) and \\(Cx=d\\), where \\(W\\) is assumed to be positive definite. Obviously the solutions to \\(\\mathcal{P}\\) and \\(\\mathcal{P}'\\) are the same. The two loss function values only differ by the constant term \\(y'Q'VQy\\).\nWe do not solve \\(\\mathcal{P}'\\) drectly, but we use Lagrangian duality and solve the dual quadratic programmng problem. The Lagrangian for \\(\\mathcal{P}'\\) is\n\\[\\begin{equation}\n\\mathcal{L}(x,\\lambda,\\mu)=\\frac12(x-y)'W(x-y)-\n\\lambda'(Ax - b)-\\mu'(Cx-d),\n(\\#eq:qplagrange)\n\\end{equation}\\]\nwhere \\(\\lambda\\geq 0\\) and \\(\\mu\\) are the Lagrange multipliers.\nNow\n\\[\\begin{align}\n\\begin{split}\n\\max_{\\lambda\\geq 0}\\max_\\mu\\mathcal{L}(x,\\lambda,\\mu)&=\\\\\n&=\\begin{cases}\\frac12(x-y)'W(x-y)-\\lambda'(Ax - b)-\\mu'(Cx-d)&\\text{ if }Ax\\geq b,\\\\\n+\\infty&\\text{ otherwise},\n\\end{cases}\n\\end{split}\n(\\#eq:qpxcalc1)\n\\end{align}\\]\nand thus\n\\[\\begin{equation}\n\\min_x\\max_{\\lambda\\geq 0}\\max_\\mu\\mathcal{L}(x,\\lambda,\\mu)=\\min_{Ax\\geq b}\\min_{Cx=d}\\frac12(x-y)'W(x-y),\n(\\#eq:qpxcalc2)\n\\end{equation}\\]\nwhich is our original simplified primal problem \\(\\mathcal{P}'\\).\nWe now look at the dual problem \\(\\mathcal{D}'\\) (of \\(\\mathcal{P}'\\)), which means solving\n\\[\\begin{equation}\n\\max_{\\lambda\\geq 0}\\max_\\mu\\min_x\\mathcal{L}(x,\\lambda,\\mu).\n(\\#eq:qpdual)\n\\end{equation}\\]\nThe inner minimum over \\(x\\) for given \\(\\lambda\\) and \\(\\mu\\) is attained at\n\\[\\begin{equation}\nx=y+W^{-1}(A'\\mid C')\\begin{bmatrix}\\lambda\\\\\\mu\\end{bmatrix},\n(\\#eq:qpxsolve)\n\\end{equation}\\]\nand is equal to \\(-g(\\lambda,\\mu)\\), where\n\\[\\begin{equation}\n\\frac12\\begin{bmatrix}\\lambda&\\mu\\end{bmatrix}\n\\begin{bmatrix}AW^{-1}A'&AW^{-1}C'\\\\CW^{-1}A'&CW^{-1}C'\\end{bmatrix}\\begin{bmatrix}\\lambda\\\\\\mu\\end{bmatrix}+\\\\\n+\\lambda'(Ay-b)\\\\\n+\\mu'(Cy-d)\n(\\#eq:qpdualf)\n\\end{equation}\\]\nOur strategy is to solve \\(\\mathcal{D'}\\) for \\(\\lambda\\geq 0\\) and/or \\(\\mu\\). Because of our biases we do not maximize \\(-g\\), we minimize \\(g\\). Then compute the solution of both \\(\\mathcal{P}'\\) and \\(\\mathcal{P}\\) from @ref(eq:qpxsolve). The duality theorem for quadratic programming tells us the values of \\(f\\) at the optimum of \\(\\mathcal{P}'\\) and \\(-g\\) at the optimum of \\(\\mathcal{D}'\\) are equal, and of course the value at the optimum of \\(\\mathcal{P}\\) is that of \\(\\mathcal{P}'\\) plus the constant \\(y'QVQy\\).\nFrom here on we can proceed with unweighting in various ways. We could, for instance, minimize out \\(\\mu\\) and then unweight the resulting quadratic form. Instead, we go the easy way. Majorize the partitioned matrix \\(K\\) in the quadratic part of @ref(eq:qpdualf) by a similarly partitioned diagonal positive matrix \\(E\\).\n\\[\\begin{equation}\nE:=\\begin{bmatrix}F&\\emptyset\\\\\\emptyset&G\\end{bmatrix}\\gtrsim K:=\\begin{bmatrix}AW^{-1}A'&AW^{-1}C'\\\\CW^{-1}A'&CW^{-1}C'\\end{bmatrix}\n(\\#eq:qpdumaj)\n\\end{equation}\\]\nSuppose \\(\\tilde\\lambda\\geq 0\\) and \\(\\tilde\\mu\\) are the current best solutions of the dual problem. Put them on top of each other to define \\(\\tilde\\gamma\\), and do the same with \\(\\lambda\\) and \\(\\mu\\) to get \\(\\gamma\\). Then \\(g(\\lambda,\\mu)\\) becomes\n\\[\\begin{equation}\n\\frac12 (\\tilde\\gamma+(\\gamma-\\tilde\\gamma))'E(\\tilde\\gamma+(\\gamma-\\tilde\\gamma))+\\gamma'(Ry-e)=\\\\\n=\\frac12(\\gamma-\\tilde\\gamma)'E(\\gamma-\\tilde\\gamma)+ (\\gamma-\\tilde\\gamma)'E(\\tilde\\gamma+(Ry-e))+\\\\+\\frac12\\tilde\\gamma'E\\tilde\\gamma+\\tilde\\gamma'(Ry-e)\n(\\#eq:qpdualcomp)\n\\end{equation}\\]\nThe last two terms do not depend on \\(\\gamma\\), so for the majorization algorithm is suffices to minimize\n\\[\\begin{equation}\n\\frac12(\\gamma-\\tilde\\gamma)'F(\\gamma-\\tilde\\gamma)+ (\\gamma-\\tilde\\gamma)'E(\\tilde\\gamma+(Ry-e))\n(\\#eq:qpdualproj)\n\\end{equation}\\]\nLet\n\\[\\begin{equation}\n\\xi:=\\tilde\\gamma-F^{-1}E(\\tilde\\gamma+(Ry-e))\n(\\#eq:qpdefxi)\n\\end{equation}\\]\nthen @ref(eq:qpdualproj) becomes\n\\[\\begin{equation}\n\\frac12(\\gamma-\\xi)'F(\\gamma-\\xi)-\\frac12\\xi'F\\xi\n(\\#eq:qpdualsimp)\n\\end{equation}\\]\nBecause \\(F\\) is diagonal \\(\\lambda_i=\\max(0,\\xi_i)\\) for \\(i=1,\\cdots m_1\\) and and \\(\\mu_i=\\xi_{i+m_1}\\) for \\(i=1,\\cdots m_2\\).\nSection @ref(apcodemathadd) has the R code for qpmaj(). The defaults are set to do a simple isotone regression, but of course the function has a much larger scope. It can handle equality constraints, linear convexity constraints, partial orders, and much more general linear inequalities. It can fit polynomials, monotone polynomials, splnes, and monotone splines of various sorts. It is possible to have only inequality constraints, only equality constraints, or both. The matrix \\(H\\) of predictors in @ref(eq:qpbase) can either be there or not be there.\nThe function qpmaj() returns both \\(x\\) and \\(\\lambda\\), and the values of \\(\\mathcal{P}\\), \\(\\mathcal{P}'\\), and \\(\\mathcal{D}'\\). And also the predicted values \\(Hx\\), and the constraint values \\(Ax-b\\) and \\(Cx-d\\), if applicable. It’s always nice to check complimentary slackness \\(\\lambda'(Ax-b)=0\\), and another check is provided because the values of \\(\\mathcal{P}'\\) and \\(\\mathcal{D}'\\) must be equal. Finally qpmaj() returns the number of iterations for the dual problem.\nThe function qpmaqj() does not have the pretense to compete in efficiency with the sophisticated pivoting and active set strategies for quadratic programming discussed for example by Best (2017). But it seems to do a reliable job on our small examples, and it is an interesting example of majorization and unweighting.\n\n9.3.2.1 Example 1: Simple Monotone Regression\nHere are the two simple monotone regression examples from section @ref(mathsimpiso), the first one without weights and the second one with a diagonal matrix of weights.\n\ny&lt;-c(1,2,1,3,2,-1,3)\nqpmaj(y)\n\n$x\n[1] 1.0 1.4 1.4 1.4 1.4 1.4 3.0\n\n$fprimal\n[1] 4.6\n\n$fdual\n[1] 4.6\n\n$lambda\n[1] 0.0000000 0.5999999 0.1999999 1.7999999 2.3999999 0.0000000\n\n$inequalities\n[1]  4.000001e-01 -2.760349e-08 -4.466339e-08 -4.466339e-08 -2.760349e-08\n[6]  1.600000e+00\n\n$itel\n[1] 146\n\n\n\nqpmaj(y, v = diag(c(1,2,3,4,3,2,1)))\n\n$x\n[1] 1.000000 1.400000 1.400000 1.777778 1.777778 1.777778 3.000000\n\n$fprimal\n[1] 11.37778\n\n$fdual\n[1] 11.37778\n\n$lambda\n[1] 0.000000 1.200000 0.000000 4.888889 5.555555 0.000000\n\n$inequalities\n[1]  4.000000e-01  0.000000e+00  3.777778e-01 -4.294379e-08 -2.976007e-08\n[6]  1.222222e+00\n\n$itel\n[1] 81\n\n\n\n\n9.3.2.2 Example 2: Monotone Regression with Ties\nNow suppose the data have tie-blocks, which we indicate with \\(\\{1\\}\\leq\\{2,3,4\\}\\leq\\{5,6\\}\\leq\\{7\\}\\). The Hasse diagram of the partial order (courtesy of Ciomek (2017)) is\n\n\n\n\n\n\n\n\n\nIn the primary approach to ties the inequality constraints \\(Ax\\geq 0\\) are coded with \\(A\\) equal to\n\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n [1,]   -1   +1   +0   +0   +0   +0   +0\n [2,]   -1   +0   +1   +0   +0   +0   +0\n [3,]   -1   +0   +0   +1   +0   +0   +0\n [4,]   +0   -1   +0   +0   +1   +0   +0\n [5,]   +0   -1   +0   +0   +0   +1   +0\n [6,]   +0   +0   -1   +0   +1   +0   +0\n [7,]   +0   +0   -1   +0   +0   +1   +0\n [8,]   +0   +0   +0   -1   +1   +0   +0\n [9,]   +0   +0   +0   -1   +0   +1   +0\n[10,]   +0   +0   +0   +0   -1   +0   +1\n[11,]   +0   +0   +0   +0   +0   -1   +1\n\n\nApplying our algorithm gives\n\nqpmaj(y, a = a)\n\n$x\n[1] 1.000000 1.333333 1.000000 1.333333 2.000000 1.333333 3.000000\n\n$fprimal\n[1] 4.333333\n\n$fdual\n[1] 4.333333\n\n$lambda\n [1] 0.000000e+00 2.069906e-15 0.000000e+00 0.000000e+00 6.666667e-01\n [6] 0.000000e+00 0.000000e+00 0.000000e+00 1.666667e+00 0.000000e+00\n[11] 0.000000e+00\n\n$inequalities\n [1]  3.333333e-01  4.107825e-15  3.333334e-01  6.666667e-01  8.182895e-08\n [6]  1.000000e+00  3.333333e-01  6.666666e-01 -8.182895e-08  1.000000e+00\n[11]  1.666667e+00\n\n$itel\n[1] 96\n\n\nIn the secondary approach we require \\(Cx=0\\), with \\(C\\) equal to\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]   +0   +1   -1   +0   +0   +0   +0\n[2,]   +0   +1   +0   -1   +0   +0   +0\n[3,]   +0   +0   +0   +0   +1   -1   +0\n\n\nIn addition we construct \\(A\\) to require \\(x_1\\leq x_2\\leq x_5\\leq x_7\\). This gives\n\nqpmaj(y, a = a, c = c)\n\n$x\n[1] 1.0 1.4 1.4 1.4 1.4 1.4 3.0\n\n$fprimal\n[1] 4.6\n\n$fdual\n[1] 4.6\n\n$lambda\n[1] 0.0 1.8 0.0\n\n$inequalities\n[1]  4.000000e-01 -5.100425e-08  1.600000e+00\n\n$mu\n[1] -0.4  1.6 -2.4\n\n$equations\n[1] -2.055633e-08 -2.055633e-08  3.443454e-08\n\n$itel\n[1] 163\n\n\nIn the tertiary approach, without weights, we require \\(x_1\\leq\\frac{x_2+x_3+x_4}{3}\\leq\\frac{x_5+x_6}{2}\\leq x_7\\) which means\n\na &lt;- matrix(c(-1,1/3,1/3,1/3,0,0,0,\n              0,-1/3,-1/3,-1/3,1/2,1/2,0,\n              0,0,0,0,-1/2,-1/2,1),\n              3,7,byrow = TRUE)\nmatrixPrint(a, d = 2, w = 5)\n\n     [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7] \n[1,] -1.00 +0.33 +0.33 +0.33 +0.00 +0.00 +0.00\n[2,] +0.00 -0.33 -0.33 -0.33 +0.50 +0.50 +0.00\n[3,] +0.00 +0.00 +0.00 +0.00 -0.50 -0.50 +1.00\n\n\nThis gives\n\nqpmaj(y, a = a)\n\n$x\n[1]  1.0  1.4  0.4  2.4  2.9 -0.1  3.0\n\n$fprimal\n[1] 1.35\n\n$fdual\n[1] 1.35\n\n$lambda\n[1] 0.0 1.8 0.0\n\n$inequalities\n[1]  4.000000e-01 -1.716935e-08  1.600000e+00\n\n$itel\n[1] 30\n\n\n\n\n9.3.2.3 Example 3: Weighted Rounding\nThis is a silly example in which a vector \\(y=\\) 0.5855288, 0.709466, -0.1093033, -0.4534972, 0.6058875, -1.817956, 0.6300986, -0.2761841, -0.2841597, -0.919322 is “rounded” so that its elements are between \\(-1\\) and \\(+1\\). The weights \\(V=W\\) are a banded positive definite matrix.\n\na&lt;-rbind(-diag(10),diag(10))\nb&lt;-rep(-1, 20)\nw&lt;-ifelse(outer(1:10,1:10,function(x,y) abs(x-y) &lt; 4), -1, 0)+7*diag(10)\nqpmaj(y, v = w, a = a, b = b)\n\n$x\n [1]  0.737345533  0.917584527  0.215666042 -0.075684750  1.000000017\n [6] -1.000000039  1.000000023  0.061944776 -0.002332657 -0.754345762\n\n$fprimal\n[1] 1.110748\n\n$fdual\n[1] 1.110749\n\n$lambda\n [1] 0.0000000 0.0000000 0.0000000 0.0000000 0.0722112 0.0000000 0.1554043\n [8] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[15] 0.0000000 2.8209838 0.0000000 0.0000000 0.0000000 0.0000000\n\n$inequalities\n [1]  2.626545e-01  8.241547e-02  7.843340e-01  1.075685e+00 -1.735205e-08\n [6]  2.000000e+00 -2.303727e-08  9.380552e-01  1.002333e+00  1.754346e+00\n[11]  1.737346e+00  1.917585e+00  1.215666e+00  9.243153e-01  2.000000e+00\n[16] -3.922610e-08  2.000000e+00  1.061945e+00  9.976673e-01  2.456542e-01\n\n$itel\n[1] 224\n\n\n\n\n9.3.2.4 Example 4: Monotone Polynomials\nThis example has a matrix \\(H\\) with the monomials of degree \\(1,2,3\\) on the 20 points \\(1,\\cdots 20\\). We want to fit a third-degree polynomial which is monotone, non-negative, and anchored at zero (which is why we do not have a monomial of degree zero, i.e. an intercept). Monotonicity is imposed by \\((h_{i+1}-h_{i})'x\\geq 0\\) and non-negativity by \\(h_1'x\\geq 0\\). Thus there are \\(19+1\\) inequality restrictions. For \\(y\\) we choose points on the quadratic curve \\(y=x^2\\), perturbed with random error.\n\nset.seed(12345)\nh &lt;- cbind(1:20,(1:20)^2,(1:20)^3)\na &lt;- rbind (h[1,],diff(diag(20)) %*% h)\ny&lt;-seq(0,1,length=20)^2+rnorm(20)/20\nplot(1:20, y)\nout&lt;-qpmaj(y,a=a,h=h,verbose=FALSE,itmax=1000, eps = 1e-15)\nlines(1:20,out$pred,type=\"l\",lwd=3,col=\"RED\")\n\n\n\n\n\n\n\n\nThe plot above and the output below shows what qpmaj() does in this case.\n\n\n$x\n[1] -2.311264e-03  2.292295e-03  1.895686e-05\n\n$fprimal\n[1] 0.0003265426\n\n$fdual\n[1] 0.0003265446\n\n$ftotal\n[1] 0.01655943\n\n$predict\n [1] -1.255287e-08  4.698305e-03  1.420869e-02  2.864490e-02  4.812065e-02\n [6]  7.274970e-02  1.026458e-01  1.379226e-01  1.786940e-01  2.250737e-01\n[11]  2.771753e-01  3.351127e-01  3.989996e-01  4.689497e-01  5.450767e-01\n[16]  6.274945e-01  7.163167e-01  8.116571e-01  9.136294e-01  1.022347e+00\n\n$lambda\n [1] 0.1587839 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [8] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[15] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n\n$inequalities\n [1] -1.255287e-08  4.698318e-03  9.510389e-03  1.443620e-02  1.947576e-02\n [6]  2.462905e-02  2.989609e-02  3.527686e-02  4.077138e-02  4.637964e-02\n[11]  5.210164e-02  5.793738e-02  6.388687e-02  6.995009e-02  7.612706e-02\n[16]  8.241776e-02  8.882221e-02  9.534040e-02  1.019723e-01  1.087180e-01\n\n$itel\n[1] 97\n\n\nWe now want to accomplish more or less the same thing, but using a cubic of the form \\(f(x)=x(c+bx+ax^2)\\). Choosing \\(a, b\\) and \\(c\\) to be nonnegative guarantees monotonicity (and convexity) on the positive axis, with a root at zero. If \\(b^2\\geq 4ac\\) then the cubic has two additional real roots, and by AM/GM we can guarantee this by \\(b\\geq a + c\\). So \\(a\\geq 0\\), \\(c\\geq 0\\), and \\(b\\geq a+c\\) are our three inequalities.\n\nh &lt;- cbind(1:20,(1:20)^2,(1:20)^3)\na &lt;- matrix(c(1,0,0,0,0,1,-1,1,-1), 3, 3, byrow = TRUE)\nplot(1:20, y)\nout&lt;-qpmaj(y,a=a,h=h,verbose=FALSE,itmax=10000, eps = 1e-15)\nlines(1:20,out$pred,type=\"l\",lwd=3,col=\"RED\")\n\n\n\n\n\n\n\n\nThe results of this alternative way of fitting the cubic are more or less indistinguishable from the earlier results, although this second approach is quite a bit faster (having only three inequalities instead of 21).\n\n\n$x\n[1] -5.673813e-09  1.945808e-03  3.098195e-05\n\n$fprimal\n[1] 0.0007170899\n\n$fdual\n[1] 0.000717091\n\n$ftotal\n[1] 0.01694997\n\n$predict\n [1] 0.001976784 0.008031075 0.018348765 0.033115745 0.052517907 0.076741142\n [7] 0.105971343 0.140394402 0.180196208 0.225562656 0.276679635 0.333733038\n[13] 0.396908757 0.466392682 0.542370707 0.625028722 0.714552619 0.811128290\n[19] 0.914941626 1.026178519\n\n$lambda\n[1] 0.2021458 0.0000000 0.0000000\n\n$inequalities\n[1] -5.673813e-09  3.098195e-05  1.914831e-03\n\n$itel\n[1] 25\n\n\n\n\n\n9.3.3 Examples\n\n\n\n\nBest, M. J. 2017. Quadratic Programming with Computer Programs. Advances in Applied Mathematics. CRC Press.\n\n\nCiomek, K. 2017. hasseDiagram: Drawing Hasse Diagram.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Polynomial MDS</span>"
    ]
  },
  {
    "objectID": "ordinal.html",
    "href": "ordinal.html",
    "title": "10  Ordinal MDS",
    "section": "",
    "text": "10.1 Monotone Regression\nIs it really what we want",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ordinal MDS</span>"
    ]
  },
  {
    "objectID": "ordinal.html#mathmonreg",
    "href": "ordinal.html#mathmonreg",
    "title": "10  Ordinal MDS",
    "section": "",
    "text": "10.1.1 Simple Monotone Regression\nEver since Kruskal (1964a) and Kruskal (1964b) monotone regression has played an important part in non-metric MDS. Too important, perhaps. Initially there was some competition with the rank images of Guttman (1968), but that competition has largely faded over time.\nWe only give the barest outline in this section. More details are in De Leeuw, Hornik, and Mair (2009). In (simple least squares) monotone (or isotone) regression we minimize \\((x-y)'W(x-y)\\), where \\(W\\gtrsim 0\\) is diagonal, over \\(x\\) satisfying \\(x_1\\leq\\cdots\\leq x_n\\). The vector \\(y\\) is the target or the data.\nThe algorithm, which is extremely fast and of order \\(n\\), is based on the simple rule that if elements are out of order, then you compute their weighted average, forming blocks, keeping track of the block sizes and block weights. This reduces the MR problem to a smaller MR problem, and following the rule systematically leads to a finite algorithm. A particularly efficient implementation is in (busing_21?).\nA simple illustration. The first column are the value that we compute the best monotone fit for, the second columns are the size of the blocks after merging. In this case there are no weights, in fact the block sizes serve as weights.\n\\[\\begin{align}\n&(1,2,1,3,2,-1,3) \\qquad &(1,1,1,1,1,1,1)\\\\\n&(1,\\frac32,3,2,-1,3) \\qquad &(1,2,1,1,1,1)\\\\\n&(1,\\frac32,\\frac52,-1,3) \\qquad &(1,2,2,1,1)\\\\\n&(1,\\frac32,\\frac43,3) \\qquad &(1,2,3,1)\\\\\n&(1,\\frac75,3) \\qquad &(1,5,1)\n\\end{align}\\]\nExpanding using the block size gives the solution \\((1,\\frac75,\\frac75,\\frac75,\\frac75,\\frac75,3)\\).\nIn the second example we do have weights, in the second column, and we use a third column for blocks size.\n\\[\\begin{align}\n&(1,2,1,3,2,-1,3) \\qquad &(1,2,3,4,3,2,1) \\qquad &(1,1,1,1,1,1,1)\\\\\n&(1,\\frac75,3,2,-1,3) \\qquad &(1,5,4,3,2,1) \\qquad &(1,2,1,1,1,1)\\\\\n&(1,\\frac75,\\frac{18}{7},-1,3) \\qquad &(1,5,7,2,1) \\qquad &(1,2,2,1,1)\\\\\n&(1,\\frac75,\\frac{16}{9},3) \\qquad &(1,5,9,1) \\qquad &(1,2,3,1)\n\\end{align}\\]\nExpansion gives the solution \\((1,\\frac75,\\frac75,\\frac{16}{9},\\frac{16}{9},\\frac{16}{9},3)\\).\nThe usual monotone regression algorithms used in MDS allow for slightly more complicated orders to handle ties in the data . There are basically three approaches to ties implemented. In what Kruskal calls the primary approach, only order relations between tie blocks are maintained. Within blocks no order s mposed. In the secondary approach we require equality in tie blocks. Ties in the data means we impose ties in the isotone regression. Both approaches can be incorporated in simple monotone regresson by preprocessing. The secondary approach starts with the weighted averages of the tie blocks, the primary approach orders the data within tie blocks so they are non-decreasing. De Leeuw (1977) showed that this preprocessing does indeed give the least squares solution for both approaches. In the same paper he also introduces a less restrictive tertiary approach, which merely requires that the averages of the tie blocks are in the required order.\n\n\n10.1.2 Weighted Monotone Regression\n\\[\n(x-y)'V(x-y)\n\\] \\[V(x-y)=A'\\lambda\\] \\[Ax\\geq 0\\] \\[\\lambda\\geq 0\\] \\[\\lambda'Ax=0\\] \\[x=y+V^{-1}A'\\lambda\\] go to the dual if \\(\\lambda_i&gt;0\\) then \\(a_i'x=0\\)\nunweighting actually proves weighted is unweighted for something else\n\\(MR(x+\\epsilon y)=MR(x)+\\epsilon B(y)\\) if \\(MR(x)=Bx\\)\n\n\n10.1.3 Normalized Cone Regression\nDe Leeuw (1975)\nBauschke, Bui, and Wang (2018)\n\n\n10.1.4 Iterative MR\nprimal-dual: MR is dual Dykstra vertices of cone plus CCA one iteration only",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ordinal MDS</span>"
    ]
  },
  {
    "objectID": "ordinal.html#nmdsals",
    "href": "ordinal.html#nmdsals",
    "title": "10  Ordinal MDS",
    "section": "10.2 Alternating Least Squares",
    "text": "10.2 Alternating Least Squares\nsmacof: hard squeeze double phase",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ordinal MDS</span>"
    ]
  },
  {
    "objectID": "ordinal.html#nmdskruskal",
    "href": "ordinal.html#nmdskruskal",
    "title": "10  Ordinal MDS",
    "section": "10.3 Kruskal’s Approach",
    "text": "10.3 Kruskal’s Approach\n\n10.3.1 Kruskal’s Stress\nRemember that Kruskal’s definition of stress was intended for ordinal multidimensional scaling only. Thus dissimilarities are not necessarily numerical, as in basic MDS, only their rank order is known. He first defined raw stress as\n\\[\\begin{equation}\n\\sigma^\\star_K(X):=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n} (\\hat d_{ij}-d_{ij}(X))^2,\n(\\#eq:rawstress1)\n\\end{equation}\\]\nwhere the \\(\\hat d_{ij}\\) is some set of numbers monotone with the dissimilarities.\n\nTo simplify the discussion, we delay the precise definition of \\(\\hat d\\), for a little while. (Kruskal (1964a), p. 8)\n\nKruskal then mentions that raw stress satisfies \\(\\sigma^\\star_K(\\alpha X)=\\alpha^2\\sigma^\\star_K(X)\\), which is clearly undesirable because the size of the configuration should not influence the quality of the fit.\n\nAn obvious way to cure this defect in the raw stress is to divide it by a scaling factor, that is, a quantity which has the same quadratic dependence on the scale of the configuration that raw stress does. (Kruskal (1964a), p. 8).\n\nBy the way, although the precise definition of \\(\\hat D\\) has been delayed, the uniform stretching/shrinking argument already assumes that if we multiply \\(D\\) by \\(\\alpha\\) then \\(\\hat D\\) also gets multiplied by \\(\\alpha\\). Thus it sort of gives away that \\(\\hat D\\) is also a function of \\(X\\), at least of the scale of \\(X\\).\nFor the normalization of raw stress Kruskal chooses\n\\[\\begin{equation}\n\\tau^\\star_K(X):=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n} d_{ij}^2(X),\n(\\#eq:rawtau)\n\\end{equation}\\]\n\nFinally, it is desirable to use the square root of this expression, which is analogous to choosing the standard deviation in place of the variance. (Kruskal (1964a), p. 9)\n\nThus Kruskal’s normalized loss function for ordinal MDS becomes\n\\[\\begin{equation}\n\\sigma_K(X):=\\sqrt{\\frac{\\sigma^\\star_K(X)}{\\tau^\\star_K(X)}}=\\sqrt{\\frac{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n} (\\hat d_{ij}-d_{ij}(X))^2}{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n} d_{ij}^2(X)}}.\n(\\#eq:kruskalstress)\n\\end{equation}\\]\nAt this point in Kruskal (1964a) the definition of \\(\\hat D\\) still hangs in the air, although we know that the \\(\\hat D\\) are monotone with \\(\\Delta\\), and that multiplying \\(X\\) by a constant will multiply both \\(D(X)\\) and \\(\\hat D\\) by the same constant. Matters are clarified right after the definition of stress.\n\nNow it is easy to define the \\(\\hat d_{ij}\\). They are the numbers which minimize \\(\\sigma\\) (or equivalently, \\(\\sigma^\\star\\)) subject to the monotonicity constraints. (Kruskal (1964a), p. 9)\n\nThus, actually, raw stress is the minimum over the pseudo-distance matrices \\(\\Omega\\) in \\(\\mathfrak{D}\\), the set of all monotone transformations of the dissimilarities.\n\\[\\begin{equation}\n\\sigma^\\star(X):=\\min_{\\Omega\\in\\mathfrak{D}}\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n} (\\omega_{ij}-d_{ij}(X))^2,\n(\\#eq:rawstressfinal)\n\\end{equation}\\]\nand \\(\\hat D\\) is the minimizer, which is now clearly a function of \\(X\\),\n\\[\\begin{equation}\n\\hat D(X):=\\mathop{\\text{argmin}}_{\\Omega\\in\\mathfrak{D}}\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n} (\\omega_{ij}-d_{ij}(X))^2.\n(\\#eq:dhatdef)\n\\end{equation}\\]\nSo, finally,\n\\[\\begin{equation}\n\\sigma(X):=\\min_{\\Omega\\in\\mathfrak{D}}\\sqrt{\\frac{\\sigma^\\star(X)}{\\tau^\\star(X)}}=\\sqrt{\\frac{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n} (\\omega_{ij}-d_{ij}(X))^2}{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n} d_{ij}^2(X)}}.\n(\\#eq:kruskalstressfinal)\n\\end{equation}\\]\nIn Guttman’s terminology Kruskal’s approach is hard squeeze single phase. Thus what is minimized is\n\\[\n\\sigma_{JBK}^{\\ }(X):=\\min_{ \\Delta\\in\\mathfrak{D}}\\sqrt{\\frac{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}(\\delta_{ij}-d_{ij}(X))^2}{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}d_{ij}^2(X)}}\n\\]\n\n\n10.3.2 Stress1 and Stress2",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ordinal MDS</span>"
    ]
  },
  {
    "objectID": "ordinal.html#nmdsguttman",
    "href": "ordinal.html#nmdsguttman",
    "title": "10  Ordinal MDS",
    "section": "10.4 Guttman’s Approach",
    "text": "10.4 Guttman’s Approach\nThe main alternative to the Kruskal approach to MDS, besides smacof, is the Smallest Space Analysis (SSA) of Guttman and Lingoes. I have mixed feelings about the fundamental SSA paper of Guttman (1968). It is, no doubt, a milestone MDS paper, and some of the distinctions it makes (which we will discuss later in this section) are clearly important. Its use of matrix algebra, wherever possible, is an improvement over Kruskal (1964a), and the correction matrix algorithm for SSA is an immediate predecessor of smacof. But it seems to me the derivation of the correction matrix algorithm is incomplete and could even be called incorrect. The rank images used by Guttman and Lingoes in SSA seem an ad-hoc solution invented by someone who did not yet know about monotone regression. And, above all, the paper exudes a personality cult-like atmosphere that is somewhat repellent to me. There are no gurus in science. Or at least there should not be. It is true that between 1930 and 1960 Guttman invented and elucidated about 75% of the psychometrics of his time, but 75% is still less than 100%. This book you are reading now may set a record in self-citation, but that makes sense because it is supposed to document my work in MDS and to give access to the pdf’s of my unpublished work. I try to be careful not to take credit for results that did not originate with me, and to give appropriate attributions in all cases.\n\n10.4.0.1 Rank Images\nThe rank image transformation, which replaces the monotone regression in Kruskal’s approach, has a rather complicated definition. It is simple enough when both \\(\\Delta\\) and \\(D(X)\\) have no ties. In that case the rank image \\(D^\\star\\) is just the unique permutation of \\(D(X)\\) that is monotone with \\(\\Delta\\). Thus\n\\[\\begin{equation}\n\\delta_{ij}&lt;\\delta_{kl}\\Leftrightarrow d_{ij}^\\star&lt;d_{kl}^\\star.\n(\\#eq:nmrankimage0)\n\\end{equation}\\]\nIf there are ties in \\(\\Delta\\) and/or \\(D(X)\\) then some of the uniqueness and simplicity will get lost. Guttman (1968) introduces an elaborate notation for rank images with ties, but that notation does neither him nor the reader any favors.\nIf there are ties in \\(D(X)\\) you use the rank order of the corresponding elements of \\(\\Delta\\) to order \\(D(X)\\) within tie blocks. If two elements are tied both in \\(D(X)\\) and \\(\\Delta\\), then their order in the tie block is arbitrary.\nSuppose the \\(\\Delta\\) have \\(R\\) tie-blocks, in increasing order, with \\(m_1,\\cdots,m_R\\) elements. The smallest \\(m_1\\) elements of the vector of distances become the first \\(m_1\\) elements of \\(D^\\star\\), the next \\(m_2\\) elements of \\(D^\\star\\) are the next smallest \\(m^2\\) elements of distance vector, and so on for all tie blocks. Thus tied elements in \\(\\Delta\\) can become untied in \\(D^\\star\\) and untied elements in \\(\\Delta\\) can becomes tied in \\(D^\\star\\). We require\n\\[\\begin{equation}\n\\delta_{ij}&lt;\\delta_{kl}\\Rightarrow d_{ij}^\\star\\leq d_{kl}^\\star\n(\\#eq:nmrankimage1)\n\\end{equation}\\]\nThis corresponds with Kruskal’s primary approach to ties. Guttman calls it semi-strong monotonicity. There is a small numerical example in table @ref(tab:nmriexample1).\n\n\n\nSemi-strong Rank Images\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\n\\(\\Delta\\)\n1\n2\n2\n3\n3\n4\n5\n\n\n\\(D(X)\\)\n1\n3\n1\n3\n4\n3\n4\n\n\n\\(D(X)\\ \\text{ordered}\\)\n1\n1\n3\n3\n3\n4\n4\n\n\n\\(D^\\star\\)\n1\n1\n3\n3\n3\n4\n4\n\n\n\n\n\nThe sum of the squared differences between \\(D(X)\\) and \\(D^\\star\\) is 6.\nAlternatively, we can require that tied elements in \\(\\Delta\\) correspond with tied elements in \\(D^\\star\\). Guttman calls this strong monotonicity, and requires in addition that \\(D^\\star\\) has the same number of blocks, with the same block sizes, as \\(\\Delta\\). Instead of copying ordered blocks from the sorted distances, we compute averages of blocks, and copy those into \\(D^star\\). This corresponds with Kruskal’s secondary approach to ties. Thus the elements of \\(D^\\star\\) are no longer a permutation of those in \\(D(X)\\). We have @eq:nmrankimage1, and also\n\\[\\begin{equation}\n\\delta_{ij}=\\delta_{kl}\\Rightarrow d_{ij}^\\star=d_{kl}^\\star\n(\\#eq:nmrankimage2)\n\\end{equation}\\]\nOur numerical example is now in table @ref(tab:nmriexample2).\n\n\n\nStrong Rank Images\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\n\\(\\Delta\\)\n1\n2\n2\n3\n3\n4\n5\n\n\n\\(D(X)\\)\n1\n3\n1\n3\n4\n3\n4\n\n\n\\(D(X)\\ \\text{ordered}\\)\n1\n1\n3\n3\n3\n4\n4\n\n\n\\(D^\\star\\)\n1\n2\n2\n3\n3\n4\n4\n\n\n\n\n\nNow the sum of squared differences between \\(D(X)\\) and \\(D^\\star\\) is 4, which means, surprisingly, that strong monotonicity gives a better fit than semi-strong monotonicity. This cannot happen with monotone regression, where the primary approch to ties always has a better fit than the secondary approach.\n\n\n10.4.0.2 Single and Double Phase\nNote: suppose \\(\\|D_1^\\star-D(X_2)\\|^2&lt;\\|D_1^\\star-D(X_1)\\|^2\\) but \\(\\|D_2^\\star-D(X_2)\\|^2&gt;\\|D_1^\\star-D(X_2)\\|^2\\)\n\\[\n\\sigma_G(X)=\\frac{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(d^\\star_{ij}(X)-d_{ij}(X))^2}{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}d_{ij}^2(X)}.\n\\]\n\\[\n\\sigma_G(X,D^\\star)=\\frac{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}(d^\\star_{ij}-d_{ij}(X))^2}{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}d_{ij}^2(X)}.\n\\]\n\n\n10.4.0.3 Hard and Soft Squeeze\n\\[\n\\sigma(X):=\\min_{\\delta\\in\\mathbb{K}\\cap\\mathbb{S}}\\sum_{k\\in\\mathcal{K}}w_k(\\delta_k-d_k(X))^2\n\\]\nquestion: in double phase do rank images decrease stress ? My guess is yes. Are they continuous ?\n\\[\\rho_G(X)=\\max_{P\\in\\Pi} \\delta'Pd(X)\\] is a continuous function of \\(X\\). Also (Shepard) \\[\nD_+\\rho_G(X)=\\max_{P\\in\\Pi(X)}\\delta'P\\mathcal{D}d(X)\n\\]\nrank-images \\(Pd\\) are not continuous\n\n\n10.4.1 Smoothness of Ordinal Loss Functions\nKruskal\n\\[\n\\min_{\\hat D\\in\\mathfrak{D}}\\sum\\sum w_{ij}(\\hat d_{ij}-d_{ij}(X))^2\n\\] is a differentiable function of \\(X\\)\nDouble phase rank image\n\\[\n\\sigma_{LG}(X)=\\min_P\\|Pd(X)-d(X)\\|^2\n\\] \\(P\\) in the Birkhoff polytope and satisfying inequalities, equalities.\n\\(P\\) given by \\(\\max_P d(X)'Pd(X)\\)\nDe Leeuw (1973)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ordinal MDS</span>"
    ]
  },
  {
    "objectID": "ordinal.html#scaling-with-distance-bounds",
    "href": "ordinal.html#scaling-with-distance-bounds",
    "title": "10  Ordinal MDS",
    "section": "10.5 Scaling with Distance Bounds",
    "text": "10.5 Scaling with Distance Bounds\n\\[\n\\alpha_{ij}\\leq d_{ij}(X)\\leq\\beta_{ij}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ordinal MDS</span>"
    ]
  },
  {
    "objectID": "ordinal.html#bounds-on-stress",
    "href": "ordinal.html#bounds-on-stress",
    "title": "10  Ordinal MDS",
    "section": "10.6 Bounds on Stress",
    "text": "10.6 Bounds on Stress\nDe Leeuw and Stoop (1984)\nStress1 and Stress2\n\n\n\n\nBauschke, H. H., M. N. Bui, and X. Wang. 2018. “Projecting onto the Intersection of a Cone and a Sphere.” SIAM Journal on Optimization 28: 2158–88.\n\n\nDe Leeuw, J. 1973. “Smoothness Properties of Nonmetric Loss Functions.” Technical Memorandum. Murray Hill, N.J.: Bell Telephone Laboratories.\n\n\n———. 1975. “A Normalized Cone Regression Approach to Alternating Least Squares Algorithms.” Department of Data Theory FSW/RUL.\n\n\n———. 1977. “Correctness of Kruskal’s Algorithms for Monotone Regression with Ties.” Psychometrika 42: 141–44.\n\n\nDe Leeuw, J., K. Hornik, and P. Mair. 2009. “Isotone Optimization in R: Pool-Adjacent-Violators Algorithm (PAVA) and Active Set Methods.” Journal of Statistical Software 32 (5): 1–24.\n\n\nDe Leeuw, J., and I. Stoop. 1984. “Upper Bounds for Kruskal’s Stress.” Psychometrika 49: 391–402.\n\n\nGuttman, L. 1968. “A General Nonmetric Technique for Fitting the Smallest Coordinate Space for a Configuration of Points.” Psychometrika 33: 469–506.\n\n\nKruskal, J. B. 1964a. “Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesis.” Psychometrika 29: 1–27.\n\n\n———. 1964b. “Nonmetric Multidimensional Scaling: a Numerical Method.” Psychometrika 29: 115–29.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ordinal MDS</span>"
    ]
  },
  {
    "objectID": "splinical.html",
    "href": "splinical.html",
    "title": "11  Splinical MDS",
    "section": "",
    "text": "11.1 Splines\nIn this section we give a short introduction, with examples, to (univariate) splines, B-splines, and I-splines. It is taken from De Leeuw (2017), with some edits to make it fit into the book. The report it was taken from has more detail and more examples.\nTo define spline functions we first define a finite sequence of knots \\(T=\\{t_j\\}\\) on the real line, with \\(t_1\\leq\\cdots\\leq t_p,\\) and an order \\(m\\). In addition each knot \\(t_j\\) has a multiplicity \\(m_j\\), the number of knots equal to \\(t_j\\). We suppose throughout that \\(m_j\\leq m\\) for all \\(j\\).\nA function \\(f\\) is a spline function of order \\(m\\) for a knot sequence \\(\\{t_j\\}\\) if\nHere we use \\(\\mathcal{D}^{(s)}_-\\) and \\(\\mathcal{D}^{(s)}_+\\) for the left and right \\(s^{th}\\)-derivative operator. If \\(m_j=m\\) for some \\(j\\), then the second requirement is empty, if \\(m_j=m-1\\) then the second requirement means \\(\\pi_j(t_j)=\\pi_{j+1}(t_j)\\), i.e. we require continuity of \\(f\\) at \\(t_j\\). If \\(1\\leq m_j&lt;m-1\\) then \\(f\\) must be \\(m-m_j-1\\) times differentiable, and thus continuously differentiable, at \\(t_j\\).\nIn the case of simple knots (with multiplicity one) a spline function of order one is a step function which steps from one level to the next at each knot. A spline of order two is piecewise linear, with the pieces joined at the knots so that the spline function is continuous. Order three means a piecewise quadratic function which is continuously differentiable at the knots. And so on.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Splinical MDS</span>"
    ]
  },
  {
    "objectID": "splinical.html#mathsplines",
    "href": "splinical.html#mathsplines",
    "title": "11  Splinical MDS",
    "section": "",
    "text": "\\(f\\) is a polynomial \\(\\pi_j\\) of degree at most \\(m-1\\) on each half-open interval \\(I_j=[t_j,t_{j+1})\\) for \\(j=1,\\cdots,p\\),\nthe polynomial pieces are joined in such a way that \\(\\mathcal{D}^{(s)}_-f(t_j)=\\mathcal{D}^{(s)}_+f(t_j)\\) for \\(s=0,1,\\cdots,m-m_j-1\\) and \\(j=1,2,\\cdots,p\\).\n\n\n\n\n11.1.1 B-splines\nAlternatively, a spline function of order \\(m\\) can be defined as a linear combination of B-splines (or basic splines) of order \\(m\\) on the same knot sequence. A B-spline of order \\(m\\) is a spline function consisting of at most \\(m\\) non-zero polynomial pieces. A B-spline \\(\\mathcal{B}_{j,m}\\) is determined by the \\(m+1\\) knots \\(t_j\\leq\\cdots\\leq t_{j+m}\\), is zero outside the interval \\([t_j,t_{j+m})\\), and positive in the interior of that interval. Thus if \\(t_j=t_{j+m}\\) then \\(\\mathcal{B}_{j,m}\\) is identically zero.\nFor an arbitrary finite knot sequence \\(t_1,\\cdots,t_p\\), there are \\(p-m\\) B-splines to of order \\(m\\) to be considered, although some may be identically zero. Each of the splines covers at most \\(m\\) consecutive intervals, and at most \\(m-1\\) different B-splines are non-zero at each point.\n\n11.1.1.1 Boundaries\nB-splines are most naturally and simply defined for doubly infinite sequences of knots, that go to \\(\\pm\\infty\\) in both directions. In that case we do not have to worry about boundary effects, and each subsequence of \\(m+1\\) knots defines a B-spline. For splines on finite sequences of \\(p\\) knots we have to decide what happens at the boundary points.\nThere are B-splines for \\(t_j,\\cdots,t_{j+m}\\) for all \\(j=1,\\cdots,p-m\\). This means that the first \\(m-1\\) and the last \\(m-1\\) intervals have fewer than \\(m\\) splines defined on them. They are not part of what De Boor (2001), page 94, calls the basic interval. For doubly infinite sequences of knots there is not need to consider such a basic interval.\nIf we had \\(m\\) additional knots on both sides of our knot sequence we would also have \\(m\\) additional B-splines for \\(j=1-m,\\cdots,0\\) and \\(m\\) additional B-splines for \\(j=p-m+1,\\cdots,p\\). By adding these additional knots we make sure each interval \\([t_j,t_{j+1})\\) for \\(j=1,\\cdots,p-1\\) has \\(m\\) B-splines associated with it. There is stil some ambiguity on what to do at \\(t_p\\), but we can decide to set the value of the spline there equal to the limit from the left, thus making the B-spline left-continuous there.\nIn our software we will use the convention to define our splines on a closed interval \\([a,b]\\) with \\(r\\) interior knots \\(a&lt;t_1&lt;\\cdots&lt;t_r&lt;b\\), where interior knot \\(t_j\\) has multiplicity \\(m_j\\). We extend this to a series of \\(p=M+2m\\) knots, with \\(M=\\sum_{j=1}^r m_j\\), by starting with \\(m\\) copies of \\(a\\), appending \\(m_j\\) copies of \\(t_j\\) for each \\(j=1,\\cdots,r\\), and finishing with \\(m\\) copies of \\(b\\). Thus \\(a\\) and \\(b\\) are both knots with multiplicity \\(m\\). This defines the extended partition (Schumaker (2007), p 116), which is just handled as any knot sequence would normally be.\n\n\n11.1.1.2 Normalization\nThe conditions we have mentioned only determine the B-spline up to a normalization. There are two popular ways of normalizing B-splines. The \\(N\\)-splines \\(N_{j,m}\\), a.k.a. the normalized B-splines \\(j\\) or order \\(m\\), satisfies \\[\\begin{equation}\\label{E:nsum}\n\\sum_{j}N_{j,m}(t)=1.\n\\end{equation}\\] Note that in general this is not true for all \\(t\\), but only for all \\(t\\) in the basic interval.\nAlternatively we can normalize to \\(M\\)-splines, for which \\[\\begin{equation}\\label{E:mint}\n\\int_{-\\infty}^{+\\infty}M_{j,m}(t)dt=\\int_{t_j}^{t_{j+k}}M_{j,m}(t)dt=1.\n\\end{equation}\\] There is the simple relationship \\[\\begin{equation}\\label{E:NM}\nN_{j,m}(t)=\\frac{t_{j+m}-t_j}{m}\\ M_{j,m}(t).\n\\end{equation}\\]\n\n\n11.1.1.3 Recursion\nB-splines can be defined in various ways, using piecewise polynomials, divided differences, or recursion. The recursive definition, first used as the preferred definition of B-splines by De Boor and Höllig (1985), is the most convenient one for computational purposes, and that is the one we use.\nThe recursion is due independently to Cox (1972) for simple knots and to De Boor (1972) in the general case, is \\[\\begin{equation}\\label{E:Mspline}\nM_{j,m}(t)=\\frac{t-t_j}{t_{j+m}-t_j}M_{j,m-1}(t)+\\frac{t_{j+m}-t}{t_{j+m}-t_j}M_{j+1,m-1}(t),\n\\end{equation}\\] or \\[\\begin{equation}\\label{E:Nspline}\nN_{j,m}(t)=\\frac{t-t_j}{t_{m+j-1}-t_j}N_{j,m-1}(t)+\\frac{t_{j+m}-t}{t_{j+m}-t_{j+1}}N_{j+1,m-1}(t).\n\\end{equation}\\]\nA basic result in the theory of B-splines is that the different B-splines are linearly independent and form a basis for the linear space of spline functions (of a given order and knot sequence).\nIn section @ref(apcodemathadd) the basic BSPLVB algorithm from De Boor (2001), page 111, for normalized B-splines is translated to R and C. There are two auxiliary routines, one to create the extended partition, and one that uses bisection to locate the knot interval in which a particular value is located (Schumaker (2007), p 191). The R function bsplineBasis() takes an arbitrary knot sequence. It can be combined with extendPartition(), which uses inner knots and boundary points to create the extended partion.\n\n\n11.1.1.4 Illustrations\nFor our example, which is the same as the one from figure 1 in Ramsay (1988), we choose \\(a=0\\), \\(b=1\\), with simple interior knots 0.3, 0.5, 0.6. First the step functions, which have order 1.\n\n\n\n\n\nZero Degree Splines with Simple Knots\n\n\n\n\nNow the hat functions, which have order 2, again with simple knots.\n\n\n\n\n\nPiecewise Linear Splines with Simple Knots\n\n\n\n\nNext piecewise quadratics, with simple knots, which implies continuous differentiability at the knots. This are the N-splines corresponding with the M-splines in figure 1 of Ramsay (1988).\n\n\n\n\n\nPiecwise Quadratic Splines with Simple Knots\n\n\n\n\nIf we change the multiplicities to 1, 2, 3, then we lose some of the smoothness.\n\n\n\n11.1.2 I-splines\nThere are several ways to require splines to be monotone increasing. Since B-splines are non-negative, the definite integral of a B-spline of order \\(m\\) from the beginning of the interval to a value \\(x\\) in the interval is an increasing spline of order \\(m+1\\). Integrated B-splines are known as I-splines (Ramsay (1988)). Non-negative linear combinations I-splines can be used as a basis for the convex cone of increasing splines. Note, however, that if we use an extended partition, then all I-splines start at value zero and end at value one, which means their convex combinations are those splines that are also probability distributions on the interval. To get a basis for the increasing splines we need to add the constant function to the I-splines and allow it to enter the linear combination with either sign.\nI-splines are most economically computed by using the formula first given by Gaffney (1976). If \\(\\ell\\) is defined by \\(t_{j+\\ell-1}\\leq x&lt;t_{j+\\ell}\\) then \\[\n\\int_{x_j}^x M_{j,m}(t)dt=\\frac{1}{m}\\sum_{r=0}^{ \\ell-1}(x-x_{j+r})M_{j+r,m-r}(x)\n\\] It is somewhat simpler, however, to use lemma 2.1 of De Boor, Lyche, and Schumaker (1976). This says \\[\n\\int_a^xM_{j,m}(t)dt=\\sum_{\\ell\\geq j}N_{\\ell,m+1}(x)-\\sum_{\\ell\\geq j}N_{\\ell,m+1}(a),\n\\] If we specialize this to I-splines, we find , as in De Boor (1976), formula 4.11, \\[\n\\int_{-\\infty}^x M_{j,m}(t)dt=\\sum_{\\ell=j}^{j+r}N_{\\ell,m+1}(x)\n\\] for \\(x\\leq t_{j+r+1}\\). This shows that I-splines can be computed by using cumulative sums of B-spline values.\nNote that using the definition using integration does not give a natural way to define increasing splines of degree one, i.e. increasing step functions. There is no such problem with the cumulative sum approach.\n\n11.1.2.1 Increasing Coefficients\nAs we know, a spline is a linear combination of B-splines. The formula for the derivative of a spline, for example in De Boor (2001), p 116, shows that a spline is increasing if the coefficients of the linear combination of B-splines are increasing. Thus we can fit an increasing spline by restricting the coefficients of the linear combination to be increasing, again using the B-spline basis.\nIt turns out this is in fact identical to using I-splines. If the B-spline values at \\(n\\) points are in an \\(n\\times r\\) matrix \\(H\\), then non-decreasing coefficients \\(\\beta\\) are of the form \\(\\beta=S\\alpha+\\gamma e_r\\), where \\(S\\) is lower-diagonal with all elements on and below the diagonal equal to one, where \\(\\alpha\\geq 0\\), where \\(e_r\\) has all elements equal to one, and where \\(\\gamma\\) can be of any sign. So \\(H\\beta=(HS)\\alpha+\\gamma e_n\\). Thus non-decreasing coefficients is the same thing as using cumnulative sums of the B-spline basis.\n\n\n11.1.2.2 Increasing Values\nFinally, we can simply require that the \\(n\\) elements of \\(H\\beta\\) are increasing. This is a less restrictive requirement, because it allows for the possibility that the spline is decreasing between data values. It has the rather serious disadvantage, however, that it does its computations in \\(n\\)-dimensional space, and not in \\(r\\)-dimensional space, where \\(r=M+m\\), which is usually much smaller than \\(n\\). Software for the increasing-value restrictions has been written by De Leeuw (2015). In our software, however, we prefer the cumsum() approach. It is less general, but considerably more efficient.\nWe use the same Ramsay example as before, but now cumulatively. First we integrate step functions with simple knots, which have order one, using isplineBasis(). The corresponding I-splines are piecewise linear with order two.\n\n\n\n\n\nNot Sure\n\n\n\n\nNow we integrate the hat functions, which have order 2, again with simple knots, to find piecewise quadratic I-splines of order 3. These are the functions in the example of Ramsay (1988).\n\n\n\n\n\nMonotone Piecewise Linear Splines with Simple Knots\n\n\n\n\nFinally, we change the multiplicities to 1, 2, 3, and compute the corresponding piecewise quadratic I-splines.\n\n\n\n11.1.3 Time Series Example\nOur first example smoothes a time series by fitting a spline. We use the number of births in New York from 1946 to 1959 (on an unknown scale), from Rob Hyndman’s time series archive.\n\n11.1.3.1 B-splines\nFirst we fit B-splines of order three. The basis matrix uses \\(x\\) equal to \\(1:168\\), with inner knots 12, 24, 36, 48, 60, 72, 84, 96, 108, 120, 132, 144, 156, and interval \\([1,168]\\).\n\ninnerknots &lt;- 12 * 1:13\nmultiplicities &lt;- rep(1, 13)\nlowend &lt;- 1\nhighend &lt;- 168\norder &lt;- 3\nx &lt;- 1:168\nknots &lt;-\n  extendPartition (innerknots, multiplicities, order, lowend, highend)$knots\nh &lt;- bsplineBasis (x, knots, order)\nu &lt;- lm.fit(h, births)\nres &lt;- sum ((births - h %*% u$coefficients) ^ 2) / 2\n\n\n\n\n\n\nMonotone Piecewise Quadratic Splines with Simple Knots\n\n\n\n\nThe residual sum of squares is 114.6917709.\n\n\n11.1.3.2 I-splines\nWe now fit the I-spline using the B-spline basis. Compute \\(Z=HS\\) using cumsum(), and then \\(\\overline y\\) and \\(\\overline Z\\) by centering (substracting the column means). The formula is \\[\n\\min_{\\alpha\\geq 0,\\gamma}\\mathbf{SSQ}\\ (y-Z\\alpha-\\gamma e_n)=\\min_{\\alpha\\geq 0}\\mathbf{SSQ}\\ (\\overline y-\\overline Z\\alpha).\n\\] We use pnnls() from Wang, Lawson, and Hanson (2015).\n\nknots &lt;- extendPartition (innerknots, multiplicities, order, lowend, highend)$knots\nh &lt;- isplineBasis (x, knots, order)\ng &lt;- cbind (1, h[,-1])\nu &lt;- pnnls (g, births, 1)$x\nv &lt;- g%*%u\n\n\n\n\n\n\nMonotone Piecewise Linear Splines with Simple Knots\n\n\n\n\nThe residual sum of squares is 144.2027491.\n\n\n11.1.3.3 B-Splines with monotone weights\nJust to make sure, we also solve the problem \\[\n\\min_{\\beta_1\\leq\\beta_2\\leq\\cdots\\leq\\beta_p}\\mathbf{SSQ}(y-X\\beta),\n\\] which should give the same solution, and the same loss function value, because it is just another way to fit I-splines. We use the lsi() function from Wang, Lawson, and Hanson (2015).\n\nknots &lt;-\n  extendPartition (innerknots, multiplicities, order, lowend, highend)$knots\nh &lt;- bsplineBasis (x, knots, order)\nnb &lt;- ncol (h)\nd &lt;- matrix(0, nb - 1, nb)\ndiag(d) = -1\nd[outer(1:(nb - 1), 1:nb, function(i, j)\n  (j - i) == 1)] &lt;- 1\nu &lt;- lsi(h, births, e = d, f = rep(0, nb - 1))\nv &lt;- h %*% u\n\n\n\n\n\n\n\n\n\n\nThe residual sum of squares is 144.2027491, indeed the same as before.\n\n\n11.1.3.4 B-Splines with monotone values\nFinally we solve\n\\[\n\\min_{x_1'\\beta\\leq\\cdots\\leq x_n'\\beta} \\mathbf{SSQ}\\ (y-X\\beta)\n\\]\nusing qpmaj() from section ???.\n\nknots &lt;-\n  extendPartition (innerknots, multiplicities, order, lowend, highend)$knots\nh &lt;- bsplineBasis (x, knots, order)\na &lt;- diff(diag(nrow(h))) %*% h\nu &lt;- qpmaj(births, h = h, a = a)\n\n\n\n\n\n\nMonotone Piecewise Quadratic Splines with Multiple Knots\n\n\n\n\nThe residual sum of squares is 144.1574541 , which is indeed smaller than the I-splines value, although only very slightly so.\n\n\n\n11.1.4 Local positivity, monotonicity, convexity\n\n\n\n\nCox, M. G. 1972. “The Numerical Evaluation of B-splines.” Journal of the Institute of Mathematics and Its Applications 10: 134–49.\n\n\nDe Boor, C. 1972. “On Calculating with B-splines. II. Integration.” Journal of Approximation Theory 6: 50–62.\n\n\n———. 1976. “Splines as Linear Combination of B-splines. A Survey.” In Approximation Theory II, edited by G. G. Lorentz, C. K. Chui, and L. L. Schumaker, 1–47. Academic Press.\n\n\n———. 2001. A Practical Guide to Splines. Revised Edition. New York: Springer-Verlag.\n\n\nDe Boor, C., and K. Höllig. 1985. “B-splines without Divided Differences.” Technical Report 622. Department of Computer Science, University of Wisconsin-Madison.\n\n\nDe Boor, C., T. Lyche, and L. L. Schumaker. 1976. “On Calculating with B-splines. II. Integration.” In Numerische Methoden der Approximationstheorie, edited by L. Collatz, G. Meinardus, and H. Werner, 123–46. Basel: Birkhauser.\n\n\nDe Leeuw, J. 2015. “Regression with Linear Inequality Restrictions on Predicted Values.”\n\n\n———. 2017. “Computing and Fitting Monotone Splines.” 2017. https://jansweb.netlify.app/publication/deleeuw-e-17-i/deleeuw-e-17-i.pdf.\n\n\nGaffney, P. W. 1976. “The Calculation of Indefinite Integrals of B-splines.” Journal of the Institute of Mathematics and Its Applications 17: 37–41.\n\n\nRamsay, J. O. 1988. “Monotone Regression Splines in Action.” Statistical Science 3: 425–61.\n\n\nSchumaker, L. 2007. Spline Functions: Basic Theory. Third Edition. Cambridge University Press.\n\n\nWang, Y., C. L. Lawson, and R. J. Hanson. 2015. lsei: Solving Least Squares Problems under Equality/Inequality Constraints.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Splinical MDS</span>"
    ]
  },
  {
    "objectID": "unidimensional.html",
    "href": "unidimensional.html",
    "title": "12  Unidimensional Scaling",
    "section": "",
    "text": "12.1 An example\nWe start the chapter with some pictures, similar to the ones in chapter @ref(propchapter). There are four objects. Dissimilarities are again chosen to be all equal, in this case to \\(\\frac16\\sqrt{6}\\). Weights are all equal to one.\nWe look at stress on the two-dimensional subspace spanned by the two vectors \\(y=(0,-1,+1,0)\\) and \\(z=(-1.5,-.5.,+.5,+1.5)\\). First we normalize both \\(y\\) and \\(z\\) by \\(\\rho=\\eta^2\\). This gives \\(y=(0,-\\frac18\\sqrt{6},+\\frac18\\sqrt{6},0)\\) and \\(z=(-\\frac18\\sqrt{6},-\\frac{1}{24}\\sqrt{6},+\\frac{1}{24}\\sqrt{6},+\\frac18\\sqrt{6})\\). We know from previous results (for example, De Leeuw and Stoop (1984)) that the equally spaced configuration \\(z\\) is the global minimizer of stress over \\(\\mathbb{R}^4\\). Of course it is far from unique, because all 24 permutations of \\(z\\) have the same function value, and are consequently also global minima. In fact, there are 24 local minima, which are all global minima as well. (paired)\nIn the example, we do not minimize over all of \\(\\mathbb{R}^4\\), but only over the subspace of linear combinations of \\(y\\) and \\(z\\). These linear combinations, with coefficients \\(\\alpha\\) and \\(\\beta\\), are given by\n\\[\\begin{equation}\nx=\\alpha y+\\beta z=\\frac{1}{24}\\sqrt{6}\\begin{bmatrix}\\hfill-3\\beta\\\\\\hfill-3\\alpha-\\beta\\\\\n\\hfill3\\alpha+\\beta\\\\\\hfill3\\beta\\end{bmatrix},\n(\\#eq:umdslincom)\n\\end{equation}\\]\nwith distances\n\\[\\begin{equation}\nD(x)=\\frac{1}{24}\\sqrt{6}\\begin{bmatrix}0&&&\\\\\n|3\\alpha-2\\beta|&0&&\\\\\n|3\\alpha+4\\beta|&|6\\alpha+2\\beta|&0&\\\\\n|6\\beta|&|3\\alpha+4\\beta|&|3\\alpha-2\\beta|&0\n\\end{bmatrix}.\n(\\#eq:umdsdist)\n\\end{equation}\\]\nWe see that on the line \\(\\beta=\\frac32\\alpha\\) both \\(d_{12}(x)\\) and \\(d_{34}(x)\\) are zero, on \\(\\beta=-3\\alpha\\) we have \\(d_{23}(x)=0\\), on \\(\\beta=0\\) we have \\(d_{14}(x)=0\\), and finally \\(d_{13}(x)=d_{24}(x)=0\\) on \\(\\beta=-\\frac34\\alpha\\). On those lines, through the origin, stress is not differentiable.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "unidimensional.html#an-example",
    "href": "unidimensional.html#an-example",
    "title": "12  Unidimensional Scaling",
    "section": "",
    "text": "12.1.1 Perspective\nWe first make a global perspective plot, with both \\(\\alpha\\) and \\(\\beta\\) in the range \\((-2.0,+2.0)\\).\n\n\n\n\n\n\n\n\n\n\nWhat do we see ? Definitely more ridges and valleys than in the two-dimensional example of chapter @ref(propchapter). In the one-dimensional case there is a ridge wherever two coordinates are equal, and thus one or more distances are zero. It is clear that at the bottom of each of the valleys there sits a local minimum.\n\n\n12.1.2 Contour\nA contour plot gives some additional details. In the plot we have drawn the four lines through the origin where one or more distances are zero (in red), and we have drawn the curve where \\(\\eta^2(x)=\\rho(x)\\) (in blue). Thus all local minima are on the blue line. The intersections of the red and the blue lines are the local minima of stress restricted to the red line. In those points there are both directions of ascent (along the red lines, in both directions) and of descent (into the adjoining valleys, in all directions).\n\n\n\n\n\n\n\n\n\n\n\nWe see once more the importance of the local minimum result from De Leeuw (1984) that we discussed in section @ref(proplocmin). The special relevbance of this result for UMDS was already pointed out by Pliner (1996). At a local minimum all distances are positive, and thus local minima must be in the interior of the eight cones defined by the four zero-distance lines. There are no saddle points, and only a single local maximum at the origin.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "unidimensional.html#order-formulation",
    "href": "unidimensional.html#order-formulation",
    "title": "12  Unidimensional Scaling",
    "section": "12.2 Order Formulation",
    "text": "12.2 Order Formulation\nDefine an isocone as a closed convex cone of isotone vectors, and \\(\\text{int}(K)\\) as its interior. Thus\n\\[\\begin{equation}\nK:=\\{x\\in\\mathbb{R}^n\\mid x_{i_1}\\leq\\cdots\\leq x_{i_n}\\},\n(\\#eq:isoconc)\n\\end{equation}\\]\nand\n\\[\\begin{equation}\n\\text{int}(K)=\\{x\\in\\mathbb{R}^n\\mid x_{i_1}&lt;\\cdots&lt; x_{i_n}\\}.\n(\\#eq:isoconi)\n\\end{equation}\\]\nwhere \\((i_1,\\cdots,i_n)\\) is a permutation of \\((1,\\cdots,n)\\). There are \\(n!\\) such closed isocones, and their union is all of \\(\\mathbb{R}^n\\). Thus \\(\\min_x\\sigma(x)=\\min_{K\\in\\mathcal{K}}\\min_{x\\in K}\\sigma(x)=\\,\\) where \\(\\mathcal{K}\\) are the \\(n!\\) isocones.\nFor UMDS purposes the isocones are paired, because the negative of each configuration has the same distances between the \\(n\\) points, and thus the same stress. Thus each isocone and its negative cone are equivalent for UMDS, and we only have to consider \\((n!)/2\\) distinct orders.\nLet us consider the problem of minimizing \\(\\sigma\\) over a fixed \\(K\\in\\mathcal{K}\\). Now \\[\n\\rho(x)=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\delta_{ij}s_{ij}(x_i-x_j),\n\\] and \\(s_{ij}=\\text{sign}(x_i-x_j)\\) is the sign matrix of \\(x\\).\n\\(S(x)\\) is the sign matrix of \\(x\\in\\mathbb{R}^n\\) if \\(s_{ij}(x)=\\text{sign}(x_i-x_j)\\) for all \\(i\\) and \\(j\\), i.e.\n\\[\\begin{equation}\ns_{ij}(x):=\\begin{cases}+1&\\text{ if }x_i&gt;x_j,\\\\\n-1&\\text{ if }x_i&lt;x_j,\\\\\n\\hfill 0&\\text{ if }x_i=x_j.\n\\end{cases}\n(\\#eq:signdef)\n\\end{equation}\\]\nThe set of all sign matrices is \\(\\mathcal{S}\\).\nSign matrices are hollow and anti-symmetric. A sign matrix \\(S\\) is strict if its only zeroes are on the diagonal, i.e. \\(S=S(P\\iota)\\) for some permutation matrix \\(P\\). The set of strict sign matrices is \\(\\mathcal{S}_+\\). Since there is a 1:1 correspondence between strict sign matrices and permutations, there are \\(n!\\) strict sign matrices. The row sums and column sums of a strict sign matrix are some permutation of the numbers \\(n-2\\iota+1\\).\nFor all \\(x\\in\\text{int}(K)\\) the matrix \\(S\\) is the same strict sign matrix. Now \\[\n\\rho(x)=\\frac12\\sum_{i=1}^n\\sum_{j=1}^nw_{ij}\\delta_{ij}s_{ij}(x_i-x_j)=x't_K,\n\\] where \\(t_K\\) is the vector of row sums of the Hadamard product \\(W\\times\\Delta\\times S\\), or\n\\[\n\\{t_K\\}_i:=\\sum_{j=1}^nw_{ij}\\delta_{ij}s_{ij}.\n\\] Again \\(t_K\\) only depends of \\(K\\), not on \\(x\\) as long as \\(x\\in\\text{int}(K)\\).\nThus on \\(K\\)\n\\[\n\\sigma(x)=1-2x't_K+x'Vx=1+(x-V^{-1}t_K)'V(x-V^{-1}t_K)-t_K'V^{-1}t_K^{\\ }.\n\\]\nIf there are no weights the \\(t_K\\) were first defined using isocones in De Leeuw and Heiser (1977). They point out that minimizing \\((x-V^{-1}t)'V(x-V^{-1}t)\\) over \\(x\\in K\\) is a monotone regression problem (see @ref(mathmonreg)).\nA crucial next step is in De Leeuw (2005), using the basic result in De Leeuw (1984). De Leeuw (2005) does use weights. We know if \\(x\\) is a local minimum then it must be in the interior of the isocone. If \\(V^{-1}t_K\\) is not in interior, then monotone regression will creates ties, and thus \\(x\\) will not be in the interior either. In fact for local minima of UMDS it is necessary and sufficient that \\(V^{-1}t_K\\) is in the interior of \\(K\\). This result, without weights and in somewhat different language, is also in Pliner (1984). Thus we can limit our search to those isocones for which \\(V^{-1}t_K\\in\\text{int}(K)\\). For those isocones, say the set \\(\\mathcal{K}^\\circ\\), the local minimum is at \\(x=V^{-1}t_K\\).\nThus \\[\n\\min_{K\\in\\mathcal{K}}\\min_{x\\in K}\\sigma(x)=1 -\\max_{K\\in\\mathcal{K}^\\circ}\\ t_K'V^{-1}t_K^{\\ }.\n\\] There is also an early short but excellent paper by Defays (1978), which derives basically the same result in a non-geometrical way. Defays does not use weights, so in his paper \\(V^{-1}\\) is \\(n^{-1}I\\).\nIn the two-dimensional subspace of the example some of the \\(n!\\) cones are empty.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "unidimensional.html#permutation-formulation",
    "href": "unidimensional.html#permutation-formulation",
    "title": "12  Unidimensional Scaling",
    "section": "12.3 Permutation Formulation",
    "text": "12.3 Permutation Formulation",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "unidimensional.html#sign-matrix-formulation",
    "href": "unidimensional.html#sign-matrix-formulation",
    "title": "12  Unidimensional Scaling",
    "section": "12.4 Sign Matrix Formulation",
    "text": "12.4 Sign Matrix Formulation\n\\[\\begin{equation}\n\\rho(x)=\\max_{S\\in\\mathcal{S}}\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\delta_{ij}s_{ij}(x_i-x_j),\n(\\#eq:rhosign)\n\\end{equation}\\]\nwith the maximum attained for \\(S=S(x)\\). If we define\n\\[\\begin{equation}\nt_i(y):=\\sum_{j=1}^n w_{ij}\\delta_{ij}s_{ij}(y),\n(\\#eq:tdef)\n\\end{equation}\\]\nthen\n\\[\\begin{equation}\n\\sigma(x)=\\min_y\\{1+(x-V^{-1}t(y))'V(x-V^{-1}t(y))-t(y)'V^{-1}t(y)\\}.\n(\\#eq:unipart)\n\\end{equation}\\]\nThis implies\n\\[\\begin{equation}\n\\min_x\\sigma(x)= 1 - \\max_y\\ t(y)'V^{-1}t(y)\n(\\#eq:unidual)\n\\end{equation}\\]",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "unidimensional.html#unialgorithms",
    "href": "unidimensional.html#unialgorithms",
    "title": "12  Unidimensional Scaling",
    "section": "12.5 Algorithms for UMDS",
    "text": "12.5 Algorithms for UMDS\n\n12.5.1 SMACOF\n\n\n12.5.2 SMACOF (smoothed)\nNow local minimum \\(x_i\\not= x_j\\)\n\\[\n\\min_{x\\in K}\\sigma(x)=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\delta_{ij}-s_{ij}(x))(x_i-x_j))^2\n\\]\nEach isocone has a sign matrix (hollow, antisymmetric)\n\\[\ns_{ij}(x)=\\begin{cases}+1&\\text{ if }x_i&gt;x_j,\\\\\n-1&\\text{ if }x_i&lt;x_j,\\\\\n\\hfill 0&\\text{ if }x_i=x_j.\n\\end{cases}\n\\] \\[\n\\rho(x)=\\sum_{i=1}^n\\sum_{j=1}^nw_{ij}\\delta_{ij}s_{ij}(x)(x_i-x_j)\\geq\n\\sum_{i=1}^n\\sum_{j=1}^nw_{ij}\\delta_{ij}s_{ij}(y)(x_i-x_j)=\n2\\sum_{i=1}^n x_i\\sum_{j=1}^n w_{ij}\\delta_{ij}s_{ij}(y)\n\\]\nNow\n\\[\\begin{equation}\n\\rho(x)=\\sum_{i=1}^n\\sum_{j=1}^nw_{ij}\\delta_{ij}s_{ij}(x)(x_i-x_j),\n\\end{equation}\\]\nand for all \\(y\\in\\mathbb{R}^n\\)\n\\[\\begin{equation}\n\\rho(x)\\geq\n\\sum_{i=1}^n\\sum_{j=1}^nw_{ij}\\delta_{ij}s_{ij}(y)(x_i-x_j)=\n2\\sum_{i=1}^n x_i\\sum_{j=1}^n w_{ij}\\delta_{ij}s_{ij}(y).\n\\end{equation}\\]\nStress is the maximum of a finite number of quadratics.\n\n\n12.5.3 Branch-and-Bound\n\n\n12.5.4 Dynamic Programming\n\n\n12.5.5 Simulated Annealing\n\n\n12.5.6 Penalty Methods\n\n\n\n\nDe Leeuw, J. 1984. “Differentiability of Kruskal’s Stress at a Local Minimum.” Psychometrika 49: 111–13.\n\n\n———. 2005. “Unidimensional Scaling.” In The Encyclopedia of Statistics in Behavioral Science, edited by B. S. Everitt and D. Howell, 4:2095–97. New York, N.Y.: Wiley.\n\n\nDe Leeuw, J., and W. J. Heiser. 1977. “Convergence of Correction Matrix Algorithms for Multidimensional Scaling.” In Geometric Representations of Relational Data, edited by J. C. Lingoes, 735–53. Ann Arbor, Michigan: Mathesis Press.\n\n\nDe Leeuw, J., and I. Stoop. 1984. “Upper Bounds for Kruskal’s Stress.” Psychometrika 49: 391–402.\n\n\nDefays, D. 1978. “A Short Note on a Method of Seriation.” British Journal of Mathematical and Statistical Psychology 31: 49–53.\n\n\nPliner, V. 1984. “A Class of Metric Scaling Models.” Automation and Remote Control 45: 789–94.\n\n\n———. 1996. “Metric Unidimensional Scaling and Global Optimization.” Journal of Classification 13: 3–18.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "full.html",
    "href": "full.html",
    "title": "13  Full-dimensional Scaling",
    "section": "",
    "text": "13.1 Convexity",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Full-dimensional Scaling</span>"
    ]
  },
  {
    "objectID": "full.html#fulloptimal",
    "href": "full.html#fulloptimal",
    "title": "13  Full-dimensional Scaling",
    "section": "13.2 Optimality",
    "text": "13.2 Optimality",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Full-dimensional Scaling</span>"
    ]
  },
  {
    "objectID": "full.html#fulliteration",
    "href": "full.html#fulliteration",
    "title": "13  Full-dimensional Scaling",
    "section": "13.3 Iteration",
    "text": "13.3 Iteration",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Full-dimensional Scaling</span>"
    ]
  },
  {
    "objectID": "full.html#fullcpspace",
    "href": "full.html#fullcpspace",
    "title": "13  Full-dimensional Scaling",
    "section": "13.4 Cross Product Space",
    "text": "13.4 Cross Product Space\nSo far we have formulated the MDS problem in configuration space. Stress is a function of \\(X\\), the \\(n\\times p\\) configuration matrix. We now consider an alternative formulation, where stress is a function of a positive semi-definite \\(C\\) or order \\(n\\). The relevant definitions are \\[\\begin{equation}\n\\sigma(C):=1-2\\rho(C)+\\eta(C),\n\\end{equation}\\] where \\[\\begin{align*}\n\\rho(C)&:=\\mathbf{tr}\\ B(C)C,\\\\\n\\eta(C)&:=\\mathbf{tr}\\ VC,\n\\end{align*}\\] with \\[\\begin{equation*}\nB(C):=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n} \\begin{cases}w_{ij}\\frac{\\delta_{ij}}{d_{ij}(C)}A_{ij}&\\text{ if }d_{ij}(C)&gt;0,\\\\\n0&\\text{ if }d_{ij}(C)=0.\\end{cases}\n\\end{equation*}\\] and \\(d_{ij}^2(C):=\\mathbf{tr}\\ A_{ij}C\\).\nWe call the space of all positive semi-definite \\(n\\times n\\) matrices cross product space. The problem of minimizing \\(\\sigma\\) over \\(n\\times p\\)-dimensional configuration space is equivalent to the problem of minimizing \\(\\sigma\\) over the set of matrices \\(C\\) in \\(n\\times n\\)-dimensional cross product space that have rank less than or equal to \\(p\\). The corresponding solutions are related by the simple relationship \\(C=XX'\\).\n\nStress is convex on cross product space.\n\n\nProof. First, \\(\\eta\\) is linear in \\(C\\). Second, \\[\n\\rho(C)=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n} w_{ij}\\delta_{ij}\\sqrt{\\mathbf{tr}\\ A_{ij}C}.\n\\] This is the weighted sum of square roots of non-negative functions that are linear in \\(C\\), and it is consequently concave. Thus \\(\\sigma\\) is convex.\n\nUnfortunately the subset of cross product space of all matrices with rank less than or equal to \\(p\\) is far from simple (see (datorro_15?)), so computational approaches to MDS prefer to work in configuration space.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Full-dimensional Scaling</span>"
    ]
  },
  {
    "objectID": "full.html#full-dimensional-scaling",
    "href": "full.html#full-dimensional-scaling",
    "title": "13  Full-dimensional Scaling",
    "section": "13.5 Full-dimensional Scaling",
    "text": "13.5 Full-dimensional Scaling\nCross product space, the set of all positive semi-definite matrices, is a closed convex cone \\(\\mathcal{K}\\) in the linear space of all \\(n\\times n\\) symmetric matrices. This has an interesting consequence.\n\nFull-dimensional scaling, i.e. minimizing $\\sigma$ over $\\mathcal{K}$, is a convex programming problem. Thus in FDS all local minima are global. If $w_{ij}\\delta_{ij}&gt;0$ for all $i,j$ then the minimum is unique.\n\nThis result has been around since about 1985. De Leeuw (1993) gives a proof, but the report it appeared in remained unpublished. A published proof is in “Inverse Multidimensional Scaling” (2007). Another treatment of FDS, with a somewhat different emphasis, is in De Leeuw (2014).\nNow, by a familiar theorem (Theorem 31.4 in Rockafellar (1970)), a matrix \\(C\\) minimizes \\(\\sigma\\) over \\(\\mathcal{K}\\) if and only if \\[\\begin{align}\nC&\\in\\mathcal{K},\\\\\nV-B(C)&\\in\\mathcal{K},\\\\\n\\mathbf{tr}\\ C(V-B(C))&=0.\n\\end{align}\\] We give a computational proof of this result for FDS that actually yields a bit more.\n\nFor $\\Delta\\in\\mathcal{K}$ we have\n\\begin{equation}\n\\sigma(C+\\epsilon\\Delta)=\\sigma(C)-2\\epsilon^{\\frac12}\\sum_{\\mathbf{tr}\\ A_iC = 0}w_i\\delta_i\\sqrt{\\mathbf{tr}\\ A_i\\Delta}+\\epsilon\\ \\mathbf{tr}\\ (V-B(C))\\Delta\n+o(\\epsilon).\\label{E:expand}\n\\end{equation}\n\n\nProof. Simple expansion.\n\n\n\nSuppose $C$ is a solution to the problem of minimizing $\\sigma$ over $\\mathcal{K}$. Then\n\n* $\\mathbf{tr}\\ A_{ij}C &gt; 0$ for all $i,j$ for which $w_{ij}\\delta_{ij}&gt;0$.\n* $V-B(C)$ is positive semi-definite.\n* $\\mathbf{tr}\\ C(V-B(C))=0$.\n* If $C$ is positive definite then $V=B(C)$ and $\\sigma(C)=0$.\n\n\n\n\nProof. The \\(\\epsilon^\\frac12\\) term in \\(\\eqref{E:expand}\\) needs to vanish at a local minimum. This proves the first part. It follows that at a local minimum\n\\[\\begin{equation*}\n\\sigma(C+\\epsilon\\Delta)=\\sigma(C)+\n\\epsilon\\ \\mathbf{tr}\\ (V-B(C))\\Delta+o(\\epsilon).\n\\end{equation*}\\]\nIf \\(V-B(C)\\) is not positive semi-definite, then there is a \\(\\Delta\\in\\mathcal{K}\\) such that \\(\\mathbf{tr}\\ (V-B(C))\\Delta &lt; 0\\). Thus \\(C\\) cannot be the minimum, which proves the second part. If we choose \\(\\Delta=C\\) we find\n\\[\\begin{equation*}\n\\sigma((1+\\epsilon)C)=\\sigma(C)+\n\\epsilon\\ \\mathbf{tr}\\ (V-B(C))C+o(\\epsilon).\n\\end{equation*}\\]\nand choosing \\(\\epsilon\\) small and negative shows we must have \\(\\mathbf{tr}\\ (V-B(C))C=0\\) for \\(C\\) to be a minimum. This proves the third part. Finally, if \\(\\sigma\\) has a minimum at \\(C\\), and \\(C\\) is positive definite, then from parts 2 and 3 we have \\(V=B(C)\\). Comparing off-diagonal elements shows \\(\\Delta=D(C)\\), and thus \\(\\sigma(C)=0\\).\n\nIf \\(C\\) is the solution of the FDS problem, then \\(\\mathbf{rank}(C)\\) defines the Gower rank of the dissimilarities. The number of positive eigenvalues of the negative of the doubly-centered matrix of squared dissimilarities, the matrix factored in classical MDS, defines the Torgerson rank of the dissimilarities. The Gower conjecture is that the Gower rank is less than or equal to the Torgerson rank. No proof and no counter examples have been found.\nWe compute the FDS solution using the smacof algorithm \\[\\begin{equation}\nX^{(k+1)}=V^+B(X^{(k)})\n\\end{equation}\\] in the space of all \\(n\\times n\\) configurations, using the identity matrix as a default starting point. Since we work in configuration space, not in crossproduct space, this does not guarantee convergence to the unique FDS solution, but after convergence we can easily check the necessary and sufficient conditions of theorem @ref(thm:rockafellar).\nAs a small example, consider four points with all dissimilarities equal to one, except \\(\\delta_{14}\\) which is equal to three. Clearly the triangle inequality is violated, and thus there certainly is no perfect fit mapping into Euclidean space.\nThe FDS solution turns out to have rank two, thus the Gower rank is two. The singular values of the FDS solution are\n\n\n[1] 0.4508464709 0.2125310645 0.0000001303\n\n\nGower rank two also follows from the eigenvalues of the matrix \\(B(C)\\), which are\n\n\n[1] 1.0000000000 1.0000000000 0.9205543464",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Full-dimensional Scaling</span>"
    ]
  },
  {
    "objectID": "full.html#ekman-example",
    "href": "full.html#ekman-example",
    "title": "13  Full-dimensional Scaling",
    "section": "13.6 Ekman example",
    "text": "13.6 Ekman example\nThe Ekman (1954) color data give similarities between 14 colors.\n\n\n     434  445  465  472  490  504  537  555  584  600  610  628  651\n445 0.86                                                            \n465 0.42 0.50                                                       \n472 0.42 0.44 0.81                                                  \n490 0.18 0.22 0.47 0.54                                             \n504 0.06 0.09 0.17 0.25 0.61                                        \n537 0.07 0.07 0.10 0.10 0.31 0.62                                   \n555 0.04 0.07 0.08 0.09 0.26 0.45 0.73                              \n584 0.02 0.02 0.02 0.02 0.07 0.14 0.22 0.33                         \n600 0.07 0.04 0.01 0.01 0.02 0.08 0.14 0.19 0.58                    \n610 0.09 0.07 0.02 0.00 0.02 0.02 0.05 0.04 0.37 0.74               \n628 0.12 0.11 0.01 0.01 0.01 0.02 0.02 0.03 0.27 0.50 0.76          \n651 0.13 0.13 0.05 0.02 0.02 0.02 0.02 0.02 0.20 0.41 0.62 0.85     \n674 0.16 0.14 0.03 0.04 0.00 0.01 0.00 0.02 0.23 0.28 0.55 0.68 0.76\n\n\nWe use three different transformations of the similarities to dissimilarities. The first is \\(1-x\\), the second \\((1-x)^3\\) and the third \\(\\sqrt[3]{1-x}\\). We need the following iterations to find the FDS solution (up to a change in loss of 1e-15).\n\n\npower =  1.00  itel =    6936  stress =  0.0000875293 \npower =  3.00  itel =     171  stress =  0.0110248119 \npower =  0.33  itel =     423  stress =  0.0000000000 \n\n\nFor the same three solutions we compute singular values of the thirteen-dimensional FDS solution.\n\n\n [1] 0.1797609824 0.1454675297 0.0843865491 0.0777136109 0.0486123551\n [6] 0.0393576522 0.0236290817 0.0162344515 0.0072756171 0.0000031164\n[11] 0.0000000009 0.0000000000 0.0000000000\n\n [1] 0.2159661347 0.1549184093 0.0000000727 0.0000000041 0.0000000000\n [6] 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000\n[11] 0.0000000000 0.0000000000 0.0000000000\n\n [1] 0.1336126813 0.1139019875 0.0880453752 0.0851609618 0.0710424935\n [6] 0.0664988952 0.0561005006 0.0535112029 0.0492295395 0.0479964575\n[11] 0.0468628701 0.0410193579 0.0388896490\n\n\nThus the Gower ranks of the transformed dissimilarities are, repectively, nine (or ten), two, and thirteen. Note that for the second set of dissimilarities, with Gower rank two, the first two principal components of the thirteen-dimensional solution are the global minimizer in two dimensions. To illustrate the Gower rank in yet another way we give the thirteen non-zero eigenvalues of \\(V^+B(X)\\), so that the Gower rank is the number of eigenvalues equal to one. All three solutions satisfy the necessary and sufficient conditions for a global FDS solution.\n\n\n [1] 1.0000000432 1.0000000222 1.0000000012 1.0000000005 1.0000000002\n [6] 1.0000000001 1.0000000000 1.0000000000 0.9999993553 0.9989115116\n[11] 0.9976821885 0.9942484083 0.9825147154\n\n [1] 1.0000000000 1.0000000000 0.9234970864 0.9079012130 0.8629365849\n [6] 0.8526920031 0.8298036209 0.8145561677 0.7932385763 0.7916517225\n[11] 0.7864426781 0.7476794757 0.7282682474\n\n [1] 1.0000000820 1.0000000241 1.0000000047 1.0000000009 1.0000000004\n [6] 1.0000000003 1.0000000001 1.0000000001 1.0000000001 1.0000000000\n[11] 0.9999999999 0.9999999689 0.9999999005\n\n\nWe also plot the first two principal components of the thirteen-dimensional FDS solution. Not surprisingly, they look most circular and regular for the solution with power three, because this actually is the global minimum over two-dimensional solutions. The other configurations still have quite a lot of variation in the remaining dimensions.\n\n\n\n\n\nEkman data, configurations for three powers\n\n\n\n\nFigure @ref{fig:ekmantrans} illustrates that the FDS solution with power 3 is quite different from power 1 and power one \\(1/3\\) Basically the transformations with lower powers result in dissimilarity measures that are very similar to Euclidean distances in a high-dimensional configuration, while power equal to 3 makes the dissimilarties less Euclidean. This follows from metric transform theory, where concave increasing transforms of finite metric spaces tend to be Euclidean. In particular the square root transformation of a finite metric space has the Euclidean four-point property, and there is a \\(c&gt;0\\) such that the metric transform \\(f(t)=ct/(1+ct)\\) makes a finite metric space Euclidean (Maehara (1986)).\n\n\n\n\n\nEkman data, fit plots for three powers\n\n\n\n\n\n\n\n\nDe Leeuw, J. 1993. “Fitting Distances by Least Squares.” Preprint Series 130. Los Angeles, CA: UCLA Department of Statistics. https://jansweb.netlify.app/publication/deleeuw-r-93-c/deleeuw-r-93-c.pdf.\n\n\n———. 2014. “Bounding, and Sometimes Finding, the Global Minimum in Multidimensional Scaling.” UCLA Department of Statistics. https://jansweb.netlify.app/publication/deleeuw-u-14-b/deleeuw-u-14-b.pdf.\n\n\nEkman, G. 1954. “Dimensions of Color Vision.” Journal of Psychology 38: 467–74.\n\n\n“Inverse Multidimensional Scaling.” 2007. Journal of Classification 14: 3–21.\n\n\nMaehara, H. 1986. “Metric Transforms of Finite Spaces and Connected Graphs.” Discrete Mathematics 61: 235–46.\n\n\nRockafellar, R. T. 1970. Convex Analysis. Princeton University Press.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Full-dimensional Scaling</span>"
    ]
  },
  {
    "objectID": "unfolding.html",
    "href": "unfolding.html",
    "title": "14  Unfolding",
    "section": "",
    "text": "14.1 Algebra\nThe missing data in unfolding complicate the MDS problem, in the same way as the singular value decomposition of a rectangular matrix is more complicated than the eigen decomposition of a symmetric matrix.\nThe problem we want to solve in this section is recovering \\(X\\) and \\(Y\\) (up to a translation and rotation) from \\(D(X,Y)\\).\nThe first matrix algebra results in metric unfolding were due to Ross and Cliff (1964). An actual algorithm for the “ignore-errors” case was proposed by Schönemann (1970). Schönemann’s technique was studied in more detail by Gold (1973) and Heiser and De Leeuw (1979).\nThis sectiom discusses a slightly modified version of Schönemann (1970).\nFirst, we compute the Torgerson transform \\(E(X,Y)=-\\frac12 JD^{(2)}(X,Y)J\\) It was observed for the first time by Ross and Cliff (1964) that \\(E(X,Y)=JXY'J\\).\nAssume that \\(E(X,Y)=JXY'J\\) is a full-rank decomposition, and that the rank of \\(E(X,Y)\\) is \\(r\\). Note that there are cases in which the rank of \\(JX\\) or \\(JY\\) is strictly smaller than the rank of \\(X\\) or \\(Y\\). If \\(X\\), for example, has columns \\(x\\) and \\(e-x\\), with \\(x\\) and \\(e\\) linearly independent, then its rank is two, while \\(JX\\) with columns \\(Jx\\) and \\(-Jx\\) has rank one.\nSuppose \\(E(X,Y)=GH'\\) is another full-rank decomposition. Then there exist vectors \\(u\\) and \\(v\\) with \\(r\\) elements and a non-singular \\(T\\) of order \\(r\\) such that \\[\\begin{align}\n\\begin{split}\nX&=GT+eu',\\\\\nY&=HT^{-t}+ev'.\n\\end{split}\n(\\#eq:unfundet)\n\\end{align}\\] We can assume without loss of generality that the centroid of the \\(X\\) configuration is in the origin, so that \\(JX=X\\), and \\(u=0\\) in the first equation of @ref(eq:unfundet).\nWe use the QR decomposition to compute the rank \\(r\\) of \\(E(X,Y)\\), and the factors \\(G\\) and \\(H\\).\nWe now use @ref(eq:unfundet) to show that \\(F=D^{(2)}(X,Y)+2GH'\\) is of the form \\(F=\\gamma+\\alpha e'+e\\beta'\\), with \\(\\gamma=v'v\\) and \\(M=TT'\\).\n\\[\\begin{align}\n\\begin{split}\n\\alpha_i&=g_i'Mg_i-2g_i'Tv,\\\\\n\\beta_j&=h_j'M^{-1}h_j+2h_j'T^{-t}v.\n\\end{split}\n(\\#eq:unfadditive)\n\\end{align}\\]\nIt follows that \\(JF=J\\alpha e'\\) and \\(FJ=e\\beta' J\\). Thus \\(J\\alpha\\) is any column of \\(JF\\) and \\(J\\beta\\) is any row of \\(FJ\\).\nConsider the first equation of @ref(eq:unfadditive). For the time being, we ignore the second one. Suppose \\(M_k\\) is a basis for the space of real symmetric matrices of order \\(p\\) with the \\(\\frac12 p(p+1)\\) elements \\(e_se_t'+e_te_s'\\) for \\(s\\not= t\\) and \\(e_se_s'\\) for the diagonal. Define \\(q_{ik}:=g_i'M_kg_i\\). Then\n\\[\\begin{equation}\nJ\\alpha=J\\begin{bmatrix}Q&-2G\\end{bmatrix}\\begin{bmatrix}\\mu\\\\Tv\\end{bmatrix},\n(\\#eq:unflinear)\n\\end{equation}\\]\nwith \\(\\mu\\) the coordinates of \\(M\\) for the basis \\(M_k\\).\nEquations @ref(eq:unflinear) are \\(n\\) linear equations in the \\(\\frac12 p(p+1)+p=\\frac12 p(p+3)\\) unknowns \\(\\mu\\) and \\(Tv\\). Assume they have a unique solution. Then \\(M=\\sum\\mu_kM_k\\) is PSD, and can be eigen-decomposed as \\(M=K\\Lambda^2 K'\\). Set \\(T=K\\Lambda\\)\nset.seed(12345)\nx &lt;- matrix (rnorm(16), 8, 2)\nx &lt;- apply (x, 2, function (x) x - mean (x))\ny &lt;- matrix (rnorm(10), 5, 2)\na &lt;- rowSums (x ^ 2)\nb &lt;- rowSums (y ^ 2)\nd &lt;- sqrt (outer(a, b, \"+\") - 2 * tcrossprod (x, y))",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Unfolding</span>"
    ]
  },
  {
    "objectID": "unfolding.html#algebra",
    "href": "unfolding.html#algebra",
    "title": "14  Unfolding",
    "section": "",
    "text": "14.1.1 One-dimensional\nThe one-dimensional case is of special interest, because it allows us to construct an single joint metric scale for row objects and column objects from metric dissimilarities. We have to find a solution to \\(\\delta_{ij}=|x_i-y_j|\\), without making assumptions about the order of the projections on the dimension. Compute any solution for \\(Jg\\) and \\(Jh\\) from \\(\\tau(\\Delta^{(2)})=Jgh'J\\). For data with errors we would probably use the SVD. Assume without loss of generality that \\(Jg=g\\). Then the general solution is \\(x=\\tau g\\) and \\(y=\\tau^{-1}h+\\nu e\\) for some real \\(\\tau\\) and \\(\\nu\\).\nNow\n\\[\\begin{equation}\n\\Delta^2=\\tau^2 g^{(2)}e'+\\tau^{-2}e(h_j')^{(2)}+\\nu^2E-2gh'-2\\tau\\nu g_ie'\n(\\#eq:schone)\n\\end{equation}\\]\nare \\(nm\\) equations in the two unknowns \\((\\tau,\\nu)\\). They can be solved by many methods, but we go the Schönemann way. Column-centering gives\n\\[\\begin{equation}\nJ(\\Delta^{(2)}+2g_jh_j)=\\tau^2 Jg^{(2)}-2\\tau\\nu g,\n(\\#eq:schonecol)\n\\end{equation}\\]\nwhile row-centering gives\n\\[\\begin{equation}\n(\\Delta^{(2)}+2g_jh_j)J=\\tau^{-2}e(h^{(2)})'J.\n(\\#eq:schonerow)\n\\end{equation}\\]",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Unfolding</span>"
    ]
  },
  {
    "objectID": "unfolding.html#classical-unfolding",
    "href": "unfolding.html#classical-unfolding",
    "title": "14  Unfolding",
    "section": "14.2 Classical Unfolding",
    "text": "14.2 Classical Unfolding\nMultidimensional unfolding as a data analysis technique was introduced by Coombs (1964).\nbennett-hays hays-bennett bennett\nSMACOF - Heiser and De Leeuw (1979)\nForm of V\nWhat happens to nonzero theorem ? within-set distances can be zero\n\\(\\Delta=\\begin{bmatrix}1&2&3\\\\1&2&3\\end{bmatrix}\\)\n\\(x_1=x_2=0\\) \\(y_1=1,y_2=2,y_3=3\\)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Unfolding</span>"
    ]
  },
  {
    "objectID": "unfolding.html#nonmetric-unfolding",
    "href": "unfolding.html#nonmetric-unfolding",
    "title": "14  Unfolding",
    "section": "14.3 Nonmetric Unfolding",
    "text": "14.3 Nonmetric Unfolding\nrow-conditional busing van deun deleeuw_R_06a\nStress3 – Roskam\n\n14.3.1 Degenerate Solutions\nWhat are they\n\n14.3.1.1 Which Stress\n\\[\n\\sigma(X)=\\sum_{i=1}^n\\sum_{j=1}^n w_{ij}(\\delta_{ij}-d_{ij}(X))^2\n\\] Weak order, plus normalization. Two-point solution.\n\n\n14.3.1.2 l’Hôpital’s Rule\nWe all know that \\(0/0\\) is not defined and should be avoided at all cost. But then again we have \\[\n\\lim_{x\\rightarrow 0}\\frac{\\sin(x)}{x}=\\cos(0)=1,\n\\] and in fact \\(\\sup_x \\frac{sin(x)}{x}=1\\). Or, for that matter, if \\(f\\) is differentiable at \\(x\\) then\n\\[\n\\lim_{\\epsilon\\rightarrow 0}\\frac{f(x+\\epsilon)-f(x)}{\\epsilon}=f'(x)\n\\] If \\(f:\\mathbb{R}\\Rightarrow\\mathbb{R}\\) and \\(g:\\mathbb{R}\\Rightarrow\\mathbb{R}\\) are two functions\n\ndifferentiable in an interval \\(\\mathcal{I}\\), except possibly at \\(c\\in\\mathcal{I}\\),\n\\(g'(x)\\not=0\\) for all \\(x\\in\\mathcal{I}\\),\n\\(\\lim_{x\\rightarrow c}f(x)=\\lim_{x\\rightarrow c}g(x)=0\\) or \\(\\lim_{x\\rightarrow c}f(x)=\\lim_{x\\rightarrow c}g(x)=\\pm\\infty\\), then \\[\n\\lim_{x\\rightarrow c}\\frac{f(x)}{g(x)}=\\lim_{x\\rightarrow c}\\frac{f'(x)}{g'(x)}\n\\]\n\nWe use l’Hôpital’s rule in chapter @ref(chunfolding), section @ref(unfdegenerate) on degeneracies in nonmetric unfolding. We have not explored the multivariate versions of l’Hôpitals rule, discussed for example by Lawlor (2020).\nIllustration.\n\\(\\delta_{12}&gt;\\delta_{13}=\\delta_{23}\\).\n\\(d_{12}(X_\\epsilon)=1\\)\n\\(d_{13}(X_\\epsilon)=d_{23}(X_\\epsilon)=1+\\frac12\\epsilon\\).\nThen\n\\(\\lim_{\\epsilon\\rightarrow 0}D(X_\\epsilon)=\\Delta\\).\neuclidean for \\(\\epsilon\\geq-1\\)\n\n\n14.3.1.3 Penalizing\n\n\n14.3.1.4 Restricting Regression\nBusing\nVan Deun\n\n\n\n\nCoombs, C. H. 1964. A Theory of Data. Wiley.\n\n\nGold, E. Mark. 1973. “Metric Unfolding: Data Requirement for Unique Solution & Clarification of Schönemann’s Algorithm.” Psychometrika 38 (4): 555–69.\n\n\nHeiser, W. J., and J. De Leeuw. 1979. “Metric Multidimensional Unfolding.” Methoden En Data Nieuwsbrief SWS/VVS 4: 26–50.\n\n\nLawlor, G. R. 2020. “l’Hôpital’s Rule for Multivariable Functions.” American Mathematical Monthly 127 (8): 717–25.\n\n\nRoss, J., and N. Cliff. 1964. “A Generalization of the Interpoint Distance Model.” Psychometrika 29 (167-176).\n\n\nSchönemann, P. H. 1970. “On Metric Multidimensional Unfolding.” Psychometrika 35 (3): 349–66.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Unfolding</span>"
    ]
  },
  {
    "objectID": "constrained.html",
    "href": "constrained.html",
    "title": "15  Constrained Multidimensional Scaling",
    "section": "",
    "text": "15.1 Primal-Dual (note: the base partitioning has dual aspects)\nLeast squares\n\\[\n\\sigma_\\lambda(X):=\\sigma(X)+\\lambda\\min_{Y\\in\\Omega}\\eta^2(X-Y)\n\\] \\[\n\\sigma_\\lambda(X):=\\sigma(X)+\\lambda\\min_{\\Delta\\in\\mathcal{D}}\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\delta_{ij}-d_{ij}(X)^2\n\\]",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Constrained Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "constrained.html#baspar",
    "href": "constrained.html#baspar",
    "title": "15  Constrained Multidimensional Scaling",
    "section": "15.2 Basic Partitioning",
    "text": "15.2 Basic Partitioning\nA comprehensive smacof approach to constrained MDS was developed in De Leeuw and Heiser (1980). It is a primal method that does not involve penalty parameters, and it defines the constraints directly on the configuration.\nThe starting point is the majorization partitioning \\[\\begin{equation}\n\\sigma(X)\\leq 1+\\eta^2(X-\\gamma(Y))-\\eta^2(\\gamma(Y)),\n(\\#eq:smacmdsis)\n\\end{equation}\\] with equality, of course, if \\(X=Y\\).\nNote similarity with dual approach\nThe smacof algorithm for constrained MDS has consequently two steps. In the first we compute the Guttman transform of the current configuration, and in the second we find the metric projection of this Guttman transform on the constraint set (in the metric defined by \\(V\\)). Thus, in shorthand,\n\\[\\begin{equation}\nX^{(k+1)}\\in\\mathop{\\text{Argmin}}_{Y\\in\\Omega}\\ \\eta^2 (Y-\\Gamma(X^{(k)})).\n(\\#eq:smacmds)\n\\end{equation}\\]\nTo emphasize we look for a fixed point of the composition of two maps, the Guttman transform and the projection operator \\(\\Pi_\\Omega\\), we can write in even shorter hand\n\\[\nX^{(k+1)}\\in\\Pi_\\Omega(\\Gamma(X^{(k)}))\n\\]\nThe smacof formulation of the CMDS problem is elegant, if I say so myself, but it is not always simple from the computational point of view. The Guttman transform is easy enough to compute, but projecting on \\(\\Omega\\) in the \\(V\\) metric may be complicated, depending on how \\(\\Omega\\) is defined. In this chapter we will discuss a number of examples with varying degrees of difficulty in computing the smacof projection.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Constrained Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "constrained.html#majawa",
    "href": "constrained.html#majawa",
    "title": "15  Constrained Multidimensional Scaling",
    "section": "15.3 Unweigthing",
    "text": "15.3 Unweigthing\nFor some types of constraints, for example the circular and elliptical MDS discussed in section @ref(circmds), unweighted least squares is computationally simpler than weighted least squares. In those cases it generally pays to use majorization to go from a weighted to an unweighted problem (see also Groenen, Giaquinto, and Kiers (2003)). This will tend to increase the number of iterations of smacof, but the computation within each iteration will be considerably faster.\nFrom equation @ref(eq:smacmds), the projection problem in constrained MDS is to minimize the weighted least squares loss function \\(\\phi(X):=\\text{tr}\\ (Z-X)'V(Z-X)\\) over \\(X\\in\\Omega\\). Now suppose \\(\\theta\\) is the largest eigenvalue of \\(V\\), so that \\(V\\lesssim\\theta I\\), and suppose \\(Y\\in\\Omega\\). Then\n\\[\\begin{multline}\n\\phi(X)=\\text{tr}\\ ((Z-Y)-(X-Y))'V((Z-Y)-(X-Y))\\leq\\\\\\phi(Y)-2\\ \\text{tr}\\ (Z-Y)'V(X-Y)+\\theta\\ \\text{tr}\\ (X-Y)'(X-Y).\n(\\#eq:majproj)\n\\end{multline}\\]\nCompleting the square gives the majorization\n\\[\\begin{equation}\n\\phi(X)\\leq\\phi(Y)+\\theta\\ \\text{tr}\\ (X-\\overline{Y})'(X-\\overline{Y})-\\theta\\ \\text{tr}\\ \\overline{Y}'\\overline{Y},\n(\\#eq:compsqproj)\n\\end{equation}\\]\nwith \\(\\overline{Y}\\) the matrix-convex combination\n\\[\\begin{equation}\n\\overline{Y}:=(I-\\frac{1}{\\theta}V)Y+\\frac{1}{\\theta}VZ.\n(\\#eq:projtarget)\n\\end{equation}\\]\nThe weighted projection problem from equation @ref(eq:smacmds) is replaced by one or more inner iterations of an unweighted projection problem. Set \\(X^{(k,1)}=X^{(k)}\\) and\n\\[\\begin{equation}\nX^{(k,l+1)}\\in\\mathop{\\text{Argmin}}_{Y\\in\\Omega}\\ \\text{tr}\\ (Y-\\overline{X}^{k,l})'(Y-\\overline{X}^{(k,l)}).\n(\\#eq:smaprojinner)\n\\end{equation}\\]\nAfter stopping the inner iterations at \\(X^{(k,l+s)}\\) we set \\(X^{(k+1)}=X^{(k,l+s)}\\). All \\(X^{(k,l)}\\) remain feasible, loss decreases in each inner iteration, and as long at the metric projections are continuous the map from \\(X^{(k)}\\) to \\(X^{(k+1)}\\) is continuous as well.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Constrained Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "constrained.html#constraints-on-the-distances",
    "href": "constrained.html#constraints-on-the-distances",
    "title": "15  Constrained Multidimensional Scaling",
    "section": "15.4 Constraints on the Distances",
    "text": "15.4 Constraints on the Distances\n\n15.4.1 Rectangles",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Constrained Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "constrained.html#lincons",
    "href": "constrained.html#lincons",
    "title": "15  Constrained Multidimensional Scaling",
    "section": "15.5 Linear Constraints",
    "text": "15.5 Linear Constraints\n\n15.5.1 Uniqueness\n\\(X=(Z\\mid D)\\)\n\\(X=(Z\\mid \\alpha I)\\)\nDistance smoothing\n\n\n15.5.2 Combinations\n\\(X=\\sum\\alpha_r Z_r\\)\n\n\n15.5.3 Step Size\n\\(X=Z+\\alpha G\\)\n\n\n15.5.4 Single Design Matrix\n\\(X=ZU\\)\n\n\n15.5.5 Multiple Design Matrices\n\\(x_s=G_su_s\\)\n\\[\nd_{ij}^2(X)=\\sum_{s=1}^p x_s'A_{ij}x_s=\n\\sum_{s=1}^p u_s'G_s'A_{ij}G_su_s\n\\]",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Constrained Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "constrained.html#circmds",
    "href": "constrained.html#circmds",
    "title": "15  Constrained Multidimensional Scaling",
    "section": "15.6 Circular MDS",
    "text": "15.6 Circular MDS\nDe Leeuw (2007b), De Leeuw (2007a), De Leeuw (2005)\nThere are situations in which it is desirable to have a configuration with points that are restricted to lie on some surface or manifold in \\(\\mathbb{R}^p\\). Simple examples are the circle in \\(\\mathbb{R}^2\\) or the sphere in \\(\\mathbb{R}^3\\). Some applications are discussed in Cox and Cox (1991) (also see Cox and Cox (2001), section 4.6), in Borg and Lingoes (1980), in Papazoglou and Mylonas (2017), and in De Leeuw and Mair (2009), section 5. The most prominent actual examples are probably the color circle and the spherical surface of the earth, but there are many other cases in which MDS solutions show some sort of “horseshoe” (De Leeuw (2007a)).\n\n15.6.1 Some History\n\n\n\n\n\nJohn van de Geer\n\n\n\n\nPermit me to insert some personal history here. Around 1965 I got to work at the Psychological Institute. At the time Experimental Psychology and Methodology were in the same department, with John van de Geer as its chair. John had a long-running project with Pim Levelt and Reinier Plomp at the Institute for Perception RVO/TNO on perceptual and cognitive aspects of musical intervals. In Van de Geer, Levelt, and Plomp (1962), for example, they used various cutting-edge techniques at the time, the semantic differential for data collection, the centroid method for factor analysis, and oblique simple structure rotation. A couple of years later the cutting edge had moved to triadic comparisons for data collection and nonmetric multidimensional scaling (Levelt, Van De Geer, and Plomp (1966)). The analysis in Levelt, Van De Geer, and Plomp (1966) revealed a parabolic horseshoe structure of the musical intervals.\nThis inspired John to find a technique to fit quadratic (and higher order, if necessary) structures to scatterplots. If \\(X\\) is a two-dimensional configuration of \\(n\\) points, then form the \\(n\\times 6\\) matrix \\(Z\\) with columns \\(1,x_1,x_2,x_i^2,x_2^2,x_1x_2\\). Now find \\(\\alpha\\) with \\(\\alpha'\\alpha=1\\) such that \\(\\alpha'Z'Z\\alpha\\) is as small as possible. This gives the normalized eigenvector corresponding with the smallest eigenvalue of \\(Z'Z\\), or, equivalently, the right singular vector corresponding with the smallest singular value of \\(Z\\). It is easy to see how this approach generalizes to more dimensions and higher order algebraic surfaces. I remember with how much awe this technique was received by the staff of the Psychological Institute. It probably motivated me in 1966 to develop similar techniques and get my portion of awe.\nLevelt, Van De Geer, and Plomp (1966) used the curve fitting technique to draw the best fitting parabola in the two-dimensional scatterplot of musical intervals. In their discussion they suggested that a similar quadratic structure could be found if similarities between political parties were analyzed, because for people in the middle of the left-right scale extreme-left and extreme-right parties would tend to be similar. If the effect of extremity was strong enough, the two extreme might even bend towards each other, leading to an ellipse rather than a parabola. In 1966 John asked student-researcher Dato de Gruijter to figure out if this curving back actually happened, which lead to De Gruijter (1967). Dato collected triadic comparisons between nine Dutch political parties, cumulated over 100 psychology students. The curve fitting technique indeed found best fitting ellipses.\n\n\n15.6.2 Primal Methods\nWe follow De Leeuw and Mair (2009) in distinguishing primal and dual methods. In a primal method the surface we fit is specified in parametric form. The points on the circle, for example, have \\((x_{i1},x_{i2})=(sin(\\xi_i),cos(\\xi_i))\\). Rotational invariance of MDS means we can assume the center of the circle is in the origin. This is the approach of Cox and Cox (1991). They substitute the parametrix expression for the circle in the formula for stress and minimize over the sperical coordinates \\(\\xi_i\\) using gradient methods. They develop a similar method for the sphere in \\(\\mathbb{R}^3\\). For those who want to go to higher dimensions we illustrate a parametric representation for \\(\\mathbb{R}^4\\). \\[\n(x_{i1},x_{i2},x_{i3},x_{i4})=\\\\(\\sin(\\xi_i)\\cos(\\theta_i)\\sin(\\mu_i),\\\n\\sin(\\xi_i)\\cos(\\theta_i)\\cos(\\mu_i),\\ \\sin(\\xi_i)\\sin(\\theta_i),\\ \\cos(\\xi_i)).\n\\] Spherical coordinates soon get tedious, and De Leeuw and Mair (2009) simply require the distances of all points to the origin to be the same constant. Note that this puts the center of the fitted sphere in the origin, which means that in general the center of the point cloud cannot be taken to be in the origin as well. In smacof we use the\n\\[\n\\phi(X,\\lambda):=\\text{tr}\\ (Z-\\lambda X)'V(Z-\\lambda X)\n\\] over the radius \\(\\lambda\\) and the configuration \\(X\\), which is constrained to have \\(\\text{diag}\\ XX'=I\\). De Leeuw and Mair (2009) project out \\(\\lambda\\) and minimize \\(\\phi(X,\\star):=\\min_\\lambda\\phi(X,\\lambda)\\) over \\(X\\), using Dinkelbach majorization (Dinkelbach (1967)), a block relaxation that cycles over rows of \\(X\\). Solving for each \\(p\\) vector of coordinates requires solving a secular equation.\nHere we proceed slightly differently. Our first step is to get rid of \\(V\\) using the formulas in @ref(majawa).\n\n\n\n\n\n\n\n\n\nAfter 2 iterations the primal method converges to a stress value of 0.4654367. The circle has radius 0.0122974.\n\n\n15.6.3 Dual Methods\nIn a dual method we use unrestricted smacof, but we add a penalty to the loss if the configurations do not satisfy the constraints. We use a quadratic penalty, mainly because that fits seamlessly into the smacof approach.\nWe add one point, the center of the circle, with coordinates \\(x_0\\) to the configuration, and we require that all \\(n\\) other points have an equal distance from the center. The \\(n\\) dissimilarities \\(\\delta_{0,i}\\) are unknown, so we use alternating least squares and estimate the missing dissimilarities by minimizing stress over them, requiring them to be all equal. All weights \\(w_{0,i}\\) are chosen equal to the penalty parameter \\(\\omega\\). The solution for the common \\(\\delta_{0,i}\\) is obviously the average of the \\(n\\) distances \\(d_{0,i}\\). In this case it is not necessary to use majorization to transform to unweighted least squares.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Constrained Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "constrained.html#ellimcds",
    "href": "constrained.html#ellimcds",
    "title": "15  Constrained Multidimensional Scaling",
    "section": "15.7 Elliptical MDS",
    "text": "15.7 Elliptical MDS\n\n15.7.1 Primal\nThe smacof projection problem for a \\(p\\)-axial ellipsoid minimizes \\[\n\\phi(Y,\\Lambda):=\\text{tr}\\ (Z-Y\\Lambda)'V(Z-Y\\Lambda)\n\\] with \\(\\text{diag}\\ YY'=I\\) and with \\(\\Lambda\\) diagonal and PSD.\nALS\nMinimizing \\(\\phi\\) over \\(\\Lambda\\) for fixed \\(Y\\) is easy. For dimension \\(s\\) we have\n\\[\n\\lambda_s=\\frac{y_s'Vz_s}{y_s'Vy_s}.\n\\]\nTo minimize \\(\\phi\\) over \\(Y\\) for fixed \\(\\Lambda\\) we use \\(Z-Y\\Lambda=(Z\\Lambda^{-1}-Y)\\Lambda\\) so that\n\\[\n\\phi(Y,\\Lambda)=\\text{tr}\\ \\Lambda^2(\\tilde Z-Y)'V(\\tilde Z-Y)\n\\] with \\(\\tilde Z=Z\\Lambda^{-1}\\). We now use a slight modification of the majorization technique in section @ref(majawa). Set \\(Y=Y_{\\text{old}}+(Y-Y_{\\text{old}})\\). Then \\[\n\\phi(Y,\\Lambda)=\\text{tr}\\ \\Lambda^2((\\tilde Z-Y_{\\text{old}})-(Y-Y_{\\text{old}}))'V((\\tilde Z-Y_{\\text{old}})-(Y-Y_{\\text{old}}))=\\\\\n\\phi(Y_{\\text{old}},\\Lambda)-2\\ \\text{tr}\\ \\Lambda^2(\\tilde Z-Y_{\\text{old}})'V(Y-Y_{\\text{old}})+\\text{tr}\\ \\Lambda^2(Y-Y_{\\text{old}})'V(Y-Y_{\\text{old}})\n\\]\n\\[\n\\text{tr}\\ \\Lambda^2(Y-Y_{\\text{old}})'V(Y-Y_{\\text{old}})\\leq\n\\theta\\lambda_{\\text{max}}^2\\ \\text{tr}\\ (Y-Y_{\\text{old}})'(Y-Y_{\\text{old}})\n\\]\nwhere, as before, \\(\\theta\\) is the largest eigenvalue of \\(V\\).\n\\[\n\\theta\\lambda_{\\text{max}}^2\\ (Y-Y_{\\text{old}})=V(\\tilde Z - Y_{\\text{old}})\\Lambda^2\n\\] (abadir_magnus_05, p 283)\nNormalize the rows of\n\\[\nY_{\\text{old}}+\\frac{1}{\\theta\\lambda_{\\text{max}}^2}V(Z\\Lambda^{-1}-Y_{\\text{old}})\\Lambda^2\n\\]\n\n\n\n\n\n\n\n\n\n0.0053277, 0.0095667\n2\n0.415335\n\n\n15.7.2 Dual\nWe will only develop a dual method for ellipses in two dimensions, because there is no easy characterization in terms of distances in higher dimensions (that I know of). But in two dimensions the famous pin-and-string construction uses the fact that for all points on the ellipse the sum of the distances to two focal points is constant. Thus our dual method now adds two points to the \\(n\\) points in the configuration, chooses the weights for the \\(2n\\) components of stress to be the penalty parameter w, and finds the \\(2n\\) unknown dissimilarities between the two focal points and the \\(n\\) points on the ellipse to add up to a constant.\nThis means we have to minimize \\(\\text{tr}\\ (\\Delta-D)'(\\Delta-D)\\) over \\(\\Delta\\) satisfying \\(\\Delta e=\\gamma e\\), where for the time being \\(\\Delta\\) and \\(D\\) are \\(n\\times 2\\) submatrices. The Lagrangian is \\(\\text{tr}\\ (\\Delta-D)'(\\Delta-D)-2\\mu'(\\Delta e-\\gamma e)\\), and thus we must have \\(\\Delta=D+\\mu e'\\). Taking row sums gives \\(\\gamma e = De+p\\mu\\) and thus \\(\\mu = \\frac12(\\gamma e-De)\\). This implies \\(\\Delta-D=\\mu e'=\\frac{1}{2}(\\gamma E_{np}-DE_{pp}),\\) and to minimize loss over \\(\\gamma\\) we choose \\(\\gamma=\\frac{1}{n}e_n'De_p\\). This gives \\[\n\\Delta=DJ+\\frac{e'De}{2n}ee'.\n\\] Thus we take \\(D\\), transform its \\(n\\) rows to deviations from the mean, and then add the overall mean to all elements.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhyperbola: difference of distances constant \\(|d((x_i,x_2),(f_1,0))-d((x_i,x_2),(g_1,0))|=c\\)\nparabola: equal distance from the focus point and the directrix (horizontal axis) \\(d((x_i,x_2),(f_1,f_2))=d((x_1,x_2),d(x_1,0))\\)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Constrained Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "constrained.html#distance-bounds",
    "href": "constrained.html#distance-bounds",
    "title": "15  Constrained Multidimensional Scaling",
    "section": "15.8 Distance Bounds",
    "text": "15.8 Distance Bounds\nDe Leeuw (2017c) De Leeuw (2017b) De Leeuw (2017a)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Constrained Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "constrained.html#localized-mds",
    "href": "constrained.html#localized-mds",
    "title": "15  Constrained Multidimensional Scaling",
    "section": "15.9 Localized MDS",
    "text": "15.9 Localized MDS",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Constrained Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "constrained.html#mds-as-mva",
    "href": "constrained.html#mds-as-mva",
    "title": "15  Constrained Multidimensional Scaling",
    "section": "15.10 MDS as MVA",
    "text": "15.10 MDS as MVA\nQ methodology\nhttp://qmethod.org\nStephenson (1953)\nDe Leeuw and Meulman (1986) Meulman (1986) Meulman (1992)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Constrained Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "constrained.html#conshorseshoes",
    "href": "constrained.html#conshorseshoes",
    "title": "15  Constrained Multidimensional Scaling",
    "section": "15.11 Horseshoes",
    "text": "15.11 Horseshoes\n\n\n\n\nBorg, I., and J. C. Lingoes. 1980. “A Model and Algorithm for Multidimensional Scaling with External Constraints on the Distances.” Psychometrika 45 (1): 25–38.\n\n\nCox, T. F., and M. A. A. Cox. 1991. “Multidimensional Scaling on a Sphere.” Communications in Statistics 20: 2943–53.\n\n\n———. 2001. Multidimensional Scaling. Second Edition. Monographs on Statistics and Applied Probability 88. Chapman & Hall.\n\n\nDe Gruijter, D. N. M. 1967. “The Cognitive Structure of Dutch Political Parties in 1966.” Report E019-67. Psychological Institute, University of Leiden.\n\n\nDe Leeuw, J. 2005. “Fitting Ellipsoids by Least Squares.” UCLA Department of Statistics. 2005. https://jansweb.netlify.app/publication/deleeuw-u-05-j/deleeuw-u-05-j.pdf.\n\n\n———. 2007a. “A Horseshoe for Multidimensional Scaling.” Preprint Series 530. Los Angeles, CA: UCLA Department of Statistics.\n\n\n———. 2007b. “Quadratic Surface Embedding.” UCLA Department of Statistics. 2007. https://jansweb.netlify.app/publication/deleeuw-u-07-h/deleeuw-u-07-h.pdf.\n\n\n———. 2017a. “Multidimensional Scaling with Distance Bounds.” 2017.\n\n\n———. 2017b. “Multidimensional Scaling with Lower Bounds.” 2017.\n\n\n———. 2017c. “Multidimensional Scaling with Upper Bounds.” 2017.\n\n\nDe Leeuw, J., and W. J. Heiser. 1980. “Multidimensional Scaling with Restrictions on the Configuration.” In Multivariate Analysis, Volume V, edited by P. R. Krishnaiah, 501–22. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\nDe Leeuw, J., and P. Mair. 2009. “Multidimensional Scaling Using Majorization: SMACOF in R.” Journal of Statistical Software 31 (3): 1–30. https://www.jstatsoft.org/article/view/v031i03.\n\n\nDe Leeuw, J., and J. J. Meulman. 1986. “Principal Component Analysis and Restricted Multidimensional Scaling.” In Classification as a Tool of Research, edited by W. Gaul and M. Schader, 83–96. Amsterdam, London, New York, Tokyo: North-Holland.\n\n\nDinkelbach, W. 1967. “On Nonlinear Fractional Programming.” Management Science 13: 492–98.\n\n\nGroenen, P. J. F., P. Giaquinto, and H. A. L Kiers. 2003. “Weighted Majorization Algorithms for Weighted Least Squares Decomposition Models.” Econometric Institute Report EI 2003-09. Econometric Institute, Erasmus University Rotterdam. https://repub.eur.nl/pub/1700.\n\n\nLevelt, W. J. M., J. P. Van De Geer, and R. Plomp. 1966. “Triadic Comparions of Musical Intervals.” British Journal of Mathematical and Statistical Psychology 19: 163–79.\n\n\nMeulman, J. J. 1986. “A Distance Approach to Nonlinear Multivariate Analysis.” PhD thesis, Leiden University.\n\n\n———. 1992. “The Integration of Multidimensional Scaling and Multivariate Analysis with Optimal Transformations.” Psychometrika 57 (4): 539–65.\n\n\nPapazoglou, S., and K. Mylonas. 2017. “An Examination of Alternative Multidimensional Scaling Techniques.” Educational and Psychological Measurement 77 (3): 429–48.\n\n\nStephenson, W. 1953. The Study of Behavior: Q-technique and its Methodology. University of Chicago Press.\n\n\nVan de Geer, J. P., W. J. M. Levelt, and R. Plomp. 1962. “The Connotation of Musical Consonance.” Acta Psychologica 20: 308–19.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Constrained Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "individual.html",
    "href": "individual.html",
    "title": "16  Individual Differences",
    "section": "",
    "text": "16.1 Stress Majorization\nThe CS majorizaton in this case is the same as our treatment in chapter @ref(cmds). For all \\((X_1,\\cdots,X_m)\\) and \\((Y_1,\\cdots,Y_m)\\) we ahve \\[\\begin{equation}\n\\sigma_k(X_k)\\leq\\frac12\\sum_{k=1}^m\\eta_k^2(X_k-\\Gamma(Y_k))-\\frac12\\sum_{k=1}^m\\eta^2_k(\\Gamma(Y_k)),\n(\\#eq:inddiffmaj)\n\\end{equation}\\] and thus the smacof approach tells us to minimize, or at least decrease, the first term on the right of @ref(eq:inddiffmaj) over the \\(X_k\\).\nAs in CMDS, within an iteration we have to (approximately) solve a projection problem on some linear or nonlinear manifold, in the metric defined by the weights. Minimize over X, project \\(X\\), conforming with chapter @ref(cmds).",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Individual Differences</span>"
    ]
  },
  {
    "objectID": "individual.html#types-of-constraints",
    "href": "individual.html#types-of-constraints",
    "title": "16  Individual Differences",
    "section": "16.2 Types of Constraints",
    "text": "16.2 Types of Constraints\n\n16.2.1 Unconstrained\nIf there are no constraints on the configurations of stress the minimization over \\(X_1,\\cdots,X_m\\) simply means solving \\(m\\) separate MDS problems, one for each \\(k\\). Thus it does not bring anything new.\n\n\n16.2.2 Replications\nThe first constraint that comes to mind is \\(X_k=X\\) for all \\(k=1,\\cdots,m\\). Thus the configuration is the same for all slices, and there really are no individual differences in this case. The computational aspects are discussed in sufficient detail in section @ref(minrepl).\n\n\n16.2.3 INDSCAL/PARAFAC\n\\[\nx_{ijk}=\\sum_{s=1}^Sa_{is}b_{js}\\lambda_{ks}\n\\]\n\\[X_k=A\\Lambda_kB'\\]\n\n\n16.2.4 IDIOSCAL/TUCKALS\n\\[\nx_{ijk}=\\sum_{s=1}^S\\sum_{t=1}^T\\sum_{u=1}^Ua_{is}b_{jt}c_{ku}d_{stu}\n\\] \\[\nX_k=AV_kB'\n\\] \\[\nV_k=\\sum_{u=1}^Uc_{ku}D_u\n\\]\n\n\n16.2.5 PARAFAC2\n\\[\nx_{ijk}=\\sum_{s=1}^Sa_{is}b_{kjs}\\lambda_{ks}\n\\]\n\\[X_k=K\\Lambda_k^{\\ }L_k'\\]\n\n\n16.2.6 Factor Models\nQ and R techniques\n\\[X_k=\\begin{bmatrix}X&\\mid&Y_k\\end{bmatrix}\\]\n\\[X_k=\\begin{bmatrix}K&\\mid&L\\end{bmatrix}\\begin{bmatrix}A_k\\\\--\\\\Y_k\\end{bmatrix},\\]\n\\[X_k'=\\begin{bmatrix}K&\\mid&L\\end{bmatrix}\\begin{bmatrix}A_k\\\\--\\\\Y_k\\end{bmatrix}\\]",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Individual Differences</span>"
    ]
  },
  {
    "objectID": "individual.html#nonmetric-individual-differences",
    "href": "individual.html#nonmetric-individual-differences",
    "title": "16  Individual Differences",
    "section": "16.3 Nonmetric Individual Differences",
    "text": "16.3 Nonmetric Individual Differences\n\n16.3.1 Conditionality",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Individual Differences</span>"
    ]
  },
  {
    "objectID": "asymmetry.html",
    "href": "asymmetry.html",
    "title": "17  Asymmetry in MDS",
    "section": "",
    "text": "17.1 Conditional Rankings\nTwo sets (= unfolding), one set (much tighter) solution Young_75",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Asymmetry in MDS</span>"
    ]
  },
  {
    "objectID": "asymmetry.html#confusion-matrices",
    "href": "asymmetry.html#confusion-matrices",
    "title": "17  Asymmetry in MDS",
    "section": "17.2 Confusion Matrices",
    "text": "17.2 Confusion Matrices\nChoice theory",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Asymmetry in MDS</span>"
    ]
  },
  {
    "objectID": "asymmetry.html#the-slide-vector",
    "href": "asymmetry.html#the-slide-vector",
    "title": "17  Asymmetry in MDS",
    "section": "17.3 The Slide Vector",
    "text": "17.3 The Slide Vector",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Asymmetry in MDS</span>"
    ]
  },
  {
    "objectID": "asymmetry.html#dedicom",
    "href": "asymmetry.html#dedicom",
    "title": "17  Asymmetry in MDS",
    "section": "17.4 DEDICOM",
    "text": "17.4 DEDICOM",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Asymmetry in MDS</span>"
    ]
  },
  {
    "objectID": "asymmetry.html#constantine-gower",
    "href": "asymmetry.html#constantine-gower",
    "title": "17  Asymmetry in MDS",
    "section": "17.5 Constantine-Gower",
    "text": "17.5 Constantine-Gower",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Asymmetry in MDS</span>"
    ]
  },
  {
    "objectID": "nominal.html",
    "href": "nominal.html",
    "title": "18  Nominal MDS",
    "section": "",
    "text": "18.1 Binary Dissimilarities\nSuppose we have and equivalence relation \\(\\simeq\\) on \\(n\\) objects \\(\\mathcal{O}\\). Let \\(d_{ij}=0\\) if \\(i\\simeq j\\) and \\(d_{ij}=1\\) if \\(i\\not\\simeq j\\). Suppose the quotient set \\(\\mathcal{O}/\\mathord{\\simeq}\\) has \\(K\\) equivalence classes. Thus the objects can be ordered in such a way that \\(\\Delta\\) has \\(K\\) zero matrices with size \\(n_1\\cdots, n_K\\) in the diagonal blocks and ones everywhere else. The \\(n_k\\) are the sizes of the equivalence classes, and they add up on \\(n\\).\n\\([x]\\) canonical projection",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Nominal MDS</span>"
    ]
  },
  {
    "objectID": "nominal.html#indicator-matrix",
    "href": "nominal.html#indicator-matrix",
    "title": "18  Nominal MDS",
    "section": "18.2 Indicator matrix",
    "text": "18.2 Indicator matrix\nQ technique\n\n\n\nExample Indicator Matrix\n\n\n\nA\nB\nC\n\n\n\n\n1\n1\n0\n0\n\n\n2\n1\n0\n0\n\n\n3\n1\n0\n0\n\n\n4\n1\n0\n0\n\n\n5\n1\n0\n0\n\n\n6\n1\n0\n0\n\n\n7\n1\n0\n0\n\n\n8\n1\n0\n0\n\n\n9\n1\n0\n0\n\n\n10\n1\n0\n0\n\n\n11\n0\n1\n0\n\n\n12\n0\n1\n0\n\n\n13\n0\n1\n0\n\n\n14\n0\n1\n0\n\n\n15\n0\n1\n0\n\n\n16\n0\n0\n1\n\n\n17\n0\n0\n1\n\n\n18\n0\n0\n1\n\n\n19\n0\n0\n1\n\n\n20\n0\n0\n1\n\n\n\n\n\n   1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20\n1   0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1\n2   0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1\n3   0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1\n4   0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1\n5   0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1\n6   0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1\n7   0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1\n8   0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1\n9   0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1\n10  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1\n11  1  1  1  1  1  1  1  1  1  1  0  0  0  0  0  1  1  1  1  1\n12  1  1  1  1  1  1  1  1  1  1  0  0  0  0  0  1  1  1  1  1\n13  1  1  1  1  1  1  1  1  1  1  0  0  0  0  0  1  1  1  1  1\n14  1  1  1  1  1  1  1  1  1  1  0  0  0  0  0  1  1  1  1  1\n15  1  1  1  1  1  1  1  1  1  1  0  0  0  0  0  1  1  1  1  1\n16  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0  0  0  0  0\n17  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0  0  0  0  0\n18  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0  0  0  0  0\n19  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0  0  0  0  0\n20  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0  0  0  0  0\n\n\n\n\n\n\n\nExample Object Scores",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Nominal MDS</span>"
    ]
  },
  {
    "objectID": "nominal.html#unfolding-indicator-matrices",
    "href": "nominal.html#unfolding-indicator-matrices",
    "title": "18  Nominal MDS",
    "section": "18.3 Unfolding Indicator Matrices",
    "text": "18.3 Unfolding Indicator Matrices\nall within category distances smaller than all between category distances (for each variable separately) as in MCA: joint persons and categories, primary approach to ties\nThese order constraints are rather strict. They do not only imply that the category clouds are separated, they also mean the clouds must be small and rather far apart.\nThink of the situation where the two categories are balls in \\(\\mathbb{R}^p\\) with centers \\(x\\) and \\(y\\) and $radius \\(r\\). The largest within-category distance is \\(2r\\). The smallest between-category distance is \\(\\max(0,d(x,y)-2r)\\). Thus all within-category distances are all smaller than all between-category distances if and only if \\(d(x,y)\\geq 4r\\).\n\n\n\n\n\nDistance Based MCA\n\n\n\n\n\n\n\n\n\nWithin and Between Distances\n\n\n\n\n\n\n\n\n\nWithin and Between Distances\n\n\n\n\n\n\n\n\n\nWithin and Between Distances\n\n\n\n\nWe can make the requirements less strict by\nFor all \\(k\\) \\[\n\\max_{i\\in I_k\\ j\\in I_k}d(x_i,x_j)\\leq\\min_{i\\in I_k\\ j\\in I\\backslash I_k}d(x_i,x_j).\n\\] \\[\n\\max(kk)\\leq\\min(k\\overline{k})\n\\] \\[\n\\max_k\\max(kk)\\leq\\min_{i\\not= j}\\min(ij)\n\\] the within category distances of category 1 are less than the smallest between-category distance",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Nominal MDS</span>"
    ]
  },
  {
    "objectID": "nominal.html#linear-separation",
    "href": "nominal.html#linear-separation",
    "title": "18  Nominal MDS",
    "section": "18.4 Linear Separation",
    "text": "18.4 Linear Separation\nline perpendicular to line connecting category points separates categories\nall closer to their star center than to other star centers: primary monotone regression over all rows of g\nsuppose the star centers are \\(y\\) and \\(z\\). The plane is \\((x-\\frac12(y+z))'(y-z)=0\\) If \\(u\\) is in the \\(y\\) category we must have \\((u-\\frac12(y+z))'(y-z)\\geq 0\\)\nJust in terms of distances\n\n\n\n\n\nDistance Based MCA\n\n\n\n\n\n\n\n\n\nDistance Based MCA\n\n\n\n\n\n\n\n\n\nDistance Based MCA\n\n\n\n\nWhat if the star centers are on a straight line",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Nominal MDS</span>"
    ]
  },
  {
    "objectID": "nominal.html#circular-separation",
    "href": "nominal.html#circular-separation",
    "title": "18  Nominal MDS",
    "section": "18.5 Circular Separation",
    "text": "18.5 Circular Separation\nmonotone regression on each column of g\nk within balls must be disjoint =&gt; they can be separated by straight lines",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Nominal MDS</span>"
    ]
  },
  {
    "objectID": "nominal.html#convex-hull-scaling",
    "href": "nominal.html#convex-hull-scaling",
    "title": "18  Nominal MDS",
    "section": "18.6 Convex Hull Scaling",
    "text": "18.6 Convex Hull Scaling",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Nominal MDS</span>"
    ]
  },
  {
    "objectID": "nominal.html#voronoi-scaling",
    "href": "nominal.html#voronoi-scaling",
    "title": "18  Nominal MDS",
    "section": "18.7 Voronoi Scaling",
    "text": "18.7 Voronoi Scaling",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Nominal MDS</span>"
    ]
  },
  {
    "objectID": "nominal.html#multidimensional-scalogram-analysis",
    "href": "nominal.html#multidimensional-scalogram-analysis",
    "title": "18  Nominal MDS",
    "section": "18.8 Multidimensional Scalogram Analysis",
    "text": "18.8 Multidimensional Scalogram Analysis\nGuttman’s MSA: Inner points, outer points\nSuitably mysterious\nLingoes (1968b) Lingoes (1968a) Guttman (1967)\n\n\n\n\nDe Leeuw, J. 1968. “Canonical Discriminant Analysis of Relational Data.” Research Note 007-68. Department of Data Theory FSW/RUL.\n\n\n———. 1969. “Some Contributions to the Analysis of Categorical Data.” Research Note 004-69. Leiden, The Netherlands: Department of Data Theory FSW/RUL.\n\n\n———. 1973. “Canonical Analysis of Categorical Data.” PhD thesis, University of Leiden, The Netherlands. https://jansweb.netlify.app/publication/deleeuw-b-73/deleeuw-b-73.pdf.\n\n\nGifi, A. 1990. Nonlinear Multivariate Analysis. New York, N.Y.: Wiley.\n\n\nGuttman, L. 1941. “The Quantification of a Class of Attributes: A Theory and Method of Scale Construction.” In The Prediction of Personal Adjustment, edited by P. Horst, 321–48. New York: Social Science Research Council.\n\n\n———. 1967. “The Development of Nonmetric Space Analysis: A Letter to Professor John Ross.” Multivariate Behavioral Research 2 (1): 71–82.\n\n\nLingoes, J. C. 1968a. “The Multivariate Analysis Of Qualitative Data.” Multivariate Behavioral Research 3 (1): 61–94.\n\n\n———. 1968b. “The Rationale of the Guttman-Lingoes Nonmetric Series: A Letter to Doctor Philip Runkel.” Multivariate Behavioral Research 3 (4): 495–507.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Nominal MDS</span>"
    ]
  },
  {
    "objectID": "nonordinal.html",
    "href": "nonordinal.html",
    "title": "19  Nonmonotonic MDS",
    "section": "",
    "text": "19.1 Filler\nThis chapter will discuss techniques in which the relation between dissimilarities and distances is not necessarily monotone (in as far as these techniques fit into the smacof framework). I have in mind the mapping of high-dimensional manifolds into low-dimensional Euclidean ones, in the spirit of Shepard and Carroll (1966). The prime examples are still cutting and unrolling the circle, the sphere, or the torus.\nThe dissimilarities define the high-dimensional space, the distances the low-dimensional space. The relation between distances and dissimilarities may not be functional, i.e. we can have \\(F(D(X),\\Delta)=0\\) or \\(\\Delta\\) could be a function of \\(D(X)\\).\nIn general, small distances in low-dimensional space are small distances in high-dimensional space, but large distances in low-dimensional space can be small distances in high-dimensional space. This suggests an inverse Shepard plot, with dissimilarity as a function of distance. Or \\[\n\\sigma(X)=\\sum\\sum(\\```{r echo = FALSE}\nsource(\"loadMe.R\")\n```\ndelta_{ij}-f(d_{ij}(X)))^2\n\\]\nEuclidean\n\\[\n\\sqrt{2-2\\ \\cos |i-j|\\theta}\n\\]\nCircular\n\\[\n|i-j|\\frac{2\\pi}{n}\n\\] Linear \\[\n|i-j|\n\\] ::: {.cell layout-align=“center”} ::: {.cell-output-display}  ::: :::\nQuadratic\n\\(F(\\Delta,D(X))=0\\)\n\\[\na_1\\delta_{ij}^2+a_2\\delta_{ij} d_{ij}(X)+a_3d_{ij}^2(X)+a_4\\delta_{ij}+a_5 d_{ij}(X)+a_6=0\n\\] \\[\npd_{ij}^2(X)+2qd_{ij}(X)+r=p(d_{ij}(X)-q/p)^2+r-(q/p)^2\n\\] \\(f(\\Delta)=g(D(X))\\)",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Nonmonotonic MDS</span>"
    ]
  },
  {
    "objectID": "nonordinal.html#filler",
    "href": "nonordinal.html#filler",
    "title": "19  Nonmonotonic MDS",
    "section": "",
    "text": "Shepard, R. N., and J. D. Carroll. 1966. “Parametric Representation of Nonlinear Data Structures.” In Mulktivariate Analysis, edited by P. R. Krishnaiah, 561–92. Academic Press.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Nonmonotonic MDS</span>"
    ]
  },
  {
    "objectID": "nonpairs.html",
    "href": "nonpairs.html",
    "title": "20  Compound Objects",
    "section": "",
    "text": "20.1 Filler\nA compound object is a set of \\(m&gt;1\\) objects. This chapter treats the analysis of dissimilarities between compound objects, again as far as they fit into the smacof framework. Thus we do not, for example, look at the pairwise dissimilarities within a triad of objects, but at the dissimilarities of the triads themselves.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Compound Objects</span>"
    ]
  },
  {
    "objectID": "sstress.html",
    "href": "sstress.html",
    "title": "21  sstress",
    "section": "",
    "text": "21.0.1 sstress and stress\nClearly configurations with small stress will tend to have small sstress, and vice versa.\nweighting residuals – fitting large dissimilarities\n\\[\n\\sum d_{ij}^2(\\delta_{ij}^2-d_{ij}^2)=0\n\\]\nWe can write sstress as\n\\[\\begin{equation}\n\\sigma_2(X)=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\delta_{ij}+d_{ij}(X))^2(\\delta_{ij}-d_{ij}(X))^2.\n\\end{equation}\\]\nThus if the \\(d_{ij}(X)\\) provide a good fit to the \\(\\delta_{ij}\\) we have the approximation\n\\[\\begin{equation}\n\\sigma_2(X)\\approx 4\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq  n}w_{ij}^{\\ }\\delta_{ij}^2(\\delta_{ij}-d_{ij}(X))^2\n(\\#eq:sstressapp)\n\\end{equation}\\]\nSince \\(\\mathcal{D}f(\\delta)=2\\delta\\) in this case, @ref(eq:sstressapp) also follows from @ref(eq:wfstress).\nNow\n\\[\\begin{equation}\n\\frac{\\sigma_2(X)}{\\sigma(X)}=\\frac{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\delta_{ij}+d_{ij}(X))^2(\\delta_{ij}-d_{ij}(X))^2}{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}^{\\ }(\\delta_{ij}-d_{ij}(X))^2}.\n\\end{equation}\\]\nThis is a weighted average of the quantities \\((\\delta_{ij}+d_{ij}(X))^2\\), and thus\n\\[\\begin{equation}\n4\\{\\min(\\delta_{ij}+d_{ij}(X))\\}^2\\sigma(X)\\leq\\sigma_2(X)\\leq 4\\{\\max(\\delta_{ij}+d_{ij}(X))\\}^2\\sigma(X).\n\\end{equation}\\]",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>sstress</span>"
    ]
  },
  {
    "objectID": "sstress.html#strain",
    "href": "sstress.html#strain",
    "title": "21  sstress",
    "section": "21.1 strain",
    "text": "21.1 strain\nClassical scaling as formulated by Torgerson (1958) computes the dominant non-negative eigenvalues, with corresponding eigenvectors, of the Torgerson transform of the squared dissimilarities. This is usually presented as an “ignore-errors” technique. It is clear what it does in the case of perfect fit of distances to dissimilarities, it is not so obvious how it measures approximation errors in the case of imperfect fit. Or, to put it differently, MDS lore has it that in classical scaling loss is defined on the scalar products, which are a transformation of the dissimilarity data, and not on the dissimilarities themselves. This is presented as somehow being a disadvantage (see, for example, Takane, Young, and De Leeuw (1977), Browne (1987)).\nIf we agree to use weights, then strain is defined straightforwardly as\n\\[\\begin{equation}\n\\sigma_\\tau(X):=\\sum_{i=1}^n\\sum_{j=1}^nw_{ij}(\\tau_{ij}(\\Delta^{(2)})-x_i'x_j^{\\ })^2.\n(\\#eq:straindef)\n\\end{equation}\\]\nNote that this summation includes the diagonal elements, so in general we cannot expect \\(W\\) to be hollow. In fact, @(eq:straindef) adds diagonal elements only once, while the off-diagonal elements are added twice. This observation is the basis of the excellent paper by Bailey and Gower (1990).\nBecause of the weights, minimizing strain in this form does not lead to an eigenvalue problem, unless there is a non-negative vector \\(u\\) such that \\(w_{ij}=u_iu_j\\). In that case \\[\\begin{equation}\n\\sigma_\\tau(X):=\\text{tr}\\ (U\\tau(\\Delta^{(2)})U-UXX'U)^2,\n(\\#eq:strainuu)\n\\end{equation}\\]\nwhere \\(U=\\text{diag}(u)\\), and we can find \\(UX\\), and thus \\(X\\), by eigen decomposition of \\(U\\tau_{ij}(\\Delta^{(2)})U\\).\nWe can use our general results on unweighting, as in section @ref(minunweight), to get rid of the weights, but this leads to a sequence of eigenvalue problems. Nevertheless, if the weights are important, this is an option.\n\n21.1.1 Unweighted\nFor column-centered configurations \\[\nJ(\\Delta^{(2)}-D^{(2)}(X))J=-2(\\tau(\\Delta^{(2)})-XX')\n\\] Thus if all weights are equal to one then \\[\n4\\sigma_\\tau(X)=\\text{tr}\\ J(\\Delta^{(2)}-D^{(2)}(X))J(\\Delta^{(2)}-D^{(2)}(X)),\n\\] which shows that strain is a matrix-weighted version of sstress. It also shows (De Leeuw and Heiser (1982), theorem 21) that \\(\\sigma_\\tau(X)\\leq\\frac14\\sigma_2(X).\\)\n\n\n21.1.2 Bailey-Gower\n\\[\n\\min_{C\\gtrsim 0}\\sigma(C):=\\sum_{i=1}^n\\sum_{j=1}^nw_{ij}(s_{ij}-c_{ij})^2\n\\] If \\(E\\) is psd then \\[\n\\sum_{i=1}^n\\sum_{j=1}^nw_{ij}((s_{ij}-c_{ij})-\\alpha e_{ij})^2=\\\\\\sigma(C)-2\\alpha\\sum_{i=1}^n\\sum_{j=1}^nw_{ij}(s_{ij}-c_{ij})e_{ij}+\\alpha^2\\sum_{i=1}^n\\sum_{j=1}^nw_{ij}e_{ij}^2\\geq\\sigma(C)\n\\] \\[\\begin{align}\n&\\sum_{i=1}^n\\sum_{j=1}^nw_{ij}(c_{ij}-s_{ij})e_{ij}\\geq 0,\\\\\n&\\sum_{i=1}^n\\sum_{j=1}^nw_{ij}(c_{ij}-s_{ij})c_{ij}=0.\n\\end{align}\\] ### Using Additivity\n\\[\n\\sigma_\\tau(X)=\\min_{\\alpha}\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\delta_{ij}^2-(\\alpha_i+\\alpha_j)-2x_i'x_j^{\\ }))^2\n\\]\n\\[\n\\sigma_2(X)=\\min_{\\alpha\\geq 0}\\min_{\\text{diag}XX'=I}\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\delta_{ij}^2-(\\alpha_i^2+\\alpha_j^2-2\\alpha_i\\alpha_j\\ x_i'x_j^{\\ }))^2\n\\]\nprojection\nIf there are no weights, or if we unweight the weighted loss function\n\n\n\n\nBailey, R. A., and J. C. Gower. 1990. “Approximating a Symmetric Matrix.” Psychometrika 55 (4): 665–75.\n\n\nBrowne, M. W. 1987. “The Young-Householder Algorithm and the Least Squares Multidimensional Scaling of Squared Distances.” Journal of Classification 4: 175–90.\n\n\nDe Leeuw, J. 1968. “Nonmetric Multidimensional Scaling.” Research Note 010-68. Department of Data Theory FSW/RUL. https://jansweb.netlify.app/publication/deleeuw-r-68-9/deleeuw-r-68-g.pdf.\n\n\n———. 1975. “An Alternating Least Squares Approach to Squared Distance Scaling.” Department of Data Theory FSW/RUL.\n\n\nDe Leeuw, J., and B. Bettonvil. 1986. “An Upper Bound for SSTRESS.” Psychometrika 51: 149–53.\n\n\nDe Leeuw, J., P. Groenen, and R. Pietersz. 2016. “An Alternating Least Squares Approach to Squared Distance Scaling.” 2016. https://jansweb.netlify.app/publication/deleeuw-groenen-pietersz-e-16-m/deleeuw-groenen-pietersz-e-16-m.pdf.\n\n\nDe Leeuw, J., and W. J. Heiser. 1982. “Theory of Multidimensional Scaling.” In Handbook of Statistics, Volume II, edited by P. R. Krishnaiah and L. Kanal. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\nGreenacre, M. J., and M. W. Browne. 1986. “An Efficient Alternating Least Squares Algorithm to Perform Multidimensional Unfolding.” Psychometrika 51 (2): 241–50.\n\n\nRoss, J., and N. Cliff. 1964. “A Generalization of the Interpoint Distance Model.” Psychometrika 29 (167-176).\n\n\nTakane, Y., F. W. Young, and J. De Leeuw. 1977. “Nonmetric Individual Differences in Multidimensional Scaling: An Alternating Least Squares Method with Optimal Scaling Features.” Psychometrika 42: 7–67.\n\n\nTorgerson, W. S. 1958. Theory and Methods of Scaling. New York: Wiley.\n\n\nYoung, F. W., Y. Takane, and R. Lewyckyj. 1978a. “ALSCAL: A Nonmetric Multidimensional Scaling Program with Several Individual-Differences Options.” Behavior Research Methods & Instrumentation 10 (3): 451–53.\n\n\n———. 1978b. “Three Notes on ALSCAL.” Psychometrika 43 (3): 433–35.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>sstress</span>"
    ]
  },
  {
    "objectID": "rstress.html",
    "href": "rstress.html",
    "title": "22  fstress and rstress",
    "section": "",
    "text": "22.1 fstress\nFstress is a straightforward generalization of stress. Suppose \\(f\\) is any non-decreasing real-valued function, and define\n\\[\\begin{equation}\n\\sigma_f(X):=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(f(\\delta_{ij})-f(d_{ij}(X)))^2\n(\\#eq:fstress)\n\\end{equation}\\]\nWe discuss various specific examples in this chapter, such as the square and the logarithm, but let’s first mention some general results.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>fstress and rstress</span>"
    ]
  },
  {
    "objectID": "rstress.html#fstress",
    "href": "rstress.html#fstress",
    "title": "22  fstress and rstress",
    "section": "",
    "text": "22.1.1 Use of Weights\nSuppose the \\(d_{ij}(X)\\) are close to the \\(\\delta_{ij}\\), so that we have a good fit and a low stress. Then the approximation\n\\[\\begin{equation}\nf(d_{ij}(X))\\approx f(\\delta_{ij})+\\mathcal{D}f(\\delta_{ij})(d_{ij}(X)-\\delta_{ij})\n(\\#eq:wfapprox)\n\\end{equation}\\]\nwill be close. Thus\n\\[\\begin{equation}\n\\sigma_f(X)\\approx\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\mathcal{D}f(\\delta_{ij}))^2(\\delta_{ij}-d_{ij}(X))^2.\n(\\#eq:wfstress)\n\\end{equation}\\]\nThus we can approximately minimize fstress by minimizing stress with weights \\(w_{ij}(\\mathcal{D}f(\\delta_{ij}))^2\\). If the fit is good, we can expect to be close. If the fit is perfect, the approximation is perfect too. Note that we do not assume that \\(f\\) is increasing, i.e. that \\(f'\\geq 0\\).\n\n\n22.1.2 Convexity\n\\[\nf(g(\\lambda x + (1-\\lambda)y))\\leq f(\\lambda g(x)+(1-\\lambda)g(y))\\leq\\lambda f(g(x))+(1-\\lambda)f(g(y))\n\\] Thus if \\(g\\) is convex (for instance distance) and \\(f\\) is convex and increasing then \\(f\\circ g\\) is convex (and thus stress is DC). Unfortunately a concave \\(f\\) is more interesting.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>fstress and rstress</span>"
    ]
  },
  {
    "objectID": "rstress.html#rstress",
    "href": "rstress.html#rstress",
    "title": "22  fstress and rstress",
    "section": "22.2 rStress",
    "text": "22.2 rStress\n\\[\\begin{equation}\n\\sigma_r(X):=\\mathop{\\sum\\sum}_{1\\leq j&lt;i\\leq n}w_{ij}(\\delta_{ij}^r-d_{ij}^r(X))^2.\n(\\#eq:rstress)\n\\end{equation}\\]\nIn definition @ref(eq:rstress) we approximate the r-th power of the dissimilarities by the r-th power of the distances. Alternatively, we could have defined\n\\[\\begin{equation}\n\\sigma_r(X):=\\mathop{\\sum\\sum}_{1\\leq j&lt;i\\leq n}w_{ij}(\\delta_{ij}-d_{ij}^{r}(X))^2\n(\\#eq:rstressalt)\n\\end{equation}\\]\nIn definition @ref(eq:rstress) we are still approximating the dissimilarities by the distances, as in basic MDS, but we are defining errors of approximation as the differences between the r-th powers. In definition @ref(eq:rstressalt)\nI am not sure which of the two formulations is the more natural one. I am sure, however, that for basic MDS the two formulations are effectively the same, because we can just define dissimilarities in @ref(eq:rstressalt) as the r-th power of the ones in @ref(eq:rstress). And in ordinal MDS the two formulations are the same as well, because the rank orders of the unpowered and powered dissimilarities are the same.\n\n22.2.1 Using Weights\n\\[\nd_{ij}^{2r}(X)-\\delta_{ij}^{2r}=\n(d_{ij}^{r}(X)+\\delta_{ij}^{r})(d_{ij}^{r}(X)-\\delta_{ij}^{r})\n\\]\nIf \\(\\delta_{ij}\\approx d_{ij}(X)\\) then \\[\n(d_{ij}^{2r}(X)-\\delta_{ij}^{2r})^2\\approx 4\\delta_{ij}^{2r}(d_{ij}^{r}(X)-\\delta_{ij}^{r})^2\n\\] If \\(r\\) is a power of 2 ==&gt;\n\n\n22.2.2 Minimizing rstress\nrstress, qstress, power stress\nGroenen and De Leeuw (2010) De Leeuw (2014) De Leeuw, Groenen, and Mair (2016b) De Leeuw, Groenen, and Mair (2016c) De Leeuw, Groenen, and Mair (2016a)",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>fstress and rstress</span>"
    ]
  },
  {
    "objectID": "rstress.html#mstress",
    "href": "rstress.html#mstress",
    "title": "22  fstress and rstress",
    "section": "22.3 mstress",
    "text": "22.3 mstress\nThe loss function used by by Ramsay (1977) in his MULTISCAL program for MDS can be written as\n\\[\\begin{equation}\n\\sigma_0(X):=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\log\\delta_{ij}-\\log(d_{ij}(X)))^2.\n(\\#eq:mstress)\n\\end{equation}\\]\nTo justify the notation \\(\\sigma_0\\) we define \\(f_r\\), for all \\(x&gt;0\\) and \\(r&lt;1\\), by \\(f_r(x):=r^{-1}\\frac{x^r-1}{r}\\). \\(f_r\\) is concave for all \\(r\\), it majorizes the log because \\(f_r(x)\\geq\\log(x)\\) for all \\(x\\), with equality iff \\(x=1\\), and\n\\[\\begin{equation}\n\\lim_{r\\rightarrow 0}\\frac{x^r-1}{r}=\\log x.\n(\\#eq:loglim)\n\\end{equation}\\] We have drawn the logarithm, in red, and \\(f_r\\) for \\(r\\) equal to 0.001, 0.01, 0.1, 0.5, 1 over the interval \\([.01,5]\\) in the figure that follows.\n\n\n\n\n\n\n\n\n\nFor \\(r=.001\\) and \\(r=.01\\) the logarithm and \\(f_r\\) are practically indistinguishable, and even for \\(r=.1\\) we have an approximation which is probably good enough (in the given range) for most practical purposes.\nUsing the approximation of the logarithm with small \\(r\\) gives\n\\[\\begin{align}\n\\begin{split}\n\\sigma_0(X)&\\approx\n\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\delta_{ij}-\\frac{d_{ij}^r(X)-1}{r})^2\\\\\n&=r^{-2}\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq w_{ij}}((r\\delta_{ij}+1)-d_{ij}^r(X))^2.\n\\end{split}\n(\\#eq:strapp)\n\\end{align}\\]\nIf \\(r\\) is really small both \\(r\\delta_{ij}+1\\) and \\(d_{ij}^r(X)\\) will be very close to one, which will make minimization of the approximation difficult. A simple suggestion is to start with the SMACOF solution for \\(r=1\\), then use that solution for \\(r=1\\) as a starting point for \\(r=\\frac12\\), and so on.\nAlternative ?\n\\[\n\\log d_{ij}(X)-\\log \\delta_{ij}\\leq \\frac{\\{\\frac{d_{ij}(X)}{\\delta_{ij}}\\}^r-1}{r}=\\frac{d_{ij}^r(X)-\\delta_{ij}^r}{r\\delta{ij}^r}\n\\] \\[\n\\log d_{ij}(X)-\\log d_{ij}(Y)\\leq \\frac{\\{\\frac{d_{ij}(X)}{d_{ij}(Y)}\\}^r-1}{r}=\\frac{d_{ij}^r(X)-d_{ij}^r(Y)}{rd_{ij}^r(Y)}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>fstress and rstress</span>"
    ]
  },
  {
    "objectID": "rstress.html#astress",
    "href": "rstress.html#astress",
    "title": "22  fstress and rstress",
    "section": "22.4 astress",
    "text": "22.4 astress\nrobust MDS (zho@u_xu_li_19) LAR (Heiser (1988))\n\\[\n\\sigma_{11}(X)=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}|\\delta_{ij}-d_{ij}(X)|\n\\] \\[\n|(\\delta_{ij}-d_{ij}(X))+\\epsilon|\\leq\\frac12\\frac{((\\delta_{ij}-d_{ij}(X))+\\epsilon)^2+((\\delta_{ij}-d_{ij}(Y))+\\epsilon)^2}{|(\\delta_{ij}-d_{ij}(Y))+\\epsilon|}\n\\] \\[\n\\sigma_{rs}(X)=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}|\\delta_{ij}^r-d_{ij}^r(X)|^s\n\\]",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>fstress and rstress</span>"
    ]
  },
  {
    "objectID": "rstress.html#pstress",
    "href": "rstress.html#pstress",
    "title": "22  fstress and rstress",
    "section": "22.5 pstress",
    "text": "22.5 pstress\nThe p in pstress stands for panic. We define\n\\[\n\\sigma(X):=\\mathop{\\sum\\sum\\sum\\sum}_{(i&lt;j)\\leq(k&lt;l)} w_{ijkl}(\\delta_{ij}-d_{ij}(X))(\\delta_{kl}-d_{kl}(X)),\n\\] where \\((i&lt;j)\\leq(k&lt;l)\\) means that index pair \\((i,j)\\) is lexicographically not larger than pair \\((k,l)\\).\nCovariances/variances\nHow many of these weights \\(w_{ijkl}\\) are there ?\n\\[\n\\frac12\\binom{n}{2}(\\binom{n}{2}+1)=\\frac18 n(n-1)(n^2-n+2)\n\\]\nNo reason to panic. Again, majorization comes to the rescue (Groenen, Giaquinto, and Kiers (2003)). Suppose there is a \\(K&gt;0\\) and a hollow, symmetric, non-negative \\(\\Omega\\) such that \\[\n\\mathop{\\sum\\sum\\sum\\sum}_{(i&lt;j)\\leq(k&lt;l)} w_{ijkl}z_{ij}z_{kl}\\leq K\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}\\omega_{ij}z_{ij}^2.\n\\] Often the form of the weights \\(w_{ijkl}\\) will suggest how to choose \\(K\\). In the worst case scenario we choose \\(\\Omega=E-I\\) and compute \\(K\\) by the power method. If all \\(w_{ijkl}\\) are equal to one, then ref becomes \\[\n\\frac12\\left\\{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}z_{ij}\\right\\}^2+\n\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}z_{ij}^2\\leq K\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}\\omega_{ij}z_{ij}^2.\n\\]\n\\[\n\\sigma(X)\\leq\\sigma(Y)+2\\sum \\theta_{ij}(d_{ij}(Y)-d_{ij}(X))+K\\sum \\omega_{ij}(d_{ij}(Y)-d_{ij}(X))^2\n\\]\nwith (modify slightly !!)\n\\[\n\\theta_{ij}=\\mathop{\\sum\\sum}_{1\\leq k&lt;l\\leq n}w_{ijkl}(\\delta_{kl}-d_kl{Y})\n\\]\n\\[\n\\Phi(\\Delta,D(X))\n\\]\n\n\n\n\nDe Leeuw, J. 2014. “Minimizing rStress Using Nested Majorization.” UCLA Department of Statistics. https://jansweb.netlify.app/publication/deleeuw-u-14-c/deleeuw-u-14-c.pdf.\n\n\nDe Leeuw, J., P. Groenen, and P. Mair. 2016a. “Minimizing qStress for Small q.” 2016. https://jansweb.netlify.app/publication/deleeuw-groenen-mair-e-16-h/deleeuw-groenen-mair-e-16-h.pdf.\n\n\n———. 2016b. “Minimizing rStress Using Majorization.” 2016. https://jansweb.netlify.app/publication/deleeuw-groenen-mair-e-16-a/deleeuw-groenen-mair-e-16-a.pdf.\n\n\n———. 2016c. “Second Derivatives of rStress, with Applications.” 2016. https://jansweb.netlify.app/publication/deleeuw-groenen-mair-e-16-c/deleeuw-groenen-mair-e-16-c.pdf.\n\n\nGroenen, P. J. F., and J. De Leeuw. 2010. “Power-Stress for Multidimensional Scaling.” https://jansweb.netlify.app/publication/groenen-deleeuw-u-10/groenen-deleeuw-u-10.pdf.\n\n\nGroenen, P. J. F., P. Giaquinto, and H. A. L Kiers. 2003. “Weighted Majorization Algorithms for Weighted Least Squares Decomposition Models.” Econometric Institute Report EI 2003-09. Econometric Institute, Erasmus University Rotterdam. https://repub.eur.nl/pub/1700.\n\n\nHeiser, W. J. 1988. “Multidimensional Scaling with Least Absolute Residuals.” In Classification and Related Methods of Data Analysis, edited by H. H. Bock, 455–62. North-Holland Publishing Co.\n\n\nRamsay, J. O. 1977. “Maximum Likelihood Estimation in Multidimensional Scaling.” Psychometrika 42: 241–66.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>fstress and rstress</span>"
    ]
  },
  {
    "objectID": "altls.html",
    "href": "altls.html",
    "title": "23  Alternative Least Squares Loss",
    "section": "",
    "text": "23.1 Sammon’s MDS",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Alternative Least Squares Loss</span>"
    ]
  },
  {
    "objectID": "altls.html#kamade-kawai-spring",
    "href": "altls.html#kamade-kawai-spring",
    "title": "23  Alternative Least Squares Loss",
    "section": "23.2 Kamade-Kawai Spring",
    "text": "23.2 Kamade-Kawai Spring",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Alternative Least Squares Loss</span>"
    ]
  },
  {
    "objectID": "altls.html#mcgees-work",
    "href": "altls.html#mcgees-work",
    "title": "23  Alternative Least Squares Loss",
    "section": "23.3 McGee’s Work",
    "text": "23.3 McGee’s Work",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Alternative Least Squares Loss</span>"
    ]
  },
  {
    "objectID": "altls.html#shepards-nonmetric-mds",
    "href": "altls.html#shepards-nonmetric-mds",
    "title": "23  Alternative Least Squares Loss",
    "section": "23.4 Shepard’s Nonmetric MDS",
    "text": "23.4 Shepard’s Nonmetric MDS",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Alternative Least Squares Loss</span>"
    ]
  },
  {
    "objectID": "altls.html#guttmans-nonmetric-mds",
    "href": "altls.html#guttmans-nonmetric-mds",
    "title": "23  Alternative Least Squares Loss",
    "section": "23.5 Guttman’s Nonmetric MDS",
    "text": "23.5 Guttman’s Nonmetric MDS",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Alternative Least Squares Loss</span>"
    ]
  },
  {
    "objectID": "altls.html#positive-orthant-nonmetric-mds",
    "href": "altls.html#positive-orthant-nonmetric-mds",
    "title": "23  Alternative Least Squares Loss",
    "section": "23.6 Positive Orthant Nonmetric MDS",
    "text": "23.6 Positive Orthant Nonmetric MDS\n!! Richard Johnson 1973\n!! Guttman Absolute Value\n!! Hartmann",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Alternative Least Squares Loss</span>"
    ]
  },
  {
    "objectID": "altls.html#interrole",
    "href": "altls.html#interrole",
    "title": "23  Alternative Least Squares Loss",
    "section": "23.7 Role Reversal",
    "text": "23.7 Role Reversal\nKruskal, arithmetic with dissimilarities\n\\[\n\\sigma(X)=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\delta_{ij}-P_r(d_{ij}(X)))^2\n\\]",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Alternative Least Squares Loss</span>"
    ]
  },
  {
    "objectID": "inverse.html",
    "href": "inverse.html",
    "title": "24  Inverse Multidimensional Scaling",
    "section": "",
    "text": "24.1 Basic IMDS\nSuppose \\(X\\) is a regular and normalized configuration satisfying the stationary equation \\((V-B(X))X=0\\). Our first IMDS step is to describe the set \\(\\mathfrak{D}(X)\\) of all \\(\\Delta\\)E for which \\(X\\) is stationary.\nThus \\(\\mathfrak{D}(X)\\) is a non-empty affine space, a translation of a linear subspace, closed under all linear combinations with coefficients that add up to one. Since \\(S\\) is symmetric of order \\(n-p-1\\), equation @ref(eq:invalldelta) defines an affine subspace of dimension \\(\\frac12(n-p)(n-p-1)\\). If \\(n=3\\) and \\(p=1\\), or if \\(n=4\\) and \\(p=2\\), the dimension is one. If \\(n=4\\) and \\(p=1\\) the dimension is three.\nIf \\(\\Delta_1,\\cdots,\\Delta_m\\) are in \\(\\mathfrak{D}(X)\\), then so is the affine subspace spanned by the \\(\\Delta_j\\). For all configurations \\(X\\) we have \\(D(X)\\in\\mathfrak{D}(X)\\). Specifically, if we compute a solution to the stationary equations \\(X\\) with some MDS algorithm such as smacof, then the whole line through the data \\(\\Delta\\) and \\(D(X)\\) is in \\(\\mathfrak{D}(X)\\).\nThe next result is corollary 6.3 in “Inverse Multidimensional Scaling” (2007).\n**`r corollary_nums(\"full_result\", display = \"f\")`** If $p=n-1$ then $\\Delta\\in\\mathfrak{D}(X)$ if and only if $\\Delta=D(X)$ if and only if $\\sigma(X)=0$.\nFor any two elements of \\(\\mathfrak{D}(X)\\), one cannot be elementwise larger (or smaller) than the other. This is corollary 3.3 in “Inverse Multidimensional Scaling” (2007).\nIf $\\Delta_1$ and $\\Delta_2$ are both in $\\mathfrak{DS}(X)$ and $\\Delta_1\\leq\\Delta_2$, then $\\Delta_1=\\Delta_2$.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Inverse Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "inverse.html#basic-imds",
    "href": "inverse.html#basic-imds",
    "title": "24  Inverse Multidimensional Scaling",
    "section": "",
    "text": "Suppose \\(X\\) is an \\(n\\times p\\) matrix of rank \\(p&lt;n\\). Suppose \\(K\\) is an \\(n\\times(n-p)\\) orthonormal matrix with \\(K'X=0\\). Then a symmetric matrix \\(A\\) satisfies \\(AX=0\\) if and only if there is a symmetric \\(S\\) such that \\(A=KSK'\\). If \\(\\text{rank}(K'AK)=r\\) then \\(S\\) can be chosento be of order \\(r\\).\n\n\nProof. Suppose \\(X=LT\\) with \\(L\\) an orthonormal basis for the column space of \\(X\\) and \\(T\\) non-singular. Write \\(A\\) as\n\\[\\begin{equation}\nA=\\begin{bmatrix}L&K\\end{bmatrix}\n\\begin{bmatrix}\nA_{11}&A_{12}\\\\A_{21}&A_{22}\n\\end{bmatrix}\n\\begin{bmatrix}L'\\\\K'\\end{bmatrix}.\n(\\#eq:invasolve)\n\\end{equation}\\]\nThen \\(AX=LA_{11}T+KA_{21}=0\\) which is true if and only if \\(A_{11}=0\\) and \\(A_{21}=0\\), and by symmetry \\(A_{12}=0\\). Thus \\(A=KA_{22}K'\\).\n\n\n\\(\\Delta\\in\\mathfrak{D}(X)\\) if and only if there is a symmetric \\(S\\) of order \\(n-p-1\\) such that for all \\(i\\not= j\\)\n\\[\\begin{equation}\n\\delta_{ij}=d_{ij}(X)\\left\\{1-\\frac{k_i'Sk_j^{\\ }}{w_{ij}}\\right\\}.\n(\\#eq:invalldelta)\n\\end{equation}\\]\n\n\nProof. By lemma XXX we have \\((V-B(X))X=0\\) if and only if there is a symmetric \\(S\\) such that \\(V-B(X)=KSK'\\). This can be rearranged to yield @ref(eq:invalldelta). Since the vector \\(e\\) satisfies both \\(X'e=0\\) and \\((V-B(X))e=0\\) we can choose \\(S\\) to be of order \\(n-p-1\\).\n\n\n\n\n\n\nProof. If \\(p=n-1\\) then \\(S\\) in theorem @ref(thm:keyresult) is of order zero.\n\n\n\n\nProof. With obvious notation \\(\\mathbf{tr}\\ X'(B_1(X)-B_2(X))X=0\\), which can be written as\n\\[\\begin{equation}\n\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n} w_{ij}\\left(\\delta_{1ij}-\\delta_{2ij}\\right)d_{ij}(X)=0,\n(\\#eq:invcompare)\n\\end{equation}\\]\nand thus \\(\\Delta_1\\leq\\Delta_2\\) implies \\(\\Delta_1=\\Delta_2\\).",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Inverse Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "inverse.html#non-negative-dissimilarities",
    "href": "inverse.html#non-negative-dissimilarities",
    "title": "24  Inverse Multidimensional Scaling",
    "section": "24.2 Non-negative Dissimilarities",
    "text": "24.2 Non-negative Dissimilarities\nFrom equation @ref(eq:invalldelta) it follows \\(\\delta_{ij}\\) is a decreasing function of \\(\\tau_{ij}\\), and \\(\\delta_{ij}\\geq 0\\) if and only if \\(\\tau_{ij}\\leq w_{ij}\\).\nConvex cone, affine convex cone\n\nA non-vacuous polyhedral convex set $C=\\{x\\mid Ax\\leq b\\}$ is bounded if and only if $Q=\\{x\\mid Ax\\leq 0\\}=\\{0\\}$. \n\n\nProof. See Goldman (1956), corollary 1B.\n\nIf \\(C\\) satisfies the conditions of lemma @ref(lem:goldman) then it is a bounded convex polyhedron and is the convex hull of its finite set of extreme vectors.\nThere are, of course, affine combinations \\(\\Delta\\) with negative elements. We could decide that we are only interested in non-negative dissimilarities. In order to deal with non-negativity we define \\(\\Delta_+\\) as the polyhedral convex cone of all symmetric, hollow, and non-negative matrices.\n\nWe have $\\Delta\\in\\Delta(W,X)\\cap\\Delta_+$ if and only if there is a symmetric $S$ such that \\@(eq:invalldelta) holds and such that \n\\begin{equation}\n\\mathbf{low}\\ (KSK')\\leq \\mathbf{low}\\ (W).\n\\end{equation}\nThus $\\Delta(W,X)\\cap\\Delta_+$ is a convex polyhedron, closed under non-negative linear combinations with coefficients that add up to one.\n\n\nProof. Follows easily from the representation in theorem @ref(thm:keyresult)} display = “n”)`.\n\nOf course the minimum of \\(\\sigma(X,W,\\Delta)\\) over \\(\\Delta\\in\\Delta(W,X)\\cap\\Delta_+\\) is zero, attained at \\(D(X)\\). The maximum of stress, which is a convex quadratic in \\(\\Delta\\), is attained at one of the vertices of \\(\\Delta(W,X)\\cap\\Delta_+\\).\n\n$\\mathfrak{D}(X)\\cap\\mathfrak{D}_+$ is bounded, i.e. it is a convex polygon.\n\n\nProof. This is corollary 3.2 in “Inverse Multidimensional Scaling” (2007), but the proof given there is incorrect. A hopefully correct proof goes as follows. A polyhedron is bounded if and only if its recession cone is the zero vector. If the polyhedron is defined by \\(Ax\\leq b\\) then the recession cone is the solution set of \\(Ax\\leq 0\\). Thus, in our case, the recession cone consists of all matrices \\(S\\) for which \\(\\text{low}\\ (KSK')\\leq 0\\). Since \\(U:=KSK'\\) is doubly-centered, and \\(K\\) is orthogonal to \\(X\\), we have\n\\[\\begin{equation}\n0=\\text{tr}\\ X'UX=2\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}u_{ij}d_{ij}^2(X).\\\n\\end{equation}\\]\\end{equation}\nThis implies \\(U=0\\), and the recession cone is the zero vector.\n\nWe can compute the vertices of \\(\\Delta(W,X)\\cap\\Delta_+\\) using the complete description method of (fukuda_15?), with an R implementation in the rcdd package by (geyer_meeden_15?). Alternatively, as a check on our computations, we also use the lrs method of Avis (2015), with an R implementation in the vertexenum package by (robere_15?). Both methods convert the H-representation of the polygon, as the solution set of a number of linear inequalities, to the V-representation, as the convex combinations of a number of vertices.\nThere is also a brute-force method of converting H to V that is somewhat wasteful, but still practical for small examples. Start with the H-representation \\(Ax\\leq b\\), where \\(A\\) is \\(n\\times m\\) with \\(n\\geq m\\). Then look at all \\(\\binom{n}{m}\\) choices of \\(m\\) rows of \\(A\\). Each choice partitions \\(A\\) into the \\(m\\times m\\) matrix \\(A_1\\) and the \\((n-m)\\times m\\) matrix \\(A_2\\) and \\(b\\) into \\(B-1\\) and \\(b_2\\). If \\(rank(A_1)&lt;m\\) there is no extreme point associated with this partitioning. If \\(rank(A_1)=m\\) we compute \\(\\hat v=A_1^{-1}b_1\\) and if \\(A_2\\hat v\\leq b_2\\) then we add \\(\\hat v\\) to the V representation.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Inverse Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "inverse.html#zero-weights-andor-distances",
    "href": "inverse.html#zero-weights-andor-distances",
    "title": "24  Inverse Multidimensional Scaling",
    "section": "24.3 Zero Weights and/or Distances",
    "text": "24.3 Zero Weights and/or Distances\nIf a distance is zero then the corresponding element of \\(B(X)\\) must be zero as well. If a weight is zero, then the corresponding elements of both \\(V\\) and \\(B(X)\\) are zero. It is still true that \\((V-B)X=0\\) if and only if there is an \\(S\\) such that \\(B=V-KSK'\\), but it may no longer be possible to find the \\(\\Delta\\) corresponding with some \\(V+KSK'\\). In other words, not all solutions \\(B\\) to \\((V-B)X=0\\) correspond with a proper \\(B(X)\\). Specifically, zero weights and/or distances imply that one or more elements of \\(B\\) are required to be zero. If these zero requirements are taken into account then not all matrices \\(S\\) are allowed.\nIf, for example, \\(X\\) is\n\n\n     [,1]\n[1,] -0.5\n[2,] -0.5\n[3,]  0.5\n[4,]  0.5\n\n\nand the weights are all one, then \\(V-B\\) must be a linear combination of the three matrices, say \\(P_{11}, P_{22}\\) and \\(P_{12}\\),\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1   -1    1   -1\n[2,]   -1    1   -1    1\n[3,]    1   -1    1   -1\n[4,]   -1    1   -1    1\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1   -1   -1    1\n[2,]   -1    1    1   -1\n[3,]   -1    1    1   -1\n[4,]    1   -1   -1    1\n\n\n     [,1] [,2] [,3] [,4]\n[1,]   -2    2    0    0\n[2,]    2   -2    0    0\n[3,]    0    0    2   -2\n[4,]    0    0   -2    2\n\n\nand \\(B\\) is \\(V\\) minus the linear combination. For any \\(B\\) computed this way we have \\(BX=VX\\), but we have \\(b_{12}=b_{34}=0\\) if and only if \\(B=V-\\alpha P_{11} + (1-\\alpha) P_{22}\\).",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Inverse Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "inverse.html#examples",
    "href": "inverse.html#examples",
    "title": "24  Inverse Multidimensional Scaling",
    "section": "24.4 Examples",
    "text": "24.4 Examples\n\n24.4.1 First Example\nAs our first example we take \\(X\\) equal to four points in the corners of a square. This example is also used in “Inverse Multidimensional Scaling” (2007) and De Leeuw (2012). Here \\(X\\) is\n\n\n     [,1] [,2]\n[1,] -0.5 -0.5\n[2,] -0.5  0.5\n[3,]  0.5  0.5\n[4,]  0.5 -0.5\n\n\nwith distances\n\n\n         1        2        3\n2 1.000000                  \n3 1.414214 1.000000         \n4 1.000000 1.414214 1.000000\n\n\nand \\(K\\) is the vector\n\n\n[1] -0.5  0.5 -0.5  0.5\n\n\nFor unit weights we have \\(\\Delta\\in\\Delta(X,W)\\) if and only if \\(\\Delta=D(X)\\{W-\\lambda kk'\\}\\) for some real \\(\\lambda\\). This means that \\(\\Delta\\in\\Delta(X,W)\\cap\\Delta_+\\) if and only if \\(-4\\leq\\lambda\\leq 4\\). The endpoints of this interval correspond with the two dissimilarity matrices \\[\n\\Delta_1:=2\\sqrt{2}\\begin{bmatrix}0&0&1&0\\\\0&0&0&1\\\\1&0&0&0\\\\0&1&0&0\\end{bmatrix},\n\\] and \\[\n\\Delta_2:=2\\begin{bmatrix}0&1&0&1\\\\1&0&1&0\\\\0&1&0&1\\\\1&0&1&0\\end{bmatrix}.\n\\] Thus \\(\\Delta(X,W)\\cap\\Delta_+\\) are the convex combinations \\[\n\\Delta(\\alpha):=\\alpha\\Delta_1+(1-\\alpha)\\Delta_2=\n\\begin{bmatrix}0&2(1-\\alpha)&2\\alpha\\sqrt{2}&2(1-\\alpha)\\\\2(1-\\alpha)&0&2(1-\\alpha)&2\\alpha\\sqrt{2}\\\\2\\alpha\\sqrt{2}&2(1-\\alpha)&0&2(1-\\alpha)\\\\2(1-\\alpha)&2\\alpha\\sqrt{2}&2(1-\\alpha)&0\\end{bmatrix}.\n\\] This can be thought of as the distances between points on a (generally non-Euclidean) square with sides \\(2(1-\\alpha)\\) and diagonal \\(2\\alpha\\sqrt{2}\\). The triangle inequalities are satisfied if the length of the diagonal is less than twice the length of the sides, i.e. if \\(\\alpha\\leq\\frac{2}{2+\\sqrt{2}}\\approx .585786\\).\nThe distances are certainly Euclidean if Pythagoras is satisfied, i.e. if the square of the length of the diagonal is twice the square of the length of the sides. This gives \\(\\alpha=\\frac12\\), for which \\(\\Delta=D(X)\\). For a more precise analysis, observe that the two binary matrices, say \\(E_1\\) and \\(E_2\\), in the definition of \\(\\Delta_1\\) and \\(\\Delta_2\\) commute, and are both diagonalized by\n\\[\nL:=\\begin{bmatrix}\n\\frac12&\\frac12\\sqrt{2}&0&\\frac12\\\\\n\\frac12&0&\\frac12\\sqrt{2}&-\\frac12\\\\\n\\frac12&-\\frac12\\sqrt{2}&0&\\frac12\\\\\n\\frac12&0&-\\frac12\\sqrt{2}&-\\frac12\n\\end{bmatrix}\n\\]\nThe diagonal elements of \\(L'E_1L\\) are \\(1,-1,-1,1\\) and those of \\(L'E_2 L\\) are \\(2,0,0,-2\\). Because \\(L\\) diagonalizes \\(E_1\\) and \\(E_2\\), it also diagonalizes \\(\\Delta_1\\) and \\(\\Delta_2\\), as well as the elementwise squares \\(\\Delta^2_1\\) and \\(\\Delta^2_2\\). And consequently also the Torgerson transform \\(-\\frac12 J\\Delta^2(\\alpha)J\\), which has eigenvalues \\(0,4\\alpha^2,4\\alpha^2,4(1-2\\alpha)\\). All eigenvalues are non-negative for \\(\\alpha\\leq\\frac12\\).\nWe see that \\(\\Delta(\\alpha)\\) is two-dimensional Euclidean for \\(\\alpha=\\frac12\\), one-dimensional Euclidean for \\(\\alpha=0\\), and three-dimensional Euclidean for \\(0&lt;\\alpha&lt;\\frac12\\). In particular for \\(\\alpha=\\frac{1}{1+\\sqrt{2}}\\) all dissimilarities are equal and \\(\\Delta(\\alpha)\\) is the distance matrix of a regular simplex.\nOn the unit interval stress is the quadratic \\(32(\\alpha-\\frac12)^2\\), which attains its maximum equal to 8 at the endpoints.\n\n\n24.4.2 Second Example\nWe next give another small example with four equally spaced points on the line, normalized to sum of squares one, and unit weights. Thus \\(X\\) is\n\n\n           [,1]\n[1,] -0.6708204\n[2,] -0.2236068\n[3,]  0.2236068\n[4,]  0.6708204\n\n\nand \\(D(X)\\) is\n\n\n         1        2        3\n2 3.464102                  \n3 4.472136 2.828427         \n4 6.324555 4.472136 3.464102\n\n\nFor \\(S\\) we use the basis \\(\\begin{bmatrix}1&0\\\\0&0\\end{bmatrix}\\), \\(\\begin{bmatrix}0&0\\\\0&1\\end{bmatrix}\\), and \\(\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}\\). Thus \\(\\Delta(X,W)\\) are the affine linear combinations of \\(D(X)\\) and the three matrices\n\n\n         1        2        3\n2 4.330127                  \n3 5.590170 2.121320         \n4 4.743416 5.590170 4.330127\n\n\n         1        2        3\n2 3.983717                  \n3 3.801316 4.101219         \n4 6.640783 3.801316 3.983717\n\n\n         1        2        3\n2 1.914908                  \n3 5.472136 2.828427         \n4 6.324555 3.472136 5.013295\n\n\nBoth scdd() from rcdd and `fromvertexenum` find the same seven vertices. All non-negative dissimilarity matrices for which \\(x\\) is stationary are convex combinations of these seven matrices.\n\n\n          1         2         3\n2 20.784610                    \n3  0.000000  5.656854          \n4  0.000000 13.416408  0.000000\n          1         2         3\n2 13.856406                    \n3  4.472136  0.000000          \n4  0.000000 13.416408  0.000000\n         1        2        3\n2 20.78461                  \n3  0.00000 22.62742         \n4  0.00000  0.00000 20.78461\n          1         2         3\n2  0.000000                    \n3 13.416408  5.656854          \n4  0.000000  0.000000 20.784610\n          1         2         3\n2  0.000000                    \n3 13.416408  0.000000          \n4  0.000000  4.472136 13.856406\n         1        2        3\n2 0.000000                  \n3 4.472136 0.000000         \n4 8.432738 4.472136 0.000000\n          1         2         3\n2  0.000000                    \n3  0.000000  5.656854          \n4 12.649111  0.000000  0.000000\n\n\nThe stress values for these seven vertices are\n\n\n[1]  460.00000  248.00000 1072.00000  460.00000  248.00000   36.44444  112.00000\n\n\nand we know that 1072 is the maximum of stress over \\(\\Delta\\in\\Delta(X,W)\\cap\\Delta_+\\).\nIn general the vanishing of the stationary equations does not imply that \\(X\\) corresponds with a local minimum. It can also give a local maximum or a saddle point. We know, however, that the only local maximum of stress is the origin (De Leeuw, Groenen, and Mair (2016a)), and that in the one-dimensional case all solutions of the stationary equations that do not have tied coordinates are local minima (De Leeuw (2005)).\n\n\n24.4.3 Third Example\nThe number of extreme points of the polytope \\(\\Delta(W,X)\\cap\\Delta_+\\) grows very quickly if the problem becomes larger. In our next example we take six points equally spaced on the unit sphere. Due to the intricacies of floating point comparisons (testing for zero, testing for equality) it can be difficult to determine exactly how many extreme points there are.\n“Inverse Multidimensional Scaling” (2007) analyzed this example and found 42 extreme points. We repeat their analysis with our R function bruteForce() from the code section. We select \\(\\binom{15}{6}=5005\\) sets of six linear equations from our 15 linear inequalities, test them for non-singularity, and solve them to see if they satisfy the remaining nine inequalities. This gives 1394 extreme points of the polytope, but many of them are duplicates. We use our function cleanUp() to remove duplicates, which leaves 42 vertices, same number as found by “Inverse Multidimensional Scaling” (2007). The 42 stress values are\n\n\n [1] 24.00000 24.00000 24.00000 21.75000 12.00000 13.33333  6.66667 13.33333\n [9] 17.33333 17.33333 19.68000 13.33333  6.66667 17.33333  6.66667 21.75000\n[17]  6.66667 60.00000 24.00000 21.75000 19.68000 17.33333 21.75000 13.33333\n[25] 19.68000 17.33333 17.33333 21.75000 17.33333 17.33333 13.33333  6.66667\n[33] 21.75000 17.33333 19.68000 13.33333 17.33333 19.68000 19.68000 17.33333\n[41]  6.66667 17.33333\n\n\nand their maximum is 60.\nIf we perform the calculations more efficiently in rcdd, using rational arithmetic, we come up with a list of 154 extreme points. Using cleanUp() to remove what seem to be duplicates leaves 42.\nThe fact that we get different numbers of vertices with different methods is somewhat disconcerting. We test the vertices found by rcdd and vertexenum that are not found with the brute force method by using our function rankTest(). This test is based on the fact that a vector \\(x\\) satisfying the \\(n\\times m\\) system \\(Ax\\leq b\\) is an extreme point if and only if matrix with all rows \\(a_i\\) for which \\(a_i'x=b_i\\) is of rank \\(m\\). It turns out all the additional vertices found by rcdd and vertexenum do not satisfy this rank test, because the matrix of active constraints (satisfied as equalities) is of rank 5.\n\n\n24.4.4 Fourth Example\nThis is an unfolding example with \\(n=3+3\\) points, configuration\n\n\n           [,1]       [,2]\n[1,]  0.0000000  0.0000000\n[2,]  0.0000000  0.0000000\n[3,]  0.0000000 -0.8164966\n[4,]  0.0000000  0.4082483\n[5,]  0.7071068  0.0000000\n[6,] -0.7071068  0.4082483\n\n\nand weight matrix\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    0    0    0    1    1    1\n[2,]    0    0    0    1    1    1\n[3,]    0    0    0    1    1    1\n[4,]    1    1    1    0    0    0\n[5,]    1    1    1    0    0    0\n[6,]    1    1    1    0    0    0\n\n\nNote that row-points one and two in \\(X\\) are equal, and thus \\(d_{12}(X)=0\\). The example has both zero weights and zero distances. We now require that \\((V-B(X))X=0\\), but also that the elements of \\(B(X)\\) in the two \\(3\\times 3\\) principal submatrices corresponding with the rows and columns are zero. This means that for the elements of \\(B(X)\\) we require \\(k_i'Sk_j\\leq 1\\) for \\(w_{ij}=1\\) and both \\(k_i'Sk_j\\leq 0\\) and \\(-k_i'Sk_j\\leq 0\\) for \\(w_{ij}=0\\). We can then solve for edges of the off-diagonal block of dissimilarities. There are no constraints on the dissimilarities in the diagonal blocks, because they are not part of the MDS problem.\nUsing our brute force method, we find the two edges\n\n\n          4        5        6\n1 0.0000000 1.414214 1.632993\n2 0.8164966 0.000000 0.000000\n3 1.2247449 1.080123 1.414214\n\n\n          4        5        6\n1 0.8164966 0.000000 0.000000\n2 0.0000000 1.414214 1.632993\n3 1.2247449 1.080123 1.414214\n\n\nand the off-diagonal blocks for which \\(X\\) is an unfolding solution are convex combinations of these two.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Inverse Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "inverse.html#mds-sensitivity",
    "href": "inverse.html#mds-sensitivity",
    "title": "24  Inverse Multidimensional Scaling",
    "section": "24.5 MDS Sensitivity",
    "text": "24.5 MDS Sensitivity\nSuppose \\(X\\) is a solution to the MDS problem with dissimilarities \\(\\Delta\\), found by some iterative algorithm such as smacof. We can then compute \\(\\mathfrak{D}(X)\\cap\\mathfrak{D}_+\\), which is a convex neighborhood of the data \\(\\Delta\\), and consists of all non-negative dissimilarity matrices that have \\(X\\) as a solution to the stationary equations. The size of this convex neighborhood can be thought of as a measure of stability or sensitivity.\nFor typical MDS examples there is no hope of computing all vertices of \\(\\mathfrak{D}(X)\\cap\\mathfrak{D}_+\\). Consider the data from De Gruijter (1967), for example, with \\(n=9\\) objects, to be scaled in \\(p=2\\) dimensions. We have \\(\\frac12 n(n-1)=36\\) dissimilarities, and because \\(m=n-p-1=6\\) there are \\(\\frac12 m(m+1)=21\\) variables. It suffices to consider that there are 5567902560 ways in which we can pick 21 rows from 36 rows to understand the number of potential vertices.\nWhat we can do, however, is to optimize linear (or quadratic functions) over \\(\\mathfrak{D}(X)\\cap\\mathfrak{D}_+\\), because 36 linear inequalities in 21 variables define an easily manageable LP (or QP) problem. As an example, not necessarily a very sensible one, we solve 36 linear programs to maximize and minimize each of the \\(\\delta_{ij}\\) in \\(\\mathfrak{D}(X)\\cap\\mathfrak{D}_+\\) separately. We use the lpSolve package (Berkelaar, M. and others (2015)), and collect the maximum and minimum \\(\\delta_{ij}\\) in a matrix. The range from the smallest possible \\(\\delta_{ij}\\) to the largest possible \\(\\delta_{ij}\\) turns out to be quite large.\nThe data are\n\n\n      KVP PvdA  VVD  ARP  CHU  CPN  PSP   BP  D66\nKVP  0.00 5.63 5.27 4.60 4.80 7.54 6.73 7.18 6.17\nPvdA 5.63 0.00 6.72 5.64 6.22 5.12 4.59 7.22 5.47\nVVD  5.27 6.72 0.00 5.46 4.97 8.13 7.55 6.90 4.67\nARP  4.60 5.64 5.46 0.00 3.20 7.84 6.73 7.28 6.13\nCHU  4.80 6.22 4.97 3.20 0.00 7.80 7.08 6.96 6.04\nCPN  7.54 5.12 8.13 7.84 7.80 0.00 4.08 6.34 7.42\nPSP  6.73 4.59 7.55 6.73 7.08 4.08 0.00 6.88 6.36\nBP   7.18 7.22 6.90 7.28 6.96 6.34 6.88 0.00 7.36\nD66  6.17 5.47 4.67 6.13 6.04 7.42 6.36 7.36 0.00\n\n\nThe optimal configuration found by smacof is\n\n\n       [,1]   [,2]\n [1,]  1.78  3.579\n [2,] -1.46  2.297\n [3,]  3.42 -2.776\n [4,]  3.30  1.837\n [5,]  3.84  0.308\n [6,] -5.09 -0.044\n [7,] -3.79  2.132\n [8,] -2.86 -4.318\n [9,]  0.86 -3.015\n\n\n\n\n\n\n\nDe Gruijter Configuration\n\n\n\n\nThe maximum and minimum dissimilarities in \\(\\Delta(X,W)\\cap\\Delta_+\\) are\n\n\n     KVP   PvdA  VVD   ARP   CHU   CPN   PSP   BP    D66  \nKVP    0.0  32.6   9.7   9.3  24.6  24.3  11.7  18.9  15.2\nPvdA  32.6   0.0  25.7  36.9  20.7  34.1  36.1  29.7  22.2\nVVD    9.7  25.7   0.0  17.4  32.0  34.4  22.3  23.5  20.4\nARP    9.3  36.9  17.4   0.0  28.0  33.7  25.6  28.9  23.0\nCHU   24.6  20.7  32.0  28.0   0.0  20.8  31.9  33.3  26.3\nCPN   24.3  34.1  34.4  33.7  20.8   0.0  24.9  30.7  31.8\nPSP   11.7  36.1  22.3  25.6  31.9  24.9   0.0  23.7  27.7\nBP    18.9  29.7  23.5  28.9  33.3  30.7  23.7   0.0  35.0\nD66   15.2  22.2  20.4  23.0  26.3  31.8  27.7  35.0   0.0\n\n\n     KVP   PvdA  VVD   ARP   CHU   CPN   PSP   BP    D66  \nKVP   0.00  3.48  0.00  0.00  2.34  0.00  0.00  0.00  0.00\nPvdA  3.48  0.00  0.00  0.00  0.00  0.00  0.93  0.00  0.00\nVVD   0.00  0.00  0.00  0.00  0.83  0.00  0.00  0.00  0.00\nARP   0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\nCHU   2.34  0.00  0.83  0.00  0.00  0.00  0.00  0.00  0.00\nCPN   0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\nPSP   0.00  0.93  0.00  0.00  0.00  0.00  0.00  0.00  0.00\nBP    0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\nD66   0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Inverse Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "inverse.html#second-order-inverse-mds",
    "href": "inverse.html#second-order-inverse-mds",
    "title": "24  Inverse Multidimensional Scaling",
    "section": "24.6 Second Order Inverse MDS",
    "text": "24.6 Second Order Inverse MDS\nAs we mentioned many times before, stationary values are not necessarily local minima. It is necessary for a local minimum that the stationary equations are satisfied, but it is also necessary that at a solution of the stationary equations the Hessian is positive semi-definite. Thus it becomes interesting to find the dissimilarities in \\(\\Delta(W,X)\\cap\\Delta_+\\) for which the Hessian is positive semi-definite. Define \\[\n\\Delta_H(W,X):=\\{\\Delta\\mid\\mathcal{D}^2\\sigma(X)\\gtrsim 0\\}.\n\\] We can now study \\(\\Delta(W,X)\\cap\\Delta_H(W,X)\\) or \\(\\Delta(W,X)\\cap\\Delta_H(W,X)\\cap\\Delta_+\\).\n\n$\\Delta_H(W,X)$ is a convex non-polyhedral set.\n\n\nProof. In MDS the Hessian is \\(2(V-H(x,W,\\Delta))\\), where \\[\\begin{equation}\nH(x,W,\\Delta):=\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\frac{\\delta_{ij}}{d_{ij}(x)}\\left\\{A_{ij}-\\frac{A_{ij}xx'A_{ij}}{x'A_{ij}x}\\right\\}.\n\\end{equation}\\] Here we use \\(x:=\\mathbf{vec}(X)\\), and both \\(A_{ij}\\) and \\(V\\) are direct sums of \\(p\\) copies of our previous \\(A_{ij}\\) and \\(V\\) (De Leeuw, Groenen, and Mair (2016c)). The Hessian is linear in \\(\\Delta\\), which means that requiring it to be positive semi-definite defines a convex non-polyhedral set. The convex set is defined by the infinite system of linear inequalities \\(y'H(x,W,\\Delta)y\\geq 0\\).\n\nIn example 3 the smallest eigenvalues of the Hessian at the 42 vertices are\n\n\n [1]   0.00000   0.00000   0.00000  -5.86238   0.00000  -2.96688   0.00000\n [8]  -2.96688  -4.47894  -4.47894  -5.71312  -2.96688   0.00000  -5.00453\n[15]   0.00000  -5.86238   0.00000 -12.00000   0.00000  -5.86238  -5.71312\n[22]  -5.00453  -5.86238  -2.96688  -5.71312  -4.47894  -5.00453  -5.86238\n[29]  -5.00453  -5.00453  -2.96688   0.00000  -5.86238  -4.47894  -5.71312\n[36]  -2.96688  -5.00453  -5.71312  -5.71312  -4.47894   0.00000  -4.47894\n\n\nand thus there are at most 11 vertices corresponding with local minima.\nThe next step is to refine the polyhedral approximation to \\(\\Delta(W,X)\\cap\\Delta_H(W,X)\\cap\\Delta_+\\) by using cutting planes. We add linear inequalities to the H-representation by using the eigenvectors corresponding to the smallest eigenvalues of all those vertices for which this smallest eigenvalue is negative. Thus, if the eigenvector is \\(y\\), we would add the inequality \\[\\begin{equation}\n\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}\\zeta_{ij}(w_{ij}-k_i'Sk_j)\\geq 0,\n\\end{equation}\\] where \\[\n\\zeta_{ij}:=y'\\left(A_{ij}-\\frac{A_{ij}xx'A_{ij}}{x'A_{ij}x}\\right)y.\n\\] It is clear, however, that adding a substantial number of linear inequalities will inevitably lead to a very large number of potential extreme points. We proceed conservatively by cutting off only the solution with the smallest negative eigenvalue in computing the new V representation, using our function bruteForceOne().",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Inverse Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "inverse.html#inverse-fds",
    "href": "inverse.html#inverse-fds",
    "title": "24  Inverse Multidimensional Scaling",
    "section": "24.7 Inverse FDS",
    "text": "24.7 Inverse FDS\nA configuration \\(X\\) is a full dimensional scaling or FDS solution (De Leeuw, Groenen, and Mair (2016a)) if \\((V-B(X))X=0\\) and \\(V-B(X)\\) is positive semi-definite. In that case \\(X\\) actually provides the global minimum of stress. The inverse FDS or iFDS problem is to find all \\(\\Delta\\) such that a given \\(X\\) is the FDS solution.\n\n$\\Delta\\in\\Delta_F(W,X)$ if and only if there is a positive semi-definite $S$ such that for all $i\\not= j$ \n\\begin{equation}\n\\delta_{ij}=d_{ij}(X)(1-\\frac{1}{w_{ij}}k_i'Sk_j),\n\\end{equation}\nThus $\\Delta_F(W,X)$ is a non-polyhedral convex set, closed under linear combinations with coefficients that add up to one.\n\n\nProof. Of course \\(\\Delta_F(W,X)\\subseteq\\Delta(W,X)\\). Thus \\(V-B(X)=KSK'\\) for some \\(S\\), and \\(XX'\\) and \\(V-B(X)\\) must both be positive semi-definite, with complementary null spaces.\n\nWe reanalyze our second example, with the four points equally spaced on the line, requiring a positive semi-definite \\(S\\). We start with the original 7 vertices, for which the minimum eigenvalues of \\(S\\) are\n\n\n[1] -4.000 -2.899 -1.708 -3.333  8.000 -1.708 -2.899\n\n\nIf the minimum eigenvalue is negative, with eigenvector \\(y\\), we add the constraints \\(y'Sy\\geq 0\\). This leads to 11 vertices with minimum eigenvalues\n\n\n [1]  8.0000 -0.0355  0.0000 -0.7911 -0.3834 -0.3834 -0.0355 -0.0853 -0.0853\n[10] -0.7911  0.0000\n\n\nWe see that the negative eigenvalues are getting smaller. Repeating the procedure of adding constraints based on negative eigenvalues three more times gives 19, 37, and 79 vertices, with corresponding minimum eigenvalues\n\n\n [1]  8.000000 -0.000021  0.000000 -0.209852 -0.202322 -0.085642 -0.085642\n [8] -0.000021 -0.106132 -0.018912 -0.008051 -0.023967 -0.008051 -0.023967\n[15] -0.106132 -0.018912 -0.209852 -0.202322  0.000000\n\n\n [1]  8.000000 -0.000021  0.000000 -0.056872 -0.053562 -0.045307 -0.062028\n [8] -0.020853 -0.020853 -0.000021 -0.028793 -0.004490 -0.001925 -0.006410\n[15] -0.001925 -0.006410 -0.028793 -0.004490 -0.000005 -0.002104 -0.021934\n[22] -0.024355 -0.021934 -0.024355 -0.000005 -0.002104 -0.004975 -0.005596\n[29] -0.005596 -0.004975 -0.002318 -0.002318 -0.056872 -0.053562 -0.045307\n[36] -0.062028  0.000000\n\n\n [1]  8.000000 -0.000021  0.000000 -0.015026 -0.013698 -0.010861 -0.017838\n [8] -0.013830 -0.013426 -0.012084 -0.013998 -0.018140 -0.005180 -0.005180\n[15] -0.000021 -0.007568 -0.001096 -0.000471 -0.001662 -0.000471 -0.001662\n[22] -0.007568 -0.001096 -0.000005 -0.000001 -0.000538 -0.000488 -0.005588\n[29] -0.005886 -0.005588 -0.005886 -0.000005 -0.000001 -0.000538 -0.000488\n[36] -0.001278 -0.001355 -0.001355 -0.001278 -0.000001 -0.000649 -0.000594\n[43] -0.005244 -0.005378 -0.005244 -0.005378 -0.000001 -0.000649 -0.000594\n[50] -0.006838 -0.006293 -0.001150 -0.001210 -0.000492 -0.000514 -0.000566\n[57] -0.001545 -0.001444 -0.000492 -0.000514 -0.000566 -0.001545 -0.001444\n[64] -0.006838 -0.006293 -0.001150 -0.001210 -0.000001 -0.000001 -0.015026\n[71] -0.013698 -0.010861 -0.017838 -0.013830 -0.013426 -0.012084 -0.013998\n[78] -0.018140  0.000000\n\n\nIt should be noted that the last step already takes an uncomfortable number of minutes to compute. Although the number of vertices goes up quickly, the diameter of the polygon (the maximum distance between two vertices) slowly goes down and will eventually converge to the diameter of \\(\\Delta_F(W,X)\\cap\\Delta_+\\). Diameters in subsequent steps are\n\n\n[1] 5.657 5.086 5.065 5.061 5.060",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Inverse Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "inverse.html#multiple-solutions",
    "href": "inverse.html#multiple-solutions",
    "title": "24  Inverse Multidimensional Scaling",
    "section": "24.8 Multiple Solutions",
    "text": "24.8 Multiple Solutions\nIf \\(X_1,\\cdots,X_s\\) are configurations with the same number of points, then the intersection \\(\\left\\{\\bigcap_{r=1}^s\\Delta(W,X_r)\\right\\}\\cap\\Delta_+\\) is again a polygon, i.e. a closed and bounded convex polyhedron (which may be empty). If \\(\\Delta\\) is in this intersection then \\(X_1,\\cdots,X_s\\) are all solutions of the stationary equations for this \\(\\Delta\\) and \\(W\\).\nLet’s look at the case of two configurations \\(X_1\\) and \\(X_2\\). We must find vectors \\(t_1\\) and \\(t_2\\) such that \\[\nd_1\\circ(e-w^\\dagger\\circ G_1t_1)=d_2\\circ(e-w^\\dagger\\circ G_2t_2).\n\\] If \\(H_1:=\\mathbf{diag}(d_1\\circ w^\\dagger)G_1\\) and \\(H_2:=\\mathbf{diag}(d_2\\circ w^\\dagger)G_2\\) then this can be written as \\[\n\\begin{bmatrix}H_1&-H_2\\end{bmatrix}\\begin{bmatrix}t_1\\\\t_2\\end{bmatrix}=d_1-d_2\n\\] This is a system of linear equations in \\(t_1\\) and \\(t_2\\). If it is solvable we can intersect it with the convex sets \\(G_1t_1\\leq w\\) and \\(G_2t_2\\leq w\\) to find the non-negative dissimilarity matrices \\(\\Delta\\) for which both \\(X_1\\) and \\(X_2\\) are stationary.\n\n\n$delta1\n         [,1]\n[1,] 1.464102\n[2,] 1.464102\n[3,] 1.464102\n[4,] 1.464102\n[5,] 1.464102\n[6,] 1.464102\n\n$delta2\n         [,1]\n[1,] 1.464102\n[2,] 1.464102\n[3,] 1.464102\n[4,] 1.464102\n[5,] 1.464102\n[6,] 1.464102\n\n$res\n[1] 1.049631e-15\n\n$rank\n[1] 2\n\n\nAs a real simple example, suppose \\(X\\) and \\(Y\\) are four by two. They both have three points in the corners of an equilateral triangle, and one point in the centroid of the other three. In \\(X\\) the fourth point is in the middle, in \\(Y\\) the first point. The only solution to the linear equations is the matrix with all dissimilarities equal.\nFor a much more complicated example we can choose the De Gruijter data. We use smacof to find two stationary points in two dimensions. The matrices \\(G_1\\) and \\(G_2\\) are \\(36\\times 21\\), and thus \\(H:=(H_1\\mid -H_2)\\) is \\(36\\times 42\\). The solutions of the linear system \\(Ht=d_1-d_2\\) are of the form \\(t_0-Lt\\), with \\(t_0\\) an arbitrary solution and \\(L\\) a \\(42\\times 6\\) basis for the space of \\(Ht=0\\). To find the non-negative solutions we can use the H representation \\(Lv\\leq t_0\\), and then compute the V representation, realizing of course that we can choose 6 rows from 42 rows in 5245786 ways.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Inverse Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "inverse.html#minimizing-istress",
    "href": "inverse.html#minimizing-istress",
    "title": "24  Inverse Multidimensional Scaling",
    "section": "24.9 Minimizing iStress",
    "text": "24.9 Minimizing iStress\nThe IMDS approach can also be used to construct an alternative MDS loss function. We call it iStress, defined as\n\\[\n\\sigma_\\iota(X,W,\\Delta):=\\min_{\\tilde\\Delta\\in\\Delta(W,X)\\cap\\Delta_+}\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\delta_{ij}-\\tilde\\delta_{ij})^2.\n\\]\nMinimizing iStress means minimizing the projection distance between the observed dissimilarity matrix and the moving convex set of non-negative dissimilarity matrices for which \\(X\\) satisfies the stationary equations. The convex set is moving, because it depends on \\(X\\). For each \\(X\\) we have to solve the IMDS problem of finding \\(\\Delta(X,W)\\cap\\Delta_+\\), and then solve the quadratic programming problem that computes the projection.\n\n$\\min_X\\sigma_\\iota(X,W,\\Delta)=0$ and the minimum is attained at all $X$ with $(V-B(X))X=0$.\n\n\nProof. If \\(X\\) is a stationary point of stress then \\(\\Delta\\in\\mathfrak{D}(X)\\cap\\mathfrak{D}_+\\) and thus iStress is zero. Conversely, if iStress is zero then \\(\\Delta\\in\\mathfrak{D}(X)\\cap\\mathfrak{D}_+\\) and \\(X\\) is a stationary point of stress.\n\nMinimizing iStress may not be a actual practical MDS method, but it has some conceptual interest, because it provides another way of looking at the relationship of MDS and IMDS.\nWe use the De Gruijter data for an example of iStress minimization. We use optim from base R, and the quadprog package of (turlach_weingessel_13?). Two different solutions are presented, the first with iterations starting at the classical MDS solution, the second starting at the smacof solution. In both cases iStress converges to zero, i.e. the configurations converge to a solution of the stationary equations for stress, and in the smacof case the initial solution already has stress equal to zero.\n\n\ninitial  value 106.624678 \niter  10 value 0.205734\niter  20 value 0.025163\niter  30 value 0.018427\niter  40 value 0.000116\niter  50 value 0.000007\niter  60 value 0.000000\nfinal  value 0.000000 \nconverged\n\n\ninitial  value 0.000000 \niter  10 value 0.000000\niter  20 value 0.000000\nfinal  value 0.000000 \nconverged\n\n\n\n\n\n\n\n\nDe Gruijter Minimim iStress Configuration",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Inverse Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "inverse.html#order-three",
    "href": "inverse.html#order-three",
    "title": "24  Inverse Multidimensional Scaling",
    "section": "24.10 Order three",
    "text": "24.10 Order three\nWe will now consider, in some detail, MDS of a dissimilarity matrix of order three.. The plan is as follows. Our MDS problem is of order three, i.e. there are only three objects, and three dissimilarities between them. Suppose, for simplicity, that all weights are one. At a stationary point of we have \\(B(X)X=3X\\). Thus the \\(B\\) matrix has an eigenvalue equal to \\(3\\) (in addition to having an eigenvalue equal to zero).\n!! Not just eval 3, but also evecs X\nIf there are only three objects we can look at the set of doubly-centered matrices of order three with an eigenvalue equal to three. If three is the largest eigenvalue then the local minimum is global, if it is the second largest eigenvalue then it may or may not be global. If \\(B\\) has two eigenvalues equal to three, then \\(B=3J\\) and the two eigenvectors give the unique global minimum in two dimensions (an equilateriaL triangle).\nA symmetric doubly-centered matrix of order three is a linear combination \\(B=\\alpha A_{12}+\\beta A_{13}+\\gamma A_{23}\\) of three diff matrices (@ref(#propmatrix)), with \\(\\alpha, \\beta, \\gamma\\) all non-negative. Thus\n\\[\\begin{equation}\nB=\\begin{bmatrix}\n\\alpha+\\beta&-\\alpha&-\\beta\\\\\n-\\alpha&\\alpha+\\gamma&-\\gamma\\\\\n-\\beta&-\\gamma&\\beta+\\gamma\n\\end{bmatrix}.\n(\\#eq:bgeneral)\n\\end{equation}\\]\n\\(B\\) has one eigenvalue equal to zero, and two real non-negative eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\). The characteristic equation is \\(f(\\lambda):=\\lambda(\\lambda^2-2\\tau\\lambda+3\\eta)=0,\\) where \\(\\tau:=\\alpha+\\beta+\\gamma\\) and \\(\\eta:=\\alpha\\beta+\\alpha\\gamma+\\beta\\gamma.\\)\nThus we have at least one eigenvalue equal to three if \\(f(3)=0\\), i.e. \\(2\\tau-\\eta=3\\). Both eigenvalues are equal to three if \\(\\eta=\\tau=3\\), which means \\(\\alpha=\\beta=\\gamma=1\\) and \\(B=3J\\).\nThe two eigenvalues are \\(\\tau\\pm\\sqrt{\\tau^2-3\\eta}\\) Note that \\(\\tau^2-3\\eta\\geq 0\\) with equality if and only if \\(\\tau=\\eta=3\\) if and only if \\(\\alpha=\\beta=\\gamma=1\\). This easily follows from \\(\\tau^2-3\\eta=\\alpha^2+\\beta^2+\\gamma^2-\\eta\\), while \\(\\eta=\\alpha\\beta+\\alpha\\gamma+\\beta\\gamma\\leq\\alpha^2+\\beta^2+\\gamma^2\\) by applying AM/GM three times. Also note that if two out of the three of \\(\\alpha,\\beta,\\gamma\\) are zero then \\(\\eta=0\\) and thus \\(\\lambda_1=2\\tau\\) and \\(\\lambda_2=0\\).\nSince \\(\\lambda_1\\geq\\lambda_2\\) we have the two possibilities \\(\\lambda_2\\leq\\lambda_1=3\\) and \\(3\\geq\\lambda_1\\geq\\lambda_2=3\\) Now \\(\\lambda_1=3\\) iff \\(\\sqrt{\\tau^2-3\\eta}=3-\\tau\\) iff \\(\\tau\\leq 3\\) and \\(2\\tau-\\eta=3\\). And \\(\\lambda_2=3\\) iff \\(\\sqrt{\\tau^2-3\\eta}=\\tau-3\\) iff \\(\\tau\\geq 3\\) and \\(2\\tau-\\eta=3\\). It also follows that it is necessary for \\(\\lambda=3\\) that \\(\\tau\\geq\\frac32\\).\nIf we have \\((\\tau,\\eta)\\) we can solve for \\(\\alpha,\\beta\\) and \\(\\gamma\\) by \\[\n\\alpha+\\beta+\\gamma=\\tau,\\\\\n\\alpha^2+\\beta^2+\\gamma^2=\\omega\n\\] Thus the set of \\((\\alpha,\\beta,\\gamma)\\) corresponding with \\(\\tau,\\eta\\) is the intersection of a sphere and an equilateral triangle in the non-negative orthant of \\(\\mathbb{R}^3\\).\n\\[\n\\gamma=\\tau-\\alpha-\\beta\\\\\n\\alpha^2+\\beta^2-\\tau\\alpha-\\tau\\beta+\\alpha\\beta=-\\eta\n\\]\nellipse with center \\(\\frac13\\tau e\\) and radius \\(\\frac13(\\tau-3)^2\\)\n\nimdsSolver(2)\n\n\n\n\n\n\n\nimdsSolver(3/2)\n\n\n\n\n\n\n\nimdsSolver(3)\npoints (1, 1, col = \"RED\")\n\n\n\n\n\n\n\nimdsSolver(8)",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Inverse Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "inverse.html#order-four",
    "href": "inverse.html#order-four",
    "title": "24  Inverse Multidimensional Scaling",
    "section": "24.11 Order Four",
    "text": "24.11 Order Four\n\\[\nB=\\alpha A_{12}+\\beta A_{13}+\\gamma A_{14}+\\delta A_{23}+\\epsilon A_{24}+\\phi A_{34}\n\\] \\[\nB=\\begin{bmatrix}\n\\alpha+\\beta+\\gamma&-\\alpha&-\\beta&-\\gamma\\\\\n-\\alpha&\\alpha+\\delta+\\epsilon&-\\delta&-\\epsilon\\\\\n-\\beta&-\\delta&\\beta+\\delta+\\phi&-\\phi\\\\\n-\\gamma&-\\epsilon&-\\phi&\\gamma+\\epsilon+\\phi\n\\end{bmatrix}.\n\\] The characteristic equation is \\[\nf(\\lambda)=\\lambda(\\lambda^3-2\\tau\\lambda^2+(3\\eta+(\\alpha\\phi+\\beta\\epsilon+\\gamma\\delta))\\lambda-Y).\n\\] \\[\nZ=\\frac12\\begin{bmatrix}\n+1&+1&+1&+1\\\\\n+1&+1&-1&-1\\\\\n+1&-1&+1&-1\\\\\n+1&-1&-1&+1\n\\end{bmatrix}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Inverse Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "inverse.html#sensitivity",
    "href": "inverse.html#sensitivity",
    "title": "24  Inverse Multidimensional Scaling",
    "section": "24.12 Sensitivity",
    "text": "24.12 Sensitivity\n\\[\n\\sigma(W,\\Delta)=\\min_X\\sigma(X,W,\\Delta)\n\\] \\[\nX(W,\\Delta)=\\mathop{\\text{Argmin}}_X\\sigma(X,W,D)\n\\] \\[\n\\mathcal{D}_1\\sigma(W,\\Delta)=(\\Delta-D(X(W,D)))^{(2)}\n\\]\n\\[\n\\mathcal{D}_2\\sigma(W,\\Delta)=W*(\\Delta-D(X(W,D)))\n\\]\n\n\n\n\nAvis, D. 2015. User’s Guide for lrs - Version 6.1. The Computational Geometry Lab at McGill.\n\n\nBerkelaar, M. and others. 2015. lpSolve: Interface to ’Lp_solve’ v. 5.5 to Solve Linear/Integer Programs.\n\n\nDe Gruijter, D. N. M. 1967. “The Cognitive Structure of Dutch Political Parties in 1966.” Report E019-67. Psychological Institute, University of Leiden.\n\n\nDe Leeuw, J. 2005. “Unidimensional Scaling.” In The Encyclopedia of Statistics in Behavioral Science, edited by B. S. Everitt and D. Howell, 4:2095–97. New York, N.Y.: Wiley.\n\n\n———. 2012. “On Inverse Multidimensional Scaling.” UCLA Department of Statistics. https://jansweb.netlify.app/publication/deleeuw-u-12-b/deleeuw-u-12-b.pdf.\n\n\nDe Leeuw, J., P. Groenen, and P. Mair. 2016a. “Full-Dimensional Scaling.” 2016. https://jansweb.netlify.app/publication/deleeuw-groenen-mair-e-16-e/deleeuw-groenen-mair-e-16-e.pdf.\n\n\n———. 2016b. “More on Inverse Multidimensional Scaling.” 2016. https://jansweb.netlify.app/publication/deleeuw-groenen-mair-e-16-f/deleeuw-groenen-mair-e-16-f.pdf.\n\n\n———. 2016c. “Second Derivatives of rStress, with Applications.” 2016. https://jansweb.netlify.app/publication/deleeuw-groenen-mair-e-16-c/deleeuw-groenen-mair-e-16-c.pdf.\n\n\n———. 2016d. “Singularities and Zero Distances in Multidimensional Scaling.” 2016.\n\n\nGoldman, A. J. 1956. “Resolution and Separation Theorems for Polyhedral Convex Sets.” In Linear Inequalities and Related Systems, edited by Kuhn H. W. and A. W. Tucker, 41–51. Annals of Mathematics Studies 38. Princeton University Press.\n\n\n“Inverse Multidimensional Scaling.” 2007. Journal of Classification 14: 3–21.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Inverse Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "stability.html",
    "href": "stability.html",
    "title": "25  Stability of MDS Solutions",
    "section": "",
    "text": "25.1 Null Distribution",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Stability of MDS Solutions</span>"
    ]
  },
  {
    "objectID": "stability.html#pseudo-confidence-ellipsoids",
    "href": "stability.html#pseudo-confidence-ellipsoids",
    "title": "25  Stability of MDS Solutions",
    "section": "25.2 Pseudo-confidence Ellipsoids",
    "text": "25.2 Pseudo-confidence Ellipsoids",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Stability of MDS Solutions</span>"
    ]
  },
  {
    "objectID": "stability.html#a-pseudo-bootstrap",
    "href": "stability.html#a-pseudo-bootstrap",
    "title": "25  Stability of MDS Solutions",
    "section": "25.3 A Pseudo-Bootstrap",
    "text": "25.3 A Pseudo-Bootstrap",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Stability of MDS Solutions</span>"
    ]
  },
  {
    "objectID": "stability.html#how-large-is-my-stress",
    "href": "stability.html#how-large-is-my-stress",
    "title": "25  Stability of MDS Solutions",
    "section": "25.4 How Large is My Stress ?",
    "text": "25.4 How Large is My Stress ?\nUsers of MDS, influenced no doubt by the tyranny of significance tests, often ask if their stress level from a smacof analysis is low or high. The appropriate answer, in many cases, is: “It’s as low as I could get it.”\nKruskal has muddied the waters.\nNormalization. Remember\n\\[\n\\sigma(X)=\\frac{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\delta_{ij}-d_{ij}(X))^2}{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\delta_{ij}^2}\n\\] so it is the weighted average of the squared residuals, relative to the weighted average of the squared dissimilarities. If we take the square root then weighted averages of squares become root-mean-squares.\nThurstonian\nRamsay\nMonte Carlo: De Leeuw and Stoop (1984), Spence (1983)\n\\[\n\\mathbb{E}(\\chi_p)=\\sqrt{2}\\frac{\\Gamma(\\frac{p+1}{2})}{\\Gamma(\\frac{p}{2})}\n\\]\n\n\n\n\nDe Leeuw, J., and I. Stoop. 1984. “Upper Bounds for Kruskal’s Stress.” Psychometrika 49: 391–402.\n\n\nSpence, I. 1983. “Monte Carlo Studies.” Applied Psychological Measurement 7 (4): 405–25.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Stability of MDS Solutions</span>"
    ]
  },
  {
    "objectID": "global.html",
    "href": "global.html",
    "title": "26  In Search of Global Minima",
    "section": "",
    "text": "26.1 Random Starts\nThe simplest way to get an idea about the local minima of stress in any specific example is to run smacof with multiple random initial configurations. The implementation is simple. Put the smacof runs in a loop from 1 to \\(N\\), start each run with a random initial configuration, and collect the results in some data structure. It is true that our analysis will take \\(N\\) times as long to finish, but just start up your PC and go and do something else while it runs. It seems to me that this ought to be standard practice for actual MDS applications. Not only do we find the best local minimum in terms of stress, but we get valuable information about the stability of our result. If we take, for example, \\(N=1000\\) and we find the same local minimum in all runs, we can be reasonably confident that we have found the global minimum. A small change in the dissimilarities will probably find the same global minimum. If we find multiple local minima, all with about the same frequency and with stress values that are close, then a small change may very well switch to another local minimum with the smallest stress value.\nThere is some freedom in the choice of method to generate random initial configurations. In the examples in this chapter we sample from the standard multivariate normal, but since we know that at local minima \\(\\eta(X)\\leq 1\\) it may be more appropriate to sample from the unit ball in \\(\\mathbb{R}^{n\\times p}\\) (Harman and Lacko (2010)). In fact, since we also know that at a local minimum \\(\\rho(X)=\\eta^2(X)\\), we may project our intial configurations on that surface.\nmetric - nonmetric",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>In Search of Global Minima</span>"
    ]
  },
  {
    "objectID": "global.html#random-starts",
    "href": "global.html#random-starts",
    "title": "26  In Search of Global Minima",
    "section": "",
    "text": "26.1.1 Examples\n\n26.1.1.1 Ekman\nWe use the Ekman data to illustrate this. We use 1000 random starts, and we stop iterating when the decrease in stress is less than 1e-15. We find 8 local minima, listed in table @ref(tab:ekmanminima).\n\n\n\nLocal Minima in Ekman Example\n\n\nno\nminimum\nfrequency\n\n\n\n\n1\n0.0086066\n824\n\n\n2\n0.0182714\n136\n\n\n3\n0.0300985\n19\n\n\n4\n0.0309922\n7\n\n\n5\n0.0317395\n8\n\n\n6\n0.0331013\n4\n\n\n7\n0.0337281\n1\n\n\n8\n0.0379281\n1\n\n\n\n\n\n\n\n\n\n\nEkman Stress Values\n\n\n\n\n\n\n\n\n\nEkman Iteration Counts\n\n\n\n\nThe results are encouraging, because they indicate that, at least in the Ekman example, the lower the local minimum, the more attractive it is for the smacof iterations. The lowest local minima seem to have the largest regions of attraction. Specifically, we find what is presumably the global minimum in more than 80% of the cases. Nevertheless, if our clients were so unwise to start their MDS analysis with a random initial configuration then about 20% of them will get the wrong answer.\nIn the example we also see a number of local minima, found only in a small number of cases, whose stress values are very close to each other. It seems likely that others of roughly the same stress level will be found if we continue sampling additional random initial configurations.\nThe configurations corresponding with the two dominant local minima are in @ref(fig:ekmanlmplot1) and @ref(fig:ekmanlmplot2). if we compare them we see that the color circle has two separate circular segments. In the second local minimum the order of the colors on one of these two segments is reversed.\n\n\n\n\n\nGlobal (?) Minimum\n\n\n\n\n\n\n\n\n\nLargest (?) Non-global Minimum\n\n\n\n\n\n\n26.1.1.2 De Gruijter\nThe Ekman example is somewhat atypical, because it has an exceptionally good fit. We perform the same computations for the De Gruijter example, still using a cut-off at 1e-15, but now allowing for up to 10,000 iterations.\nThe data in this example are averages of preference rankings for nine Dutch political parties by 100 students. Due to the heterogeneity of the population there is a considerable regression to the mean. Typically this would suggest splitting the students into more homogeneous groups (which is what De Gruijter (1967) did), and/or using a form of non-metric scaling (which is what De Gruijter did as well).\nOur 1000 runs produce 21 local minima, in table @ref{tab:gruijterlocalminima}, with stress values that are close to each other. In this case it is easy to imagine that more runs will produce many more local minima, with low frequencies, mainly permuting the political parties on the horseshoe. In other words, the situation is somewhat like the case in which all dissimilarities between the nine objects are equal, in which case we have $9!=$3.6288^{5} local minima, all with the same stress value.\n\n\n\nLocal Minima in De Gruijter Example\n\n\nno\nminimum\nfrequency\n\n\n\n\n1\n0.0222149\n155\n\n\n2\n0.0222262\n85\n\n\n3\n0.0222631\n156\n\n\n4\n0.0223017\n248\n\n\n5\n0.0232310\n130\n\n\n6\n0.0244955\n88\n\n\n7\n0.0245003\n35\n\n\n8\n0.0246216\n2\n\n\n9\n0.0254775\n26\n\n\n10\n0.0254982\n18\n\n\n11\n0.0261988\n18\n\n\n12\n0.0273897\n1\n\n\n13\n0.0274695\n7\n\n\n14\n0.0297550\n4\n\n\n15\n0.0303408\n4\n\n\n16\n0.0315674\n1\n\n\n17\n0.0315679\n1\n\n\n18\n0.0356327\n14\n\n\n19\n0.0362333\n3\n\n\n20\n0.0364538\n3\n\n\n21\n0.0375689\n1\n\n\n\n\n\nAnother comparison may be useful. The Ekman example is like a matrix with two dominant eigenvalues, the De Gruijter example is like a matrix will all eigenvalues approximately equal to each other. Since smacof is somewhat like the power method, we expect poor convergence in the De Gruijter example, and histogram @ref(fig:histgruijter2) shows exactly that. There is even one random start from which there is no convergence in 10,000 iterations. In the Ekman example the frequency of the local minima seems closely related to the stress value at the local minimum, in the De Gruijter example the frequency of the local minims seems more random. In the Ekman case we can be pretty sure we have found the global minimum, in the De Gruijter case we are far from sure.\n\n\n\n\n\nDe Gruijter Stress Values\n\n\n\n\n\n\n\n\n\nDe Gruijter Iteration Count\n\n\n\n\n\nx1 &lt;- hgruijter[[14]]$x\nx3 &lt;- hgruijter[[1]]$x\n\n\n\n\n\n\nLargest Local Minimum\n\n\n\n\n\n\n\n\n\nAnother Local Minimum",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>In Search of Global Minima</span>"
    ]
  },
  {
    "objectID": "global.html#tunneling-filling-annealing-etc.",
    "href": "global.html#tunneling-filling-annealing-etc.",
    "title": "26  In Search of Global Minima",
    "section": "26.2 Tunneling, Filling, Annealing, etc.",
    "text": "26.2 Tunneling, Filling, Annealing, etc.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>In Search of Global Minima</span>"
    ]
  },
  {
    "objectID": "global.html#globcutplanes",
    "href": "global.html#globcutplanes",
    "title": "26  In Search of Global Minima",
    "section": "26.3 Cutting Planes",
    "text": "26.3 Cutting Planes\nIn cutting plane methods we approximate a non-polyhedral compact convex set \\(\\mathcal{C}\\) by a sequence \\(\\mathcal{P}_n\\) of convex polyhedra. Approximation is from the outside, i.e. \\(\\mathcal{C}\\subset\\mathcal{P}_n\\), strictly monotonic, i.e \\(\\mathcal{P}_{n+1}\\subset\\mathcal{P}_n\\), and convergent, i.e. \\(\\lim_{n\\rightarrow\\infty}\\mathcal{P_n}=\\mathcal{C}\\). Under suitable conditions the maximum/minimum \\(f^\\star_n\\) of a function \\(f\\) on \\(\\mathcal{P}_n\\) will converge to \\(f^\\star\\), the maximum/minimum of \\(f\\) on \\(\\mathcal{C}\\).\nIf we are maximizing a convex function on \\(\\mathcal{C}\\) then the maximum on the approximating polyhedron \\(\\mathcal{P}_n\\) will be at one of the vertices \\(x^\\star\\). If \\(x^\\star\\not\\in\\mathcal{C}\\) we cut it off by finding a hyperplane that separates \\(x\\) and \\(\\mathcal{C}\\). We can, for example, project \\(x^\\star\\) on \\(\\mathcal{C}\\) and use the tangent hyperplane to \\(\\mathcal{C}\\) in the projection \\(\\hat x\\). Define \\(\\mathcal{P}_{n+1}\\cap\\{x\\mid a'x\\leq b\\}\\), with \\(a'x=b\\) the separating hyperplane that has \\(a'x\\leq b\\) for each \\(x\\in\\mathcal{C}\\) and \\(a'x^\\star &gt; b\\).\nThe basic MDS problem can be reformulated as maximization of \\(\\rho(x)\\) on the unit ball \\(\\eta^2(x)=x'x\\leq 1\\). See sections @ref(propcoefspace) and @ref(propspherespace) for the reformulation tools. The cutting plane method in the case of a ball is particulary simple, at least conceptually. If \\(x^\\star\\not\\in\\mathcal{C}\\) then its projection on the ball is \\(\\hat x=x^\\star/\\|x^\\star\\|\\) and the tangent hyperplane in \\(\\hat x\\) is \\(\\hat x'x\\leq 1\\).\nComputationally, however, matters are not so simple. The polyhedron \\(\\mathcal{P}_n\\) is described by an increasing number of linear inequalities. Finding all vertices requires a non-trivial effort, and in the general case the method seems practical only for small or moderately small examples. In an actual implementation we would have to have a scheme for dropping or not adding redundant inequalities (that are implied by earlier inequalities) and a scheme for dropping inequalities generating vertices that can never be the global maximum. Such strategfies will depend on the nature of the function \\(f\\) and on the convex set \\(\\mathcal{S}\\).\n\n26.3.1 On the Circle\nFor the circle the cutting plane method can be made very simple. Suppose we start with a \\(n\\) distinct inner points \\(x_1,\\cdots,x_n\\) on the unit circle \\(\\mathcal{S}\\), ordered clockwise so that \\(x_1\\) is at the top of the circle (high noon). Let \\(\\mathcal{Q}_n\\) be their convex hull. Then \\(\\mathcal{Q}_n\\subset\\mathcal{S}\\). The tangent lines at \\(x_i\\) and \\(x_{i+1}\\) intersect outside the circle in an outer point \\(y_i\\), with \\(y_n\\) the intersection of the tangents at \\(x_n\\) and \\(x_1\\). Let \\(\\mathcal{P}_n\\) be the convex hull of the \\(y_i\\). Then \\(\\mathcal{Q}_n\\subset\\mathcal{S}\\subset\\mathcal{P}_n\\) and thus \\[\n\\max_{i=1}^n\\rho(x_i)=\\max_{x\\in\\mathcal{Q}_n}\\rho(x)\\leq\\max_{x\\in\\mathcal{S}}\\rho(x)\\leq\\max_{x\\in\\mathcal{P}_n}\\rho(x)=\\max_{i=1}^n\\rho(y_i).\n\\] Thus we have easily computable lower and upper bounds for the global maximum of \\(\\rho\\) on \\(\\mathcal{S}\\). The next step is to add the \\(n\\) projections \\(y_i/\\|y_i\\|\\) to the \\(n\\) inner points to have a new set of \\(2n\\) inner points. Then compute the corresponding \\(2n\\) outer points, and so on. After \\(k\\) steps we have \\(2^kk_0\\) inner and outer points, where \\(k_0\\) is the number of inner points we started with.\n\n\n\n\n\nCircle Segment\n\n\n\n\nAs figure @ref(fig:circseg) shows, the outer point corresponding with two consecutive points on the circle lies on the perpendicular bisector of the line connecting the points. Using non-consecutive points will produce tangent lines which intersect farther away from the circle, and which consequently leads to a larger convex hull, and a worse approximation.\nThe next three figures illustrate the first iterations of the algorithm. We always start with \\(n\\) inner points equally distributed on the unit circle, in this case \\(n=4\\). The circle is in red, the convex hulls of the outer and inner points are in blue.\n\n\n\n\n\nStarting Point\n\n\n\n\n\n\n\n\n\nAfter First Iteration\n\n\n\n\n\n\n\n\n\nAfter Second Iteration\n\n\n\n\nWe see rapid convergence of the convex hulls to the circle. The figures also suggest an improvement of the method. Suppose \\(\\rho_0\\) is a lower bound of the global minimum \\(\\rho_\\star\\) equal to the largest \\(\\rho\\) value of the inner points. Suppose an outer point has a \\(\\rho\\) value less than or equal to \\(\\rho_0\\), consider the triangle with the outer point and the two inner points. All three vertices have a \\(\\rho\\) value less than or equal to \\(\\rho_0\\), and because \\(\\rho\\) is convex so have all points in the triangle, including a segment of the circle. Thus we can phantom that segment of the circle and create no new inner points there. If \\(\\rho_0\\) get closer to \\(\\rho_\\star\\) more and more segments of the circle are eliminated, which will presumably lead to faster computation. It seems advantageous to start with a value of \\(\\rho_0\\) that is as large as possible, for example by using the value the smacof algorithm converges to. We can then use the inner and outer points to check if the \\(\\rho\\) value is a global minimum.\n\n\n26.3.2 Cauchy Step Size\nThe standard smacof update of \\(X\\) (update method \\(\\text{up}_A\\) of section @ref(accelsimple)) is \\(\\gamma(X)=V^+B(X)X\\). The relaxed update is \\(X(\\lambda):=\\lambda \\gamma(X)+(1-\\lambda)X\\). We usually choose \\(\\lambda=2\\), which gives update method \\(\\text{up}_B\\) of section @ref(accelsimple).\nThe Cauchy or steepest descent update is \\(X(\\hat\\lambda)\\), with \\[\\begin{equation}\n\\hat\\lambda=\\mathop{\\text{argmin}}_{\\lambda\\geq 0}\\sigma(X(\\lambda)).\n(\\#eq:globlbdopt)\n\\end{equation}\\] There are some examples of the use of \\(\\hat\\lambda\\) in De Leeuw and Heiser (1980), but there the optimal step-size is computed by using constrained smacof iterations, which may actually take us to just a local minimum along the line. In this example we use our circle methodology to compute the global minimum.\nNow \\(\\text{tr}\\ X'V\\gamma(X)=\\rho(X)\\) and thus if \\(\\text{tr}\\ X'VX=1\\) then \\(Y=\\gamma(X)-\\rho(X)X\\) satisfies \\(\\text{tr}\\ X'VY=0\\) and \\(\\eta^2(Y)=\\eta^2(\\gamma(X))-\\rho^2(X)\\). Normalize \\(Y\\) such that \\(\\text{tr}\\ Y'VY=1\\), and now maximize \\(\\rho\\) over \\(\\alpha\\) and \\(\\beta\\), i.e. in configuration space maximize \\(\\rho(\\alpha X+\\beta Y)\\), where \\(\\alpha^2+\\beta^2=1\\).\nIn the example we choose \\(X\\) to be a \\(10\\times 2\\) matrix filled with random standard normals, and we start with 10 inner points on the circle. The iterations until convergence are as follows.\n\n\nitel       1 vertices      10 innermax       0.96683250 outermax       1.01451883 \nitel       2 vertices      20 innermax       0.96683250 outermax       0.97744943 \nitel       3 vertices      40 innermax       0.96683250 outermax       0.97008052 \nitel       4 vertices      80 innermax       0.96709009 outermax       0.96794626 \nitel       5 vertices     160 innermax       0.96720000 outermax       0.96739328 \nitel       6 vertices     320 innermax       0.96720681 outermax       0.96726518 \nitel       7 vertices     640 innermax       0.96721856 outermax       0.96722816 \nitel       8 vertices    1280 innermax       0.96721856 outermax       0.96722140 \nitel       9 vertices    2560 innermax       0.96721856 outermax       0.96721949 \nitel      10 vertices    5120 innermax       0.96721876 outermax       0.96721890 \nitel      11 vertices   10240 innermax       0.96721876 outermax       0.96721880 \nitel      12 vertices   20480 innermax       0.96721876 outermax       0.96721877 \nitel      13 vertices   40960 innermax       0.96721876 outermax       0.96721877 \n\n\nThe optimal \\(\\alpha\\) and \\(\\beta\\) are 0.5464817, 0.837471. In configuration space this translates to -1.7919402$ X+\\(2.6770034\\) (X)$.\n\n\n26.3.3 Balls\nIn dimension \\(p&gt;2\\), where \\(\\rho\\) must be maximized on the unit ball in \\(\\mathbb{R}^p\\), matters are not so simple any more. There is no single compelling natural ordering of the points on the sphere or hypersphere, and thus we have to improvise more. We would like to maintain both upper and lower bounds for the global minimum that both keep improving in every iteration.\n\n26.3.3.1 Outer Approximation\nLet’s first discuss a possible initial set of inner and outer points that are more or less regularly spaced inside or outside the unit sphere. For the inner points we can use the vertices of the cross-polytope (or the \\(\\ell_1\\)-ball), which is the set of all \\(x\\) in \\(\\mathbb{R}^n\\) with \\(\\sum_{i=1}^n|x_i|\\leq 1\\). If \\(|x_i|\\leq 1\\) then \\(x_i^2\\leq|x_i|\\) with equality iff \\(x_i\\in\\{-1,0,1\\}\\). Thus \\(x'x\\leq\\sum_{i=1}^n|x_i|\\) and \\(x'x=\\sum_{i=1}^n|x_i|=1\\) iff exactly one of \\(x_i\\) is \\(\\pm 1\\), i.e. there are \\(2n\\) inner points on the sphere.\nFor the outer points we choose the vertices of \\(\\max_{i=1}^n|x_i|\\leq 1\\), or equivalently \\(-1\\leq x_i\\leq +1\\) for all \\(i\\). Thus there are \\(2^n\\) vertices which have \\(x_i=\\pm 1\\) for all \\(i\\).",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>In Search of Global Minima</span>"
    ]
  },
  {
    "objectID": "global.html#distance-smoothing",
    "href": "global.html#distance-smoothing",
    "title": "26  In Search of Global Minima",
    "section": "26.4 Distance Smoothing",
    "text": "26.4 Distance Smoothing\n\\[\nd_{ij}(X,\\epsilon):=\\sqrt{d_{ij}^2(X)+\\epsilon^2}\n\\] \\[\nd_{ij}^\\epsilon(X):=\\sqrt{d_{ij}^2(X)+\\epsilon^2}\n\\] \\[\n\\mathcal{D}d_{ij}^\\epsilon(X)=\\frac{1}{d_{ij}^\\epsilon(X)}A_{ij}X\n\\] \\[\n\\nabla\\sigma_\\epsilon(X)=2(V-B_\\epsilon(X))X\n\\]\n\\[\n\\mathcal{D}^2\\sigma_\\epsilon(X)=\n\\]\n\nIf $\\epsilon\\geq\\max_{i,j}\\delta_{ij}$ then $B(X_\\epsilon)\\lesssim V$ and thus $\\mathcal{D}^2(X_\\epsilon)\\gtrsim 0$ for all $X$ and  $\\sigma_\\epsilon$ is convex.\n\n\\[\n\\mathcal{D}_1d_{ij}(X,\\epsilon)=\\frac{1}{d_{ij}(X,\\epsilon)}A_{ij}X\n\\]\n\\[\n\\nabla\\sigma_\\epsilon(X)=2\\left(V-\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\frac{\\delta_{ij}}{d_{ij}(X_\\epsilon)}A_{ij}\\right)X\n\\] \\[\n\\nabla^2\\sigma_\\epsilon(X)=2V-\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\frac{\\delta_{ij}}{d_{ij}(X_\\epsilon)}A_{ij}X\n\\]",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>In Search of Global Minima</span>"
    ]
  },
  {
    "objectID": "global.html#penalizing-dimensions",
    "href": "global.html#penalizing-dimensions",
    "title": "26  In Search of Global Minima",
    "section": "26.5 Penalizing Dimensions",
    "text": "26.5 Penalizing Dimensions\nIn Shepard (1962a) and Shepard (1962b) an NMDS technique is developed that minimizes a loss function over configurations in full dimensionality \\(n-1\\). In that sense the technique is similar to FDS. Shepard’s iterative process, however, aims to maintain monotonicity between distances and dissimilarities and at the same time concentrate as much of the variation as possible in a small number of dimensions (De Leeuw (2017)).\nLet us explore the idea of concentrating variation in \\(p&lt;n-1\\) dimensions, but use an approach which is quite different from the one used by Shepard. We remain in the FDS framework, but we aim for solutions in \\(p&lt;n-1\\) dimensions by penalizing \\(n-p\\) dimensions of the full configuration, using the classical Courant quadratic penalty function.\nPartition a full configuration \\(Z=\\begin{bmatrix}X&\\mid&Y\\end{bmatrix}\\), with \\(X\\) of dimension \\(n\\times p\\) and \\(Y\\) of dimension \\(n\\times(n-p)\\). Then \\[\\begin{equation}\\label{E:part}   \n\\sigma(Z)=1-\\mathbf{tr}\\ X'B(Z)X - \\mathbf{tr}\\ Y'B(Z)Y+\\frac12 \\mathbf{tr}\\ X'VX+\\frac12 \\mathbf{tr}\\ Y'VY.\n\\end{equation}\\] Also define the penalty term \\[\\begin{equation}\\label{E:tau}\n\\tau(Y)=\\frac12\\mathbf{tr}\\ Y'VY,\n\\end{equation}\\] and penalized stress \\[\\begin{equation}\\label{E:pi}\n\\pi(Z,\\lambda)=\\sigma(Z)+\\lambda\\ \\tau(Y).\n\\end{equation}\\]\nOur proposed method is to minimize penalized stress over \\(Z\\) for a sequence of values \\(0=\\lambda_1&lt;\\lambda_2&lt;\\cdots\\lambda_m\\). For \\(\\lambda=0\\) this is simply the FDS problem, for which we know we can compute the global minimum. For fixed \\(0&lt;\\lambda&lt;+\\infty\\) this is a Penalized FDS or PFDS problem. PFDS problems with increasing values of \\(\\lambda\\) generate a trajectory \\(Z(\\lambda)\\) in configuration space.\nThe general theory of exterior penalty functions, which we review in section XX of this paper, shows that increasing \\(\\lambda\\) leads to an increasing sequence of stress values \\(\\sigma\\) and a decreasing sequence of penalty terms \\(\\tau\\). If \\(\\lambda\\rightarrow+\\infty\\) we approximate the global minimum of the FDS problem with \\(Z\\) of the form \\(Z=\\begin{bmatrix}X&\\mid&0\\end{bmatrix}\\), i.e. of the pMDS problem. This assumes we do actually compute the global minimum for each value of \\(\\lambda\\), which we hope we can do because we start at the FDS global minimum, and we slowly increase \\(\\lambda\\). There is also a local version of the exterior penalty result, which implies that \\(\\lambda\\rightarrow\\infty\\) takes us to a local minimum of pMDS, so there is always the possibility of taking the wrong trajectory to a local minimum of pMDS.\n\n26.5.1 Local Minima\nThe stationary equations of the PFDS problem are solutions to the equations\n\\[\\begin{align}\n(V-B(Z))X&=0,\\\\\n((1+\\lambda)V-B(Z))Y&=0.\n\\end{align}\\]\nWe can easily related stationary points and local minima of the FDS and PFDS problem.\n\n1: If $X$ is a stationary point of the pMDS problem then $Z=[X\\mid 0]$ is a stationary point of the PFDS problem, no matter what $\\lambda$ is. \n\n2: If $Z=[X\\mid 0]$ is a local minimum of the PFDS problem then $X$ is a local minimum of pMDS and \n$(1+\\lambda)V-B(X)\\gtrsim 0$, or $\\lambda\\geq\\|V^+B(X)\\|_\\infty-1$, with $\\|\\bullet\\|_\\infty$ the spectral radius (largest eigenvalue).\n\n\nProof. Part 1 follows by simple substitution in the stationary equations.\nPart 2 follows from the expansion for \\(Z=[X+\\epsilon P\\mid\\epsilon Q]\\). \\[\\begin{multline}\\label{E:expand2}\n\\pi(Z)=\\pi(X)+\\epsilon\\ \\text{tr}\\ P'\\mathcal{D}\\sigma(X)\\ +\\\\+\\frac12\\epsilon^2\\ \\mathcal{D}^2\\sigma(X)(P,P)+\\frac12\\epsilon^2\\ \\text{tr}\\ Q'((1+\\lambda)V-B(X))Q+o(\\epsilon^2).\n\\end{multline}\\] At a local minimum we must have \\(\\mathcal{D}\\sigma(X)=0\\) and \\(\\mathcal{D}^2\\sigma(X)(P,P)\\gtrsim 0\\), which are the necessary conditions for a local minimum of pMDS. We also must have \\(((1+\\lambda)V-B(X))\\gtrsim 0\\).\n\nNote that the conditions in part 2 of theorem @ref(thm:globlocal) are also sufficient for PFDS to have a local minimum at \\([X\\mid 0]\\), provided we eliminate translational and rotational indeterminacy by a suitable reparametrization, as in De Leeuw (1993).\n\n\n26.5.2 Algorithm\nThe smacof algorithm for penalized stress is a small modification of the unpenalized FDS algorithm (ref). We start our iterations for \\(\\lambda_j\\) with the solution for \\(\\lambda_{j-1}\\) (the starting solution for \\(\\lambda_1=0\\) can be completely arbitrary). The update rules for fixed \\(\\lambda\\) are\n\\[\\begin{align}\nX^{(k+1)}&=V^+B(Z^{(k)})X^{(k)},\\\\\nY^{(k+1)}&=\\frac{1}{1+\\lambda}V^+B(Z^{(k)})Y^{(k)}.\n\\end{align}\\]\nThus we compute the FDS update \\(Z^{(k+1)}=V^+B(Z^{(k)})Z^{(k)}\\) and then divide the last \\(n-p\\) columns by \\(1+\\lambda\\).\nCode is in the appendix. Let us analyze a number of examples.\n\n\n26.5.3 Examples\nThis section has a number of two-dimensional and a number of one-dimensional examples. The one-dimensional examples are of interest, because of the documented large number of local minima of stress in the one-dimensional case, and the fact that for small and medium \\(n\\) exact solutions are available (for example, De Leeuw (2005)). By default we use seq(0, 1, length = 101) for \\(\\lambda\\) in most examples, but for some of them we dig a bit deeper and use longer sequences with smaller increments.\nIf for some value of \\(\\lambda\\) the penalty term drops below the small cutoff \\(\\gamma\\), for example 10^{-10}, then there is not need to try larger values of \\(\\lambda\\), because they will just repeat the same result. We hope that result is the global minimum of the 2MDS problem.\nThe output for each example is a table in which we give, the minimum value of stress, the value of the penalty term at the minimum, the value of \\(\\lambda\\), and the number of iterations needed for convergence. Typically we print for the first three, the last three, and some regularly spaced intermediate values of \\(\\lambda\\). Remember that the stress values increase with increasing \\(\\lambda\\), and the penalty values decrease.\nFor two-dimensional examples we plot all two-dimensional configurations, after rotating to optimum match (using the function matchMe() from the appendix). We connect corresponding points for different values of \\(\\lambda\\). Points corresponding to the highest value of \\(\\lambda\\) are labeled and have a different plot symbol. For one-dimensional examples we put 1:n on the horizontal axes and plot the single dimension on the vertical axis, again connecting corresponding points. We label the points corresponding with the highest value of \\(\\lambda\\), and draw horizontal lines through them to more clearly show their order on the dimension.\nThe appendix also has code for the function checkUni(), which we have used to check the solutions in the one dimensional case are indeed local minima. The function checks the necessary condition for a local minimum \\(x=V^+u\\), with \\[\nu_i=\\sum_{j=1}^nw_{ij}\\delta_{ij}\\ \\mathbf{sign}\\ (x_i-x_j).\n\\] It should be emphasized that all examples are just meant to study convergence of penalized FDS. There is no interpretation of the MDS results\n\n26.5.3.1 Chi Squares\nIn this example, of order 10, the \\(\\delta_{ij}\\) are independent draws from a chi-square distribution with two degrees of freedom. There is no structure in this example, everything is random.\n\n\nitel  198 lambda   0.000000 stress 0.175144 penalty 0.321138 \nitel    5 lambda   0.010000 stress 0.175156 penalty 0.027580 \nitel    3 lambda   0.020000 stress 0.175187 penalty 0.025895 \nitel    1 lambda   0.100000 stress 0.175914 penalty 0.015172 \nitel    1 lambda   0.200000 stress 0.177666 penalty 0.004941 \nitel    4 lambda   0.300000 stress 0.178912 penalty 0.000088 \nitel    6 lambda   0.310000 stress 0.178933 penalty 0.000020 \nitel   20 lambda   0.320000 stress 0.178939 penalty 0.000000 \n\n\n\n\n\n10 Chi Squares\n\n\n\n\nIt seems that in this example the first two dimensions of FDS are already close to optimal for 2MDS. This is because the Gower rank of the dissimilarities is only three (or maybe four, the fourth singular value of the FDS solution \\(Z\\) is very small).\n\n\n26.5.3.2 Regular Simplex\nThe regular simplex has all dissimilarities equal to one. We use an example with \\(n=10\\), for which the global minimum (as far as we know) of pMDS with \\(p=2\\) is a configuration with nine points equally spaced on a circle and one point in the center.\n\n\nitel    1 lambda   0.000000 stress 0.000000 penalty 0.400000 \nitel    7 lambda   0.010000 stress 0.000103 penalty 0.375240 \nitel    5 lambda   0.020000 stress 0.000427 penalty 0.360212 \nitel    2 lambda   0.100000 stress 0.009438 penalty 0.255499 \nitel    1 lambda   0.200000 stress 0.031826 penalty 0.152804 \nitel    1 lambda   0.300000 stress 0.059136 penalty 0.079739 \nitel    1 lambda   0.400000 stress 0.081607 penalty 0.038004 \nitel    1 lambda   0.500000 stress 0.097400 penalty 0.015499 \nitel    1 lambda   0.600000 stress 0.106700 penalty 0.004060 \nitel    1 lambda   0.700000 stress 0.109679 penalty 0.000427 \nitel    1 lambda   0.710000 stress 0.109756 penalty 0.000320 \nitel   92 lambda   0.720000 stress 0.109880 penalty 0.000000 \n\n\n\n\n\n\n\n\n\nNext, we look at the regular simplex with \\(n=4\\), for which the global minimum has four points equally spaced on a circle (i.e. in the corners of a square). We use seq(0, 1, length = 101) for the \\(\\lambda\\) sequence.\n\n\nitel    1 lambda   0.000000 stress 0.000000 penalty 0.250000 \nitel    2 lambda   0.010000 stress 0.000033 penalty 0.162166 \nitel    2 lambda   0.020000 stress 0.000158 penalty 0.156683 \nitel    2 lambda   0.100000 stress 0.004569 penalty 0.103311 \nitel    1 lambda   0.200000 stress 0.016120 penalty 0.041571 \nitel    1 lambda   0.300000 stress 0.026039 penalty 0.007635 \nitel    1 lambda   0.330000 stress 0.027355 penalty 0.003658 \nitel    5 lambda   0.340000 stress 0.028272 penalty 0.000944 \nitel   36 lambda   0.350000 stress 0.028595 penalty 0.000000 \n\n\n\n\n\n\n\n\n\nThe solution converges to an equilateral triangle with the fourth point in the centroid. This is a local minimum. What basically happens is that the first two dimensions of the FDS solution are too close to the local minimum. Or, what amounts to the same thing, the Gower rank is too large (it is \\(n-1\\) for a regular simplex) , there is too much variation in the higher dimensions, and as a consequence the first two dimensions of FDS are a bad 2MDS solution. We try to repair this by refining the trajectory, using seq(0, 1, 10001).\n\n\nitel    1 lambda   0.000000 stress 0.000000 penalty 0.250000 \nitel    2 lambda   0.000100 stress 0.000000 penalty 0.166621 \nitel    2 lambda   0.000200 stress 0.000000 penalty 0.166563 \nitel    1 lambda   0.202200 stress 0.028595 penalty 0.000001 \nitel    1 lambda   0.202300 stress 0.028595 penalty 0.000001 \nitel    1 lambda   0.202400 stress 0.028595 penalty 0.000001 \n\n\n\n\n\n\n\n\n\nNow the trajectories move us from what starts out similar to an equilateral triangle to the corners of the square, and thus we do find the global minimum in this way. It is remarkable that we manage to find the square even when we start closer to the triangle with midpoint.\n\n\n26.5.3.3 Intelligence\nThese are correlations between eight intelligence tests, taken from the smacof package. We convert to dissimilarities by taking the negative logarithm of the correlations. As in the chi-square example, the FDS and the 2MDS solution are very similar and the PMDS trajectories are short.\n\n\nitel 2951 lambda   0.000000 stress 0.107184 penalty 7.988384 \nitel    7 lambda   0.010000 stress 0.107560 penalty 0.685654 \nitel    4 lambda   0.020000 stress 0.108528 penalty 0.628538 \nitel    3 lambda   0.030000 stress 0.110045 penalty 0.573208 \nitel    3 lambda   0.040000 stress 0.112449 penalty 0.510730 \nitel    2 lambda   0.050000 stress 0.114714 penalty 0.464650 \nitel    2 lambda   0.060000 stress 0.117623 penalty 0.415037 \nitel    2 lambda   0.070000 stress 0.121095 penalty 0.364536 \nitel    2 lambda   0.080000 stress 0.125010 penalty 0.315023 \nitel    2 lambda   0.090000 stress 0.129226 penalty 0.267831 \nitel    2 lambda   0.100000 stress 0.133589 penalty 0.223898 \nitel    2 lambda   0.110000 stress 0.137944 penalty 0.183868 \nitel    3 lambda   0.120000 stress 0.143921 penalty 0.133739 \nitel    2 lambda   0.130000 stress 0.147473 penalty 0.106166 \nitel    4 lambda   0.140000 stress 0.153215 penalty 0.064499 \nitel    4 lambda   0.150000 stress 0.157159 penalty 0.037735 \nitel    9 lambda   0.160000 stress 0.161434 penalty 0.010337 \nitel   72 lambda   0.170000 stress 0.163122 penalty 0.000000 \n\n\n\n\n\n\n\n\n\nThe singular values of the FDS solution are 1.78e+00, 1.36e+00, 5.94e-01, 1.47e-01, 3.29e-03, 1.87e-16, 4.37e-17, 3.78e-18, which shows that the Gower rank is probably five, but approximately two.\n\n\n26.5.3.4 Countries\nThis is the wish dataset from the smacof package, with similarities between 12 countries. They are converted to dissimilarties by subtracting each of them from seven.\n\n\nitel 1381 lambda   0.000000 stress 4.290534 penalty 98.617909 \nitel    4 lambda   0.010000 stress 4.301341 penalty 36.137074 \nitel    3 lambda   0.020000 stress 4.336243 penalty 34.389851 \nitel    1 lambda   0.100000 stress 5.187917 penalty 23.300775 \nitel    1 lambda   0.200000 stress 7.539228 penalty 11.543635 \nitel    1 lambda   0.300000 stress 9.901995 penalty 4.963372 \nitel    1 lambda   0.400000 stress 11.523357 penalty 1.859569 \nitel    1 lambda   0.500000 stress 12.391692 penalty 0.556411 \nitel    1 lambda   0.590000 stress 12.696493 penalty 0.080144 \nitel    1 lambda   0.600000 stress 12.708627 penalty 0.060113 \nitel  100 lambda   0.610000 stress 12.738355 penalty 0.000000 \n\n\n\n\n\n\n\n\n\nThe singular values of the FDS solution are 4.20e+00, 3.71e+00, 2.67e+00, 1.80e+00, 1.33e+00, 6.64e-01, 5.97e-04, 7.74e-16, 3.18e-16, 2.25e-16, 8.42e-17, 4.53e-18, and the Gower rank is six or seven.\n\n\n26.5.3.5 Dutch Political Parties\nIn 1967 one hundred psychology students at Leiden University judged the similarity of nine Dutch political parties, using the complete method of triads (De Gruijter (1967)). Data were aggregated and converted to dissimilarities. We first print the matrix of dissimilarities.\n\n\n     KVP    PvdA   VVD    ARP    CHU    CPN    PSP    BP     D66   \nKVP  +0.000 +0.209 +0.196 +0.171 +0.179 +0.281 +0.250 +0.267 +0.230\nPvdA +0.209 +0.000 +0.250 +0.210 +0.231 +0.190 +0.171 +0.269 +0.204\nVVD  +0.196 +0.250 +0.000 +0.203 +0.185 +0.302 +0.281 +0.257 +0.174\nARP  +0.171 +0.210 +0.203 +0.000 +0.119 +0.292 +0.250 +0.271 +0.228\nCHU  +0.179 +0.231 +0.185 +0.119 +0.000 +0.290 +0.263 +0.259 +0.225\nCPN  +0.281 +0.190 +0.302 +0.292 +0.290 +0.000 +0.152 +0.236 +0.276\nPSP  +0.250 +0.171 +0.281 +0.250 +0.263 +0.152 +0.000 +0.256 +0.237\nBP   +0.267 +0.269 +0.257 +0.271 +0.259 +0.236 +0.256 +0.000 +0.274\nD66  +0.230 +0.204 +0.174 +0.228 +0.225 +0.276 +0.237 +0.274 +0.000\n\n\nThe trajectories from FDS to 2MDS show some clear movement, especially of the D’66 party, which was new at the time.\n\n\nitel  223 lambda   0.000000 stress 0.000000 penalty 0.414526 \nitel    5 lambda   0.010000 stress 0.000061 penalty 0.196788 \nitel    2 lambda   0.020000 stress 0.000199 penalty 0.190472 \nitel    1 lambda   0.100000 stress 0.004399 penalty 0.136576 \nitel    1 lambda   0.200000 stress 0.015811 penalty 0.075466 \nitel    1 lambda   0.300000 stress 0.028235 penalty 0.036636 \nitel    1 lambda   0.400000 stress 0.038275 penalty 0.012608 \nitel    1 lambda   0.500000 stress 0.043644 penalty 0.002156 \nitel    1 lambda   0.520000 stress 0.044091 penalty 0.001324 \nitel    1 lambda   0.530000 stress 0.044253 penalty 0.001019 \nitel  277 lambda   0.540000 stress 0.044603 penalty 0.000000 \n\n\n\n\n\n\n\n\n\nThere seems to be some bifurcation going on at the end, so we repeat the analysis using seq(0, 1, length = 1001) for \\(\\lambda\\). The results turn out to be basically the same.\n\n\nitel  223 lambda   0.000000 stress 0.000000 penalty 0.414526 \nitel    4 lambda   0.001000 stress 0.000001 penalty 0.204225 \nitel    2 lambda   0.002000 stress 0.000002 penalty 0.203535 \nitel    1 lambda   0.468000 stress 0.044604 penalty 0.000001 \nitel    1 lambda   0.469000 stress 0.044604 penalty 0.000000 \nitel  166 lambda   0.470000 stress 0.044603 penalty 0.000000 \n\n\n\n\n\n\n\n\n\nThe singular values of the FDS solution are 2.95e-01, 2.10e-01, 1.89e-01, 1.34e-01, 1.16e-01, 1.06e-01, 8.61e-02, 7.06e-02, 1.10e-17, and the Gower rank is probably eight. This is mainly because these data, being averages, regress to the mean and thus have a substantial additive constant. If we repeat the analysis after subtracting .1 from all dissimilarities we get basically the same solution, but with somewhat smoother trajectories.\n\n\nitel  511 lambda   0.000000 stress 0.000176 penalty 0.150789 \nitel    2 lambda   0.001000 stress 0.000176 penalty 0.037759 \nitel    2 lambda   0.002000 stress 0.000176 penalty 0.037619 \nitel    1 lambda   0.370000 stress 0.007642 penalty 0.000000 \nitel    1 lambda   0.371000 stress 0.007642 penalty 0.000000 \nitel    1 lambda   0.372000 stress 0.007642 penalty 0.000000 \n\n\n\n\n\n\n\n\n\nNow the singular values of the FDS solution are 2.05e-01, 1.34e-01, 1.11e-01, 6.03e-02, 3.11e-02, 3.97e-04, 1.18e-07, 4.55e-12, 1.16e-17, and the approximate Gower rank is more like five or six.\n\n\n26.5.3.6 Ekman\nThe next example analyzes dissimilarities between 14 colors, taken from Ekman (1954). The original similarities \\(s_{ij}\\), averaged over 31 subjects, were transformed to dissimilarities by \\(\\delta_{ij}=1-s_{ij}\\).\n\n\nitel 1482 lambda   0.000000 stress 0.000088 penalty 0.426110 \nitel    5 lambda   0.010000 stress 0.000132 penalty 0.118988 \nitel    3 lambda   0.020000 stress 0.000253 penalty 0.112777 \nitel    1 lambda   0.100000 stress 0.003195 penalty 0.070791 \nitel    1 lambda   0.200000 stress 0.010778 penalty 0.024407 \nitel    1 lambda   0.300000 stress 0.016125 penalty 0.003230 \nitel    1 lambda   0.400000 stress 0.017142 penalty 0.000165 \nitel    4 lambda   0.500000 stress 0.017213 penalty 0.000000 \nitel    1 lambda   0.600000 stress 0.017213 penalty 0.000000 \nitel    1 lambda   0.610000 stress 0.017213 penalty 0.000000 \nitel    1 lambda   0.620000 stress 0.017213 penalty 0.000000 \nitel    1 lambda   0.630000 stress 0.017213 penalty 0.000000 \n\n\n\n\n\n\n\n\n\nIf we transform the Ekman similarities by \\(\\delta_{ij}=(1-s_{ij})^3\\) then its is known (De Leeuw (2016)) that the Gower rank is equal to two. Thus the FDS solution has rank 2, and the 2MDS solution is the global minimum.\n\n\nitel   99 lambda   0.000000 stress 0.011025 penalty 0.433456 \nitel    1 lambda   0.010000 stress 0.011025 penalty 0.000000 \nitel    1 lambda   0.020000 stress 0.011025 penalty 0.000000 \nitel    1 lambda   0.100000 stress 0.011025 penalty 0.000000 \nitel    1 lambda   0.110000 stress 0.011025 penalty 0.000000 \nitel    1 lambda   0.120000 stress 0.011025 penalty 0.000000 \nitel    1 lambda   0.130000 stress 0.011025 penalty 0.000000 \n\n\n\n\n\n\n\n\n\n\n\n26.5.3.7 Morse in Two\nNext, we use dissimilarities between 36 Morse code signals (Rothkopf (1957)). We used the symmetrized version morse from the smacof package (De Leeuw and Mair (2009)).\n\n\nitel 1461 lambda   0.000000 stress 0.000763 penalty 0.472254 \nitel    6 lambda   0.010000 stress 0.000858 penalty 0.322181 \nitel    4 lambda   0.020000 stress 0.001147 penalty 0.308335 \nitel    1 lambda   0.100000 stress 0.008576 penalty 0.216089 \nitel    1 lambda   0.200000 stress 0.028903 penalty 0.119364 \nitel    1 lambda   0.300000 stress 0.051285 penalty 0.060060 \nitel    1 lambda   0.400000 stress 0.068653 penalty 0.028190 \nitel    1 lambda   0.500000 stress 0.080258 penalty 0.011356 \nitel    1 lambda   0.600000 stress 0.086572 penalty 0.003578 \nitel    1 lambda   0.700000 stress 0.089140 penalty 0.000854 \nitel    1 lambda   0.800000 stress 0.089898 penalty 0.000116 \nitel    1 lambda   0.830000 stress 0.089958 penalty 0.000053 \nitel    1 lambda   0.840000 stress 0.089970 penalty 0.000040 \nitel  197 lambda   0.850000 stress 0.089949 penalty 0.000000 \n\n\n\n\n\n\n\n\n\n\n\n26.5.3.8 Vegetables\nOur first one-dimensional example uses paired comparisons of 9 vegetables, originating with Guilford (1954). The proportions are transformed to dissimilarities by using the absolute values of the normal quantile function, i.e. \\(\\delta_{ij}=|\\Phi^{-1}(p_{ij})|\\). We use a short sequence for \\(\\lambda\\).\n\n\nitel 1412 lambda   0.000000 stress 0.013675 penalty 0.269308 \nitel    5 lambda   0.010000 stress 0.013716 penalty 0.114786 \nitel    5 lambda   0.100000 stress 0.016719 penalty 0.069309 \nitel   23 lambda   1.000000 stress 0.035301 penalty 0.000000 \n\n\n\n\n\n\n\n\n\nThis example was previously analyzed by me in De Leeuw (2005) using enumeration of all permutations. I found 14354 isolated local minima, and a global minimum equal to the one we computed here.\n\n\n26.5.3.9 Plato\n(mair_groenen_deleeuw_A_19?) use seriation of the works of Plato, from the data collected by Cox and Brandwood (1959), as an example of unidimensional scaling. We first run this example with our usual sequence of five \\(\\lambda\\) values.\n\n\nitel  169 lambda   0.000000 stress 0.000000 penalty 0.410927 \nitel    3 lambda   0.010000 stress 0.000062 penalty 0.255246 \nitel    3 lambda   0.100000 stress 0.005117 penalty 0.194993 \nitel    4 lambda   1.000000 stress 0.106058 penalty 0.019675 \nitel    9 lambda  10.000000 stress 0.139462 penalty 0.000000 \n\n\n\n\n\n\n\n\n\nThis gives the order\n\n\n     [,1]       \n[1,] \"Timaeus\"  \n[2,] \"Republic\" \n[3,] \"Critias\"  \n[4,] \"Sophist\"  \n[5,] \"Politicus\"\n[6,] \"Philebus\" \n[7,] \"Laws\"     \n\n\nwhich is different from the order at the global minimum that has Republic before Timaeus. Thus we have recovered a local minimum, and it seems our sequence of \\(\\lambda\\) values was not fine enough to do the job properly. So we try a longer and finer sequence.\n\n\nitel  169 lambda   0.000000 stress 0.000000 penalty 0.410927 \nitel    3 lambda   0.000100 stress 0.000000 penalty 0.263015 \nitel    3 lambda   0.001000 stress 0.000001 penalty 0.262280 \nitel    3 lambda   0.010000 stress 0.000064 penalty 0.255078 \nitel    3 lambda   0.100000 stress 0.005123 penalty 0.194945 \nitel    2 lambda   0.200000 stress 0.016184 penalty 0.147493 \nitel    1 lambda   0.300000 stress 0.026997 penalty 0.119323 \nitel    1 lambda   0.400000 stress 0.040023 penalty 0.093615 \nitel    1 lambda   0.500000 stress 0.053688 penalty 0.072330 \nitel    1 lambda   0.600000 stress 0.066833 penalty 0.055452 \nitel    1 lambda   0.700000 stress 0.078832 penalty 0.042269 \nitel    1 lambda   0.800000 stress 0.089439 penalty 0.032019 \nitel    1 lambda   0.900000 stress 0.098557 penalty 0.024079 \nitel    1 lambda   1.000000 stress 0.106135 penalty 0.017940 \nitel    6 lambda   2.000000 stress 0.130789 penalty 0.000148 \nitel   13 lambda   3.000000 stress 0.131135 penalty 0.000000 \n\n\n\n\n\n\n\n\n\nNow the order is\n\n\n     [,1]       \n[1,] \"Republic\" \n[2,] \"Timaeus\"  \n[3,] \"Critias\"  \n[4,] \"Sophist\"  \n[5,] \"Politicus\"\n[6,] \"Philebus\" \n[7,] \"Laws\"     \n\n\nwhich does indeed correspond to the global minimum.\nWith a different \\(\\lambda\\) sequence we find the same solution.\n\n\nitel  169 lambda   0.000000 stress 0.000000 penalty 0.410927 \nitel    3 lambda   0.001000 stress 0.000001 penalty 0.262296 \nitel    2 lambda   0.002000 stress 0.000003 penalty 0.261483 \nitel    2 lambda   0.004000 stress 0.000010 penalty 0.259872 \nitel    2 lambda   0.008000 stress 0.000041 penalty 0.256690 \nitel    2 lambda   0.016000 stress 0.000159 penalty 0.250470 \nitel    2 lambda   0.032000 stress 0.000613 penalty 0.238574 \nitel    2 lambda   0.064000 stress 0.002266 penalty 0.216785 \nitel    2 lambda   0.128000 stress 0.007791 penalty 0.180067 \nitel    2 lambda   0.256000 stress 0.023483 penalty 0.127006 \nitel    2 lambda   0.512000 stress 0.056940 penalty 0.067948 \nitel    3 lambda   1.024000 stress 0.107743 penalty 0.017937 \nitel    8 lambda   2.048000 stress 0.131059 penalty 0.000032 \nitel    9 lambda   4.096000 stress 0.131135 penalty 0.000000 \n\n\n\n\n\n\n\n\n\nThe order is\n\n\n     [,1]       \n[1,] \"Republic\" \n[2,] \"Timaeus\"  \n[3,] \"Critias\"  \n[4,] \"Sophist\"  \n[5,] \"Politicus\"\n[6,] \"Philebus\" \n[7,] \"Laws\"     \n\n\n\n\n26.5.3.10 Morse in One\nNow for a more challenging example. The Morse code data have been used to try out exact unidimensional MDS techniques, for example by Palubeckis (2013). We will enter the global minimum contest by using 10,000 values of \\(\\lambda\\), in an equally spaced sequence from 0 to 10. This is not as bad as it sounds. For the 10,000 FDS solutions system.time() tells us\n\n\n   user  system elapsed \n  4.901   0.233   5.139 \n\n\nThe one-dimensional plot show quite a bit of movement, but much of it seems to be contained in the very first change of \\(\\lambda\\).\n\n\n\n\n\n\n\n\n\nWe can also plot stress and the penalty term as functions of \\(\\lambda\\). Again, note the big change in the penalty term when \\(\\lambda\\) goes from zero to 0.001.\n\n\n\n\n\n\n\n\n\nAfter the first 2593 values of \\(\\lambda\\) the penalty term is zero and we stop, i.e. we estimate \\(\\lambda_+\\) is 2.593. At that point we have run a total of 5013 FDS iterations, and thus on average about two iterations per \\(\\lambda\\) value. Stress has increased from 0.0007634501 to 0.2303106976 and the penalty value has decreased from 0.4815136419 to 0.0000000001. We find the following order of the points on the dimension.\n\n\n      [,1]   \n [1,] \".\"    \n [2,] \"-\"    \n [3,] \"..\"   \n [4,] \".-\"   \n [5,] \"-.\"   \n [6,] \"--\"   \n [7,] \"...\"  \n [8,] \"..-\"  \n [9,] \".-.\"  \n[10,] \".--\"  \n[11,] \"....\" \n[12,] \"-..\"  \n[13,] \"-.-\"  \n[14,] \"...-\" \n[15,] \".....\"\n[16,] \"....-\"\n[17,] \"..-.\" \n[18,] \".-..\" \n[19,] \"-...\" \n[20,] \"-..-\" \n[21,] \"-....\"\n[22,] \"...--\"\n[23,] \"-.-.\" \n[24,] \"-.--\" \n[25,] \"--...\"\n[26,] \"--..\" \n[27,] \"--.-\" \n[28,] \".--.\" \n[29,] \".---\" \n[30,] \"--.\"  \n[31,] \"---\"  \n[32,] \"..---\"\n[33,] \"---..\"\n[34,] \".----\"\n[35,] \"----.\"\n[36,] \"-----\"\n\n\nOur order, and consequently our solution, is the same as the exact global solution given by Palubeckis (2013). See his table 4, reproduced below. The difference is that computing our solution takes 10 seconds, while his takes 494 seconds. But of course we would not have known we actually found the global mimimum if the exact exhaustive methods had not analyzed the same data as well.\n\n\n\n\nCox, D. R., and L. Brandwood. 1959. “On a Discriminatory Problem Connected with the Works of Plato.” Journal of the Royal Statistical Society, Series B 21: 195–200.\n\n\nDe Gruijter, D. N. M. 1967. “The Cognitive Structure of Dutch Political Parties in 1966.” Report E019-67. Psychological Institute, University of Leiden.\n\n\nDe Leeuw, J. 1993. “Fitting Distances by Least Squares.” Preprint Series 130. Los Angeles, CA: UCLA Department of Statistics. https://jansweb.netlify.app/publication/deleeuw-r-93-c/deleeuw-r-93-c.pdf.\n\n\n———. 2005. “Unidimensional Scaling.” In The Encyclopedia of Statistics in Behavioral Science, edited by B. S. Everitt and D. Howell, 4:2095–97. New York, N.Y.: Wiley.\n\n\n———. 2016. “Gower Rank.” 2016. https://jansweb.netlify.app/publication/deleeuw-e-16-k/deleeuw-e-16-k.pdf.\n\n\n———. 2017. “Shepard Non-metric Multidimensional Scaling.” 2017. https://jansweb.netlify.app/publication/deleeuw-e-17-e/deleeuw-e-17-e.pdf.\n\n\nDe Leeuw, J., and W. J. Heiser. 1980. “Multidimensional Scaling with Restrictions on the Configuration.” In Multivariate Analysis, Volume V, edited by P. R. Krishnaiah, 501–22. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\nDe Leeuw, J., and P. Mair. 2009. “Multidimensional Scaling Using Majorization: SMACOF in R.” Journal of Statistical Software 31 (3): 1–30. https://www.jstatsoft.org/article/view/v031i03.\n\n\nEkman, G. 1954. “Dimensions of Color Vision.” Journal of Psychology 38: 467–74.\n\n\nGuilford, J. P. 1954. Psychometric Methods. McGraw-Hill.\n\n\nHarman, R., and V. Lacko. 2010. “On Decompositional aAgorithms for Uniform Sampling from n-Spheres and n-Balls.” Journal of Multivariate Analysis 101: 2297–2304.\n\n\nPalubeckis, G. 2013. “An Improved Exact Algorithm for Least-Squares Unidimensional Scaling.” Journal of Applied Mathematics, 1–15.\n\n\nPliner, V. 1996. “Metric Unidimensional Scaling and Global Optimization.” Journal of Classification 13: 3–18.\n\n\nRothkopf, E. Z. 1957. “A Measure of Stimulus Similarity and Errors in some Paired-associate Learning.” Journal of Experimental Psychology 53: 94–101.\n\n\nShepard, R. N. 1962a. “The Analysis of Proximities: Multidimensional Scaling with an Unknown Distance Function. I.” Psychometrika 27: 125–40.\n\n\n———. 1962b. “The Analysis of Proximities: Multidimensional Scaling with an Unknown Distance Function. II.” Psychometrika 27: 219–46.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>In Search of Global Minima</span>"
    ]
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "27  Software",
    "section": "",
    "text": "In actual computer output using the scaling in formula @ref(eq:scaldiss1) and @ref(eq:scaldiss1) has some disadvantages. There are, say, \\(M\\) non-zero weights. The summation in #ref(eq:stressall) is really over \\(M\\) terms only. If \\(n\\) is at all large the scaled dissimilarities, and consequently the distances and the configuration, will become very small. Thus, in actual computation, or at least in the computer output, we scale our dissimilarities as \\(\\frac12\\mathop{\\sum\\sum}_{1\\leq j&lt;i\\leq n} w_{ij}^{\\ }\\delta_{ij}^2=M\\). So, we scale our dissimilarities to one in formulas and to \\(M\\) in computations. Thus the computed stress will b\nrcode\nccode\nlib",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Software</span>"
    ]
  },
  {
    "objectID": "backmatter.html",
    "href": "backmatter.html",
    "title": "(APPENDIX) Appendices",
    "section": "",
    "text": "Code",
    "crumbs": [
      "(APPENDIX) Appendices"
    ]
  },
  {
    "objectID": "backmatter.html#rapcode",
    "href": "backmatter.html#rapcode",
    "title": "(APPENDIX) Appendices",
    "section": "R Code",
    "text": "R Code\nThe MDS functions in R throughout work with square matrices of weights, dissimilarities, and distances. More efficient versions, that have their computations done in C, will be added along the way.\n\nutilities.R\n\nsource (\"common/indexing.R\")\nsource (\"common/io.R\")\nsource (\"common/linear.R\")\nsource (\"common/nextPC.R\")\nsource (\"common/decode.R\")\nsource (\"common/smacof.R\")\n\n\n\ncommon/indexing.r\n\nkron &lt;- function (i, j) {\n  return (ifelse (i == j, 1, 0))\n}\n\nein &lt;- function (i, n) {\n  return (ifelse (i == 1:n, 1, 0))\n}\n\naijn &lt;- function (i, j, n) {\n  dif &lt;- ein (i, n) - ein (j, n)\n  return (outer (dif, dif))\n}\n\njmat &lt;- function (n) {\n  return (diag(n) - 1 / n)\n}\n\nccen &lt;- function (x) {\n  return (apply (x, 2, function (y)\n    y - mean (y)))\n}\n\nrepList &lt;- function(x, n) {\n  z &lt;- list()\n  for (i in 1:n)\n    z &lt;- c(z, list(x))\n  return(z)\n}\n\nrcen &lt;- function (x) {\n  return (t (apply (x, 1, function (y)\n    y - mean (y))))\n}\n\ndcen &lt;- function (x) {\n  return (ccen (rcen (x)))\n}\n\nwdef &lt;- function (n) {\n  return (1 - diag (n))\n}\n\n\nlower_triangle &lt;- function (x) {\n  n &lt;- nrow (x)\n  return (x[outer(1:n, 1:n, \"&gt;\")])\n}\n\nfill_symmetric &lt;- function (x) {\n  m &lt;- length (x)\n  n &lt;- (1 + sqrt (1 + 8 * m)) / 2\n  d &lt;- matrix (0, n, n)\n  d[outer(1:n, 1:n, \"&gt;\")] &lt;- x\n  return (d + t(d))\n}\n\n\n\ncommon/io.r\n\nmatrixPrint &lt;- function (x,\n                    digits = 6,\n                    width = 8,\n                    format = \"f\",\n                    flag = \"+\") {\n  print (noquote (formatC (\n    x,\n    digits = digits,\n    width = width,\n    format = format,\n    flag = flag\n  )))\n}\n\niterationWrite &lt;- function (labels, values, digits, width, format) {\n  for (i in 1:length(labels)) {\n    cat (labels[i],\n         formatC(\n           values[i],\n           di = digits[i],\n           wi = width[i],\n           fo = format[i]\n         ),\n         \" \")\n  }\n  cat(\"\\n\")\n}\n\nrotateEllipse &lt;- function (x) {\n  z &lt;- (x[1,] + x[2,]) / 2\n  x &lt;- x - matrix (z, nrow(x), 2, byrow = TRUE)\n  s &lt;- sqrt (sum (x[1,] ^ 2))\n  r &lt;- matrix(c(x[1, 1], x[1, 2],-x[1, 2], x[1, 1]), 2, 2) / s\n  x &lt;- x %*% r\n  e &lt;- as.matrix (dist (x))\n  d &lt;- mean (rowSums(e[-(1:2), 1:2]))\n  a &lt;- d / 2\n  c &lt;- abs (x[1, 1])\n  b &lt;- sqrt (a ^ 2 - c ^ 2)\n  return (list(\n    x = x,\n    a = a,\n    b = b,\n    c = c\n  ))\n}\n\nplotEllipse &lt;- function (x) {\n  r &lt;- rotateEllipse (x)\n  f &lt;- seq (0, 2 * pi, length = 100)\n  z &lt;- cbind(sin (f), cos (f))\n  z[, 1] &lt;- z[, 1] * r$a\n  z[, 2] &lt;- z[, 2] * r$b\n  plot(z,\n       type = \"l\",\n       col = \"RED\",\n       lwd = 2)\n  text (r$x, as.character (1:nrow(r$x)))\n  abline(h = 0)\n  abline(v = 0)\n}\n\ndraw_ellipse &lt;- function (center,\n                          radius,\n                          a = diag (2),\n                          np = 100,\n                          ...) {\n  par (pty = \"s\")\n  e &lt;- eigen(a)\n  k &lt;- e$vectors\n  lbd &lt;- e$values\n  seq &lt;- seq(0, 2 * pi, length = np)\n  scos &lt;- (radius * sin (seq)) / lbd[1]\n  ccos &lt;- (radius * cos (seq)) / lbd[2]\n  sico &lt;- k %*% rbind(scos, ccos) + center\n  plot (sico[1,], sico[2,], type = \"l\", ...)\n}\n\n\n\ncommon/linear.r\n\n## invert with pivot\n## invert with bordering\n## solve wth bordering\n## solve with pivoting\n## jacobi\n## QR\n\ngramy &lt;- function (y, v) {\n  r &lt;- length (y)\n  s &lt;- sum (y[[1]] * (v %*% y[[1]]))\n  y[[1]] &lt;- y[[1]] / sqrt (s)\n  for (j in 2:r) {\n    for (i in 1:(j - 1)) {\n      s &lt;- sum (y[[i]] * (v %*% y[[j]]))\n      y[[j]] &lt;- y[[j]] - s * y[[i]]\n    }\n    s &lt;- sum (y[[j]] * v %*% y[[j]])\n    y[[j]] &lt;- y[[j]] / sqrt (s)\n  }\n  return (y)\n}\n\n\n\nhinv &lt;- function(x) {\n  return (apply (x, c(1, 2), function (a)\n    ifelse (a == 0, 0, 1 / a)))\n}\n\n\ncircular &lt;- function (n) {\n  x &lt;- seq (0, 2 * pi, length = n + 1)\n  z &lt;- matrix (0, n + 1, 2)\n  z[, 1] &lt;- sin (x)\n  z[, 2] &lt;- cos (x)\n  return (z[-1, ])\n}\n\ndirect_sum &lt;- function (x) {\n  n &lt;- length (x)\n  nr &lt;- sapply (x, nrow)\n  nc &lt;- sapply (x, ncol)\n  s &lt;- matrix (0, sum (nr), sum (nc))\n  k &lt;- 0\n  l &lt;- 0\n  for (j in 1:n) {\n    s[k + (1:nr[j]), l + (1:nc [j])] &lt;- x[[j]]\n    k &lt;- k + nr[j]\n    l &lt;- l + nc[j]\n  }\n  return (s)\n}\n\n\n\ncommon/nextPC.r\n\nnextPermutation &lt;- function (x) {\n  if (all (x == (length(x):1)))\n    return (NULL)\n  z &lt;- .C(\"nextPermutation\", as.integer(x), as.integer(length(x)))\n  return (z[[1]])\n}\n\nnextCombination &lt;- function (x, n) {\n  m &lt;- length (x)\n  if (all (x == ((n - m) + 1:m)))\n    return (NULL)\n  z &lt;-\n    .C(\"nextCombination\",\n       as.integer(n),\n       as.integer (m),\n       as.integer(x))\n  return (z[[3]])\n}\n\n\n\ncommon/smacof.r\n\nsmacofNormW &lt;- function(w) {\n  return (w / sum(w))\n}\n\nsmacofNormDelta &lt;- function(w, delta) {\n  return (delta / sqrt(sum (w * delta ^ 2)))\n}\n\nsmacofNormXD &lt;- function(w, delta, xold) {\n  x &lt;- apply (xold, 2, function(x)\n    x - mean(x))\n  d &lt;- as.matrix(dist(x))\n  s &lt;- sum (w * delta * d) / sum (w * d ^ 2)\n  return (list(x = x * s, d = d * s))\n}\n\nsmacofLossR &lt;- function (d, w, delta) {\n  return (sum (w * (delta - d) ^ 2) / 2)\n}\n\nsmacofBmatR &lt;- function (d, w, delta) {\n  dd &lt;- ifelse (d == 0, 0, 1 / d)\n  b &lt;- -dd * w * delta\n  diag (b) &lt;- -rowSums (b)\n  return(b)\n}\n\nsmacofVmatR &lt;- function (w) {\n  v &lt;- -w\n  diag(v) &lt;- -rowSums(v)\n  return (v)\n}\n\nsmacofGuttmanR &lt;- function (x, b, vinv) {\n  return (vinv %*% b %*% x)\n}\n\nsmacofGradientR &lt;- function (x, b, v) {\n  return (2 * ((v - b) %*% x))\n}\n\nsmacofHmatR &lt;- function (x, b, v, d, w, delta) {\n  n &lt;- nrow (x)\n  p &lt;- ncol (x)\n  r &lt;- n * p\n  h &lt;- matrix (0, r, r)\n  dd &lt;- ifelse (d == 0, 0, 1 / d)\n  cc &lt;- w * delta * (dd ^ 3)\n  for (s in 1:p) {\n    ns &lt;- (s - 1) * n + 1:n\n    for (t in 1:s) {\n      nt &lt;- (t - 1) * n + 1:n\n      cst &lt;- matrix (0, n, n)\n      for (i in 1:n) {\n        for (j in 1:n) {\n          cst[i, j] &lt;- cc[i, j] * (x[i, s] - x[j, s]) * (x[i, t] - x[j, t])\n        }\n      }\n      cst &lt;- -cst\n      diag(cst) &lt;- -rowSums(cst)\n      if (s == t) {\n        h[ns, ns] &lt;- b - cst\n      } else {\n        h[ns, nt] &lt;- -cst\n        h[nt, ns] &lt;- -cst\n      }\n    }\n  }\n  return (h)\n}\n\nsmacofHessianR &lt;- function (x, b, v, d, w, delta) {\n  n &lt;- nrow (x)\n  p &lt;- ncol (x)\n  h &lt;- -smacofHmatR (x, b, v, d, w, delta)\n  for (s in 1:p) {\n    nn &lt;- (s - 1) * n + 1:n\n    h[nn, nn] &lt;- h[nn, nn] + v\n  }\n  return(h)\n}\n\nsmacofDerGuttmanR &lt;- function(x, b, vinv, d, w, delta) {\n  n &lt;- nrow (x)\n  p &lt;- ncol (x)\n  h &lt;- smacofHmatR (x, b, v, d, w, delta)\n  for (s in 1:p) {\n    ns &lt;- (s - 1) * n + 1:n\n    for (t in 1:s) {\n      nt &lt;- (t - 1) * n + 1:n\n      h[ns, nt] &lt;- vinv %*% h[ns, nt]\n    }\n  }\n  return(h)\n}\n\nsmacofInitialR &lt;- function (delta, p) {\n  n &lt;- nrow(delta)\n  delta &lt;- delta ^ 2\n  rw &lt;- rowSums (delta) / n\n  sw &lt;- sum (delta) / (n ^ 2)\n  h &lt;- -(delta - outer (rw, rw, \"+\") + sw) / 2\n  e &lt;- eigen (h)\n  ea &lt;- e$values\n  ev &lt;- e$vector\n  ea &lt;- ifelse (ea &gt; 0, sqrt (abs(ea)), 0)[1:p]\n  return (ev[, 1:p] %*% diag (ea))\n}\n\nsmacofRandomStart &lt;- function (w, delta, n, p) {\n  x &lt;- matrix(rnorm(n * p), n, p)\n  x &lt;- apply (x, 2, function(x)\n    x - mean(x))\n  d &lt;- as.matrix(dist (x))\n  a &lt;- sum (w * delta * d) / sum (w * d ^ 2)\n  return (a * x)\n}\n\nsmacofVinvR &lt;- function (v) {\n  e &lt;- 1 / nrow(v)\n  return (solve (v + e) - e)\n}\n\nsmacofR &lt;-\n  function (w,\n            delta,\n            p,\n            xold = smacofInitialR(delta, p),\n            xstop = FALSE,\n            itmax = 1000,\n            eps = 1e-10,\n            verbose = TRUE) {\n    labels = c(\"itel\", \"eiff\", \"sold\", \"snew\")\n    digits = c(4, 10, 10, 10)\n    widths = c(6, 15, 15, 15)\n    format = c(\"d\", \"f\", \"f\", \"f\")\n    n &lt;- dim(delta)[1]\n    itel &lt;- 1\n    w &lt;- smacofNormW(w)\n    delta &lt;- smacofNormDelta(w, delta)\n    xdold &lt;- smacofNormXD(w, delta, xold)\n    xold &lt;- xdold$x\n    dold &lt;- xdold$d\n    sold &lt;- smacofLossR (dold, w, delta)\n    bold &lt;- smacofBmatR (dold, w, delta)\n    vmat &lt;- smacofVmatR (w)\n    vinv &lt;- smacofVinvR (vmat)\n    repeat {\n      xnew &lt;- smacofGuttmanR (xold, bold, vinv)\n      dnew &lt;- as.matrix (dist (xnew))\n      bnew &lt;- smacofBmatR (dnew, w, delta)\n      snew &lt;- smacofLossR (dnew, w, delta)\n      if (xstop) {\n        eiff &lt;- max (abs (xold - xnew))\n      } else {\n        eiff &lt;- sold - snew\n      }\n      if (verbose) {\n        values = c(itel, eiff, sold, snew)\n        iterationWrite (labels, values, digits, widths, format)\n      }\n      if ((eiff &lt; eps) || (itel == itmax)) {\n        break\n      }\n      itel &lt;- itel + 1\n      xold &lt;- xnew\n      bold &lt;- bnew\n      dold &lt;- dnew\n      sold &lt;- snew\n    }\n    return (\n      list (\n        x = xnew,\n        d = dnew,\n        b = bnew,\n        g = smacofGradientR(xnew, bnew, vmat),\n        h = smacofHessianR(xnew, bnew, vmat, dnew, w, delta),\n        s = snew,\n        itel = itel\n      )\n    )\n  }\n\n\n\nproperties.R\n\ncsupper &lt;- function (lbd, mbd) {\n  n &lt;- length (lbd)\n  m &lt;- length (mbd)\n  mad &lt;- nad &lt;- matrix (0, m, n)\n  sad &lt;- rep(0, n)\n  plot(lbd,\n       lbd,\n       type = \"n\",\n       ylab = \"\",\n       ylim = c(0, 2))\n  for (k in 1:m) {\n    ly &lt;- mbd[k]\n    yy &lt;- ly * z1 + (1 - ly) * z2\n    dy &lt;- dist (yy)\n    by &lt;- as.matrix(-delta / dy)\n    diag (by) &lt;- -rowSums(by)\n    for (l in 1:n) {\n      xx &lt;- lbd[l] * z1 + (1 - lbd[l]) * z2\n      dx &lt;- dist (xx)\n      rx &lt;- sum (xx * (by %*% yy))\n      nx &lt;- sum (dx ^ 2)\n      mad[k, l] &lt;- 1 - 2 * rx + nx\n    }\n    lines (lbd, mad [k,])\n    abline (v = ly)\n  }\n  lines(lbd,\n        apply(mad, 2, min),\n        type = \"l\",\n        col = \"BLUE\",\n        lwd = 3)\n  for (l in 1:n) {\n    xx &lt;- lbd[l] * z1 + (1 - lbd[l]) * z2\n    dx &lt;- dist (xx)\n    sad[l] &lt;- 1 - 2 * sum (dx * delta) + sum (dx ^ 2)\n  }\n  lines (lbd,\n         sad,\n         type = \"l\",\n         col = \"RED\",\n         lwd = 3)\n}\n\naglower &lt;- function (lbd, mbd) {\n  n &lt;- length (lbd)\n  m &lt;- length (mbd)\n  mad &lt;- matrix (0, m, n)\n  sad &lt;- rep(0, n)\n  plot(lbd,\n       lbd,\n       type = \"n\",\n       ylab = \"\",\n       ylim = c(0, 2))\n  for (k in 1:m) {\n    ly &lt;- mbd[k]\n    yy &lt;- ly * z1 + (1 - ly) * z2\n    dy &lt;- dist (yy)\n    ry &lt;- sum (delta * dy)\n    by &lt;- as.matrix(-delta / dy)\n    diag (by) &lt;- -rowSums(by)\n    for (l in 1:n) {\n      xx &lt;- lbd[l] * z1 + (1 - lbd[l]) * z2\n      dx &lt;- dist (xx)\n      sx &lt;- sum (xx * (by %*% xx))\n      nx &lt;- sum (dx ^ 2)\n      mad[k, l] &lt;- 1 - ry + nx - sx\n    }\n    lines (lbd, mad [k,])\n    abline (v = ly)\n  }\n  lines(lbd,\n        apply(mad, 2, max),\n        type = \"l\",\n        col = \"BLUE\",\n        lwd = 3)\n  for (l in 1:n) {\n    xx &lt;- lbd[l] * z1 + (1 - lbd[l]) * z2\n    dx &lt;- dist (xx)\n    sad[l] &lt;- 1 - 2 * sum (dx * delta) + sum (dx ^ 2)\n  }\n  lines (lbd,\n         sad,\n         type = \"l\",\n         col = \"RED\",\n         lwd = 3)\n}\n\n\n\nexpandOneDim.R\n\nexpandRho &lt;- function (delta, w = wdef(nrow(delta)), x, y) {\n  n &lt;- nrow(delta)\n  s0 &lt;- s1 &lt;- s2 &lt;- s3 &lt;- 0\n  for (i in 1:n) {\n    for (j in 1:n) {\n      if (i == j)\n        next\n      del &lt;- delta[i, j]\n      www &lt;- w[i, j]\n      dxx &lt;- sum((x[i, ] - x[j, ]) ^ 2)\n      dxr &lt;- ifelse (dxx &lt; 1e-15, 1, dxx)\n      dsx &lt;- sqrt(dxr)\n      dyy &lt;- sum((y[i, ] - y[j, ]) ^ 2)\n      dxy &lt;- sum((x[i, ] - x[j, ]) * (y[i, ] - y[j, ]))\n      vxy &lt;- dyy - ((dxy) ^ 2) / dxr\n      s0 &lt;- s0 + www * del * dsx\n      s1 &lt;- s1 + www * (del / dsx) * dxy\n      s2 &lt;- s2 + www * (del / dsx) * vxy\n      s3 &lt;- s3 + www * dxy * (del / (dsx ^ 3)) * vxy\n    }\n  }\n  return(c(s0, s1, s2 / 2, -s3 / 2))\n}\n\nexpandEta2 &lt;- function (delta, w = wdef(nrow(delta)), x, y) {\n  n &lt;- nrow(delta)\n  s0 &lt;- s1 &lt;- s2 &lt;- s3 &lt;- 0\n  for (i in 1:n) {\n    for (j in 1:n) {\n      dxx &lt;- sum((x[i, ] - x[j, ]) ^ 2)\n      dsx &lt;- sqrt(dxx)\n      dyy &lt;- sum((y[i, ] - y[j, ]) ^ 2)\n      dxy &lt;- sum((x[i, ] - x[j, ]) * (y[i, ] - y[j, ]))\n      s0 &lt;- s0 + w[i, j] * dxx\n      s1 &lt;- s1 + w[i, j] * dxy\n      s2 &lt;- s2 + w[i, j] * dyy\n    }\n  }\n  return(c(s0, 2 * s1, s2, s3))\n}\n\nexpandStress &lt;- function (delta, w = wdef(nrow(delta)), x, y) {\n  return (c(1, 0, 0, 0) + expandEta2(delta, w, x, y) - 2 * expandRho(delta, w, x, y))\n}\n\nexpandTester &lt;- function (delta,\n                    w = wdef(nrow(delta)),\n                    x,\n                    y,\n                    left = -1,\n                    right = 1,\n                    length = 1001,\n                    order = 3) {\n  w &lt;- w / sum (w)\n  delta &lt;- delta / sqrt(sum(w * delta ^ 2))\n  x &lt;- apply(x, 2, function (x)\n    x - mean(x))\n  y &lt;- apply(y, 2, function (x)\n    x - mean(x))\n  d &lt;- as.matrix(dist(x))\n  s &lt;- sum(w * d * delta) / sum(w * d * d)\n  d &lt;- s * d\n  x &lt;- s * x\n  h &lt;- expandStress (delta, w, x, y)\n  SEQ &lt;- seq(left, right, length = length)\n  sig &lt;- rep(0, length)\n  sag &lt;- rep(h[1], length)\n  for (i in 1:length) {\n    z &lt;- x + SEQ[i] * y\n    d &lt;- as.matrix (dist(z))\n    eta2 &lt;- sum(w * d ^ 2)\n    rho &lt;- sum(w * delta * d)\n    sig[i] &lt;- 1 - 2 * rho + eta2\n    if (order &gt; 0) {\n      sag[i] &lt;- sag[i] + SEQ[i] * h[2]\n    }\n    if (order &gt; 1) {\n      sag[i] &lt;- sag[i] + (SEQ[i] ^ 2) * h[3]\n    }\n    if (order &gt; 2) {\n      sag[i] &lt;- sag[i] + (SEQ[i] ^ 3) * h[4]\n    }\n  }\n  return(cbind(sig, sag))\n}\n\n\n\npictures.R\n\ndommy &lt;- function () {\n  ya &lt;- matrix(c(1, -1, 1, -1, 0, 1, 1, -1, -1, 0), 5, 2)\n  yy &lt;- seq (0, 2 * pi, length = 6)[1:5]\n  yb &lt;- cbind (sin (yy), cos (yy))\n  ya &lt;- apply(ya, 2, function (x)\n    x - mean(x))\n  yb &lt;- apply(yb, 2, function (x)\n    x - mean(x))\n  y1 &lt;- ya / sqrt (5 * sum (ya ^ 2))\n  y2 &lt;- yb / sqrt (5 * sum (yb ^ 2))\n  deq &lt;- as.dist (1 - diag(5))\n  deq &lt;- deq / sqrt (sum (deq ^ 2))\n  y1 &lt;- sum (dist (y1) * deq) * y1\n  y2 &lt;- sum (dist (y2) * deq) * y2\n}\n\ntwostress &lt;- function (deq, y1, y2, a, b) {\n  d &lt;- dist (a * y1 + b * y2)\n  eta2 &lt;- sum (d ^ 2)\n  rho &lt;- sum (d * deq)\n  stress &lt;- 1 - 2 * rho + eta2\n  return(list(\n    eta2 = eta2,\n    rho = rho,\n    stress = stress\n  ))\n}\n\nzeroes &lt;- function (y1, y2) {\n  n &lt;- nrow (y1)\n  for (i in 2:n) {\n    for (j in 1:(i - 1)) {\n      yy &lt;- rbind (y1[i, ] - y1[j, ], y2[i, ] - y2[j, ])\n      ee &lt;- eigen(tcrossprod(yy))$values\n      print (c(i, j, ee))\n    }\n  }\n}\n\npairme &lt;- function (x, y) {\n  n &lt;- nrow(x)\n  m &lt;- ncol(x)\n  z &lt;- matrix (0, 2, m)\n  for (i in 2:n) {\n    for (j in 1:(i - 1)) {\n      z[1,] &lt;- x[i,] - x[j,]\n      z[2,] &lt;- y[i,] - y[j,]\n      s &lt;- svd (z)\n      a &lt;- s$d[2]\n      b &lt;- s$u[, 2]\n      if (a &lt; 1e-10) {\n        cat (\n          formatC(\n            i,\n            format = \"d\",\n            digits = 2,\n            width = 4\n          ),\n          formatC(\n            j,\n            format = \"d\",\n            digits = 2,\n            width = 4\n          ),\n          formatC(\n            c(a, b),\n            format = \"f\",\n            digits = 6,\n            width = 10\n          ),\n          \"\\n\"\n        )\n      }\n    }\n  }\n}\n\ndummy &lt;- function () {\n  set.seed(12345)\n  x &lt;- matrix(rnorm(10), 5, 2)\n  x &lt;- apply (x, 2, function(x)\n    x - mean(x))\n  delta &lt;- dist(x)\n  d &lt;- dist (x)\n  eps &lt;- (-500:500) / 100\n  sy &lt;- rep (0, 1001)\n  plot (\n    0,\n    0,\n    xlim = c(-5, 5),\n    ylim = c(0, 20),\n    xlab = \"epsilon\",\n    ylab = \"stress\",\n    type = \"n\"\n  )\n  for (i in 1:5) {\n    for (j in 1:2) {\n      for (k in 1:1001) {\n        y &lt;- x\n        y[i, j] &lt;- x[i, j] + eps[k]\n        dy &lt;- dist (y)\n        sy[k] &lt;- sum ((delta - dy) ^ 2)\n      }\n      lines (eps, sy, lwd = 2, col = \"RED\")\n    }\n  }\n}\n\n\nbmat2 &lt;- function (a, b, x, y, delta) {\n  bm &lt;- matrix (0, 2, 2)\n  hm &lt;- matrix (0, 2, 2)\n  z &lt;- c(a, b)\n  for (i in 1:4) {\n    for (j in 1:4) {\n      if (i == j)\n        next\n      uij &lt;- uu (i, j, x, y)\n      uz &lt;- drop (uij %*% z)\n      dij &lt;- sqrt (sum (uij * outer (z, z)))\n      bm &lt;- bm + (delta[i, j] / dij) * uij\n      hm &lt;-\n        hm + (delta[i, j] / dij) * (uij - outer (uz, uz) / sum (z * uz))\n    }\n  }\n  return (list (b = bm, h = hm))\n}\n\nstress2 &lt;- function (a, b, x, y, delta) {\n  z &lt;- c (a, b)\n  bm &lt;- bmat2 (a, b, x, y, delta)$b\n  return (1 + sum(z ^ 2) / 2 - sum (z * bm %*% z))\n}\n\nrho2 &lt;- function (a, b, x, y, delta) {\n  z &lt;- c (a, b)\n  bm &lt;- bmat2 (a, b, x, y, delta)$b\n  return (sum (z * bm %*% z))\n}\n\nvv &lt;- function (i, j, x, y) {\n  a &lt;- matrix (0, 2, 2)\n  a[1, 1] &lt;- sum ((x[i,] - x[j, ]) ^ 2)\n  a[2, 2] &lt;- sum ((y[i,] - y[j, ]) ^ 2)\n  a[1, 2] &lt;- a[2, 1] &lt;- sum ((x[i,] - x[j, ]) * (y[i,] - y[j,]))\n  return (a)\n}\n\nuu &lt;- function (i, j, x, y) {\n  n &lt;- nrow (x)\n  asum &lt;-\n    2 * n * matrix (c (sum(x ^ 2), sum (x * y), sum (x * y), sum (y ^ 2)), 2, 2)\n  csum &lt;- solve (chol (asum))\n  return (t(csum) %*% vv (i, j, x, y) %*% csum)\n}\n\nsmacof2 &lt;-\n  function (a,\n            b,\n            x,\n            y,\n            delta,\n            eps = 1e-10,\n            itmax = 1000,\n            verbose = TRUE) {\n    zold &lt;- c(a, b)\n    bold &lt;- bmat2 (a, b, x, y, delta)$b\n    fold &lt;- 1 + sum(zold ^ 2) / 2 - sum (zold * bold %*% zold)\n    itel &lt;- 1\n    repeat {\n      znew &lt;- drop (bold %*% zold)\n      bhmt &lt;- bmat2 (znew[1], znew[2], x, y, delta)\n      bnew &lt;- bhmt$b\n      fnew &lt;- 1 + sum(znew ^ 2) / 2 - sum (znew * bnew %*% znew)\n      if (verbose) {\n        cat (\n          formatC (itel, width = 4, format = \"d\"),\n          formatC (\n            fold,\n            digits = 10,\n            width = 13,\n            format = \"f\"\n          ),\n          formatC (\n            fnew,\n            digits = 10,\n            width = 13,\n            format = \"f\"\n          ),\n          \"\\n\"\n        )\n      }\n      if ((itel == itmax) || (fold - fnew) &lt; eps)\n        break ()\n      itel &lt;- itel + 1\n      fold &lt;- fnew\n      zold &lt;- znew\n      bold &lt;- bnew\n    }\n    return (\n      list (\n        stress = fnew,\n        theta = znew,\n        itel = itel,\n        b = bnew,\n        g = znew - bnew %*% znew,\n        h = diag(2) - bhmt$h\n      )\n    )\n  }\n\n\nnewton2 &lt;-\n  function (a,\n            b,\n            x,\n            y,\n            delta,\n            eps = 1e-10,\n            itmax = 1000,\n            verbose = TRUE) {\n    zold &lt;- c(a, b)\n    bhmt &lt;- bmat2 (a, b, x, y, delta)\n    bold &lt;- bhmt$b\n    hold &lt;- diag(2) - bhmt$h\n    fold &lt;- 1 + sum(zold ^ 2) / 2 - sum (zold * bold %*% zold)\n    itel &lt;- 1\n    repeat {\n      znew &lt;- drop (solve (hold, bold %*% zold))\n      bhmt &lt;- bmat2 (znew[1], znew[2], x, y, delta)\n      bnew &lt;- bhmt$b\n      hnew &lt;- diag(nrow(bnew)) - bhmt$h\n      fnew &lt;- 1 + sum(znew ^ 2) / 2 - sum (znew * bnew %*% znew)\n      if (verbose) {\n        cat (\n          formatC (itel, width = 4, format = \"d\"),\n          formatC (\n            fold,\n            digits = 10,\n            width = 13,\n            format = \"f\"\n          ),\n          formatC (\n            fnew,\n            digits = 10,\n            width = 13,\n            format = \"f\"\n          ),\n          \"\\n\"\n        )\n      }\n      if ((itel == itmax) || abs (fold - fnew) &lt; eps)\n        break ()\n      itel &lt;- itel + 1\n      fold &lt;- fnew\n      zold &lt;- znew\n      bold &lt;- bnew\n      hold &lt;- hnew\n    }\n    return (list (\n      stress = fnew,\n      theta = znew,\n      itel = itel,\n      b = bnew,\n      g = znew - bnew %*% znew,\n      h = hnew\n    ))\n  }\n\n\n\nclassical.R\n\ntau &lt;- function (x) {\n  return (- 0.5 * dcen (x))\n}\n\nkappa &lt;- function (x) {\n  return (outer (diag (x), diag (x), \"+\") - 2 * x)\n}\n\nfcmds &lt;-\n  function (delta,\n            xold,\n            ninner = 1,\n            itmax = 100,\n            eps = 1e-6,\n            verbose = TRUE) {\n    itel &lt;- 0\n    p &lt;- ncol (xold)\n    xold &lt;- apply (xold, 2, function (x)\n      x - mean(x))\n    xold &lt;- qr.Q (qr (xold))\n    repeat {\n      xinn &lt;- xold\n      for (i in 1:ninner) {\n        xnew &lt;- delta %*% xinn\n        xnew &lt;- -apply (xnew, 2, function (x)\n          x - mean (x)) / 2\n        xinn &lt;- xnew\n        itel &lt;- itel + 1\n      }\n      qnew &lt;- qr (xnew)\n      xnew &lt;- qr.Q (qnew)\n      rnew &lt;- qr.R (qnew)\n      epsi &lt;- 2 * p - 2 * sum (svd (crossprod (xold, xnew))$d)\n      if (verbose) {\n        cat(\n          \"itel \",\n          formatC (\n            itel,\n            digits = 4,\n            width = 6,\n            format = \"d\"\n          ),\n          \"epsi \",\n          formatC (\n            epsi,\n            digits = 10,\n            width = 15,\n            format = \"f\"\n          ),\n          \"\\n\"\n        )\n      }\n      if ((epsi &lt; eps) || (itel == itmax))\n        break\n      xold &lt;- xnew\n    }\n    return (list (x = xnew, r = rnew, itel = itel))\n  }\n\ntreq &lt;- function (x) {\n  n &lt;- nrow (d)\n  m &lt;- -Inf\n  for (i in 2:n) {\n    for (j in 1:(i - 1)) {\n      for (k in 1:n) {\n        if ((k == i) || (k == j))\n          next\n        m &lt;- max (m, x[i, j] - (x[i, k] + x[k, j]))\n      }\n    }\n  }\n  return (m)\n}\n\nacbound &lt;- function (d) {\n  n &lt;- nrow (d)\n  s &lt;- qr.Q (qr (cbind (1, matrix (rnorm (\n    n * (n - 1)\n  ), n, n - 1))))\n  k &lt;- tau (d * d)\n  l &lt;- 2 * tau (d)\n  m &lt;- jmat (n) / 2\n  ma &lt;- -Inf\n  for (i in 2:n)\n    for (j in 1:(i - 1)) {\n      v &lt;- solve(polynomial(c(k[i, j], l[i, j], m[i, j])))\n      ma &lt;- max(ma, max(v))\n    }\n  return (list(ma = ma, mw = k + ma * l + m * ma ^ 2))\n}\n\naceval &lt;- function (d, bnd = c(-10, 10)) {\n  n &lt;- nrow (d)\n  k &lt;- tau (d * d)\n  l &lt;- 2 * tau (d)\n  m &lt;- jmat (n) / 2\n  s &lt;- qr.Q (qr (cbind (1, matrix (rnorm (\n    n * (n - 1)\n  ), n, n - 1))))\n  kc &lt;- (crossprod (s, k) %*% s)[-1,-1]\n  lc &lt;- (crossprod (s, l) %*% s)[-1,-1]\n  mc &lt;- (crossprod (s, m) %*% s)[-1,-1]\n  a &lt;- seq(bnd[1], bnd[2], length = 1000)\n  b &lt;- rep(0, 1000)\n  for (i in 1:1000) {\n    ww &lt;- kc + lc * a[i] + mc * (a[i] ^ 2)\n    b[i] &lt;- min (eigen(ww)$values)\n  }\n  return (list(a = a, b = b))\n}\n\nacqep &lt;- function(d) {\n  n &lt;- nrow (d)\n  k &lt;- tau (d * d)\n  l &lt;- 2 * tau (d)\n  m &lt;- jmat (n) / 2\n  s &lt;- qr.Q (qr (cbind (1, matrix (rnorm (\n    n * (n - 1)\n  ), n, n - 1))))\n  nn &lt;- n - 1\n  ns &lt;- 1:nn\n  kc &lt;- (crossprod (s, k) %*% s)[-1,-1]\n  lc &lt;- (crossprod (s, l) %*% s)[-1,-1]\n  mc &lt;- (crossprod (s, m) %*% s)[-1,-1]\n  ma &lt;- matrix(0, 2 * nn, 2 * nn)\n  ma[ns, nn + ns] &lt;- diag (n - 1)\n  ma[nn + ns, ns] &lt;- -2 * kc\n  ma[nn + ns, nn + ns] &lt;- -2 * lc\n  return (list (ma = ma, me = eigen(ma)$values))\n}\n\n\n\nminimization.R\n\n\nfull.R\n\nlibrary(MASS)\n\ntorgerson &lt;- function(delta, p = 2) {\n  doubleCenter &lt;- function(x) {\n    n &lt;- dim(x)[1]\n    m &lt;- dim(x)[2]\n    s &lt;- sum(x) / (n * m)\n    xr &lt;- rowSums(x) / m\n    xc &lt;- colSums(x) / n\n    return((x - outer(xr, xc, \"+\")) + s)\n  }\n  z &lt;-\n    eigen(-doubleCenter((as.matrix (delta) ^ 2) / 2), symmetric = TRUE)\n  v &lt;- pmax(z$values, 0)\n  return(z$vectors[, 1:p] %*% diag(sqrt(v[1:p])))\n}\n\n\nmakeA &lt;- function (n) {\n  m &lt;- n * (n - 1) / 2\n  a &lt;- list()\n  for (j in 1:(n - 1))\n    for (i in (j + 1):n) {\n      d &lt;- ein (i, n) -ein (j, n)\n      e &lt;- outer (d, d)\n      a &lt;- c(a, list (e))\n    }\n  return (a)\n}\n\nmakeD &lt;- function (a, x) {\n  return (sapply (a, function (z)\n    sqrt (sum (x * (\n      z %*% x\n    )))))\n}\n\nmakeB &lt;- function (w, delta, d, a) {\n  n &lt;- length (a)\n  m &lt;- nrow (a[[1]])\n  b &lt;- matrix (0, m , m)\n  for (i in 1:n)\n    b &lt;- b + w[i] * (delta[i] / d[i]) * a[[i]]\n  return (b)\n}\n\nmakeV &lt;- function (w, a) {\n  n &lt;- length (a)\n  m &lt;- nrow (a[[1]])\n  v &lt;- matrix (0, m, m)\n  for (i in 1:n)\n    v &lt;- v + w[i] * a[[i]]\n  return (v)\n}\n\ninBetween &lt;- function (alpha, beta, x, y, w, delta, a) {\n  z &lt;- alpha * x + beta * y\n  d &lt;- makeD (a, z)\n  return (sum (w * (delta - d) ^ 2))\n}\n\nbiBase &lt;- function (x, y, a) {\n  biBi &lt;- function (x, y, v) {\n    a11 &lt;- sum (x * (v %*% x))\n    a12 &lt;- sum (x * (v %*% y))\n    a22 &lt;- sum (y * (v %*% y))\n    return (matrix (c(a11, a12, a12, a22), 2, 2))\n  }\n  return (lapply (a, function (u)\n    biBi (x, y, u)))\n}\n\nfullMDS &lt;-\n  function (delta,\n            w = rep (1, length (delta)),\n            xini,\n            a,\n            itmax = 100,\n            eps = 1e-6,\n            verbose = TRUE) {\n    m &lt;- length (a)\n    v &lt;- makeV (w, a)\n    vv &lt;- ginv (v)\n    xold &lt;- xini\n    dold &lt;- makeD (a, xini)\n    sold &lt;- sum ((delta - dold) ^ 2)\n    bold &lt;- makeB (w, delta, dold, a)\n    itel &lt;- 1\n    repeat {\n      xnew &lt;- vv %*% bold %*% xold\n      dnew &lt;- makeD (a, xnew)\n      bnew &lt;- makeB (w, delta, dnew, a)\n      snew &lt;- sum ((delta - dnew) ^ 2)\n      if (verbose) {\n        cat (\n          formatC (itel, width = 4, format = \"d\"),\n          formatC (\n            sold,\n            digits = 10,\n            width = 13,\n            format = \"f\"\n          ),\n          formatC (\n            snew,\n            digits = 10,\n            width = 13,\n            format = \"f\"\n          ),\n          \"\\n\"\n        )\n      }\n      if ((itel == itmax) || (abs(sold - snew) &lt; eps))\n        break\n      itel &lt;- itel + 1\n      xold &lt;- xnew\n      dold &lt;- dnew\n      sold &lt;- snew\n      bold &lt;- bnew\n    }\n    return (list (\n      x = xnew,\n      d = dnew,\n      delta = delta,\n      s = snew,\n      b = bnew,\n      v = v,\n      itel = itel\n    ))\n  }\n\n\n\nunfolding.R\n\ndummy &lt;- function () {\n  set.seed(12345)\n  \n  x &lt;- matrix (rnorm(16), 8, 2)\n  x &lt;- apply (x, 2, function (x)\n    x - mean (x))\n  y &lt;- matrix (rnorm(10), 5, 2)\n  a &lt;- rowSums (x ^ 2)\n  b &lt;- rowSums (y ^ 2)\n  d &lt;- sqrt (outer(a, b, \"+\") - 2 * tcrossprod (x, y))\n  \n  set.seed (12345)\n  x &lt;- matrix(rnorm(10), 5, 2)\n  x &lt;- apply (x, 2, function (x)\n    x - mean (x))\n  x &lt;- qr.Q(qr(x))\n  y &lt;- matrix(rnorm(12), 6, 2)\n  v &lt;- apply (y, 2, mean)\n  print (v)\n  dx &lt;- diag(tcrossprod(x))\n  dy &lt;- diag(tcrossprod(y))\n  xy &lt;- tcrossprod(x, y)\n  dd &lt;- outer(dx, dy, \"+\") - 2 * xy\n  j5 &lt;- diag(5) - 1 / 5\n  j6 &lt;- diag(6) - 1 / 6\n  dc &lt;- -(j5 %*% dd %*% j6) / 2\n  sv &lt;- svd (dc, nu = 2, nv = 2)\n  xs &lt;- sv$u\n  ys &lt;- sv$v %*% diag (sv$d[1:2])\n  tt &lt;- crossprod (x, xs)\n  dk &lt;- diag (tcrossprod(xs))\n  dl &lt;- diag (tcrossprod(ys))\n  dr &lt;- dd - outer (dk, dl, \"+\") + 2 * tcrossprod (xs, ys)\n  print (dr)\n}\n\nschoenemann &lt;- function (delta, p) {\n  n &lt;- nrow (delta)\n  m &lt;- ncol (delta)\n  l &lt;- p * (p + 1) / 2\n  d &lt;- delta ^ 2\n  e &lt;- torgerson (d)\n  q &lt;- svd (e, nu = p, nv = p)\n  g &lt;- q$u\n  h &lt;- q$v %*% diag (q$d[1:p])\n  f &lt;- d + 2 * tcrossprod(g, h)\n  a &lt;- apply (ccen (f), 1, mean)\n  r &lt;- matrix (0, n, l)\n  k &lt;- 1\n  for (i in 1:p) {\n    for (j in 1:i) {\n      if (i == j) {\n        r[, k] &lt;- g[, i] ^ 2\n      } else {\n        r[, k] &lt;- 2 * g[, i] * g[, j]\n      }\n      k &lt;- k + 1\n    }\n  }\n  lhs &lt;- cbind (ccen(r), ccen (-2 * g))\n  b &lt;- lm.fit (lhs, a)$coefficients\n  k &lt;- 1\n  s &lt;- matrix (0, p, p)\n  for (i in 1:p) {\n    for (j in 1:i) {\n      if (i == j) {\n        s[i, i] = b[k]\n      } else {\n        s[i, j] &lt;- s[j, i] &lt;- b[k]\n      }\n      k &lt;- k + 1\n    }\n  }\n  e &lt;- eigen (s)\n  f &lt;- e$values\n  if (min(f) &lt; 0) {\n    stop (\"Negative eigenvalue, cannot proceed\")\n  }\n  t &lt;- e$vectors %*% diag (sqrt (f))\n  v &lt;- solve (t, b[-(1:l)])\n  x &lt;- g %*% t\n  y &lt;-\n    h %*% (e$vectors %*% diag (1 / sqrt (f))) + matrix (v, m, p, byrow = TRUE)\n  return (list (x = x, y = y))\n}\n\nunfoldals &lt;- function (offdiag) {\n  n &lt;- nrow (offdiag)\n  m &lt;- ncol (offdiag)\n  dd &lt;- offdiag ^ 2\n  delta &lt;- matrix (0, n + m, n + m)\n  delta[1:n, n + (1:m)] &lt;- dd\n  delta &lt;- pmax(delta, t(delta))\n  cc &lt;-\n    dd - outer (rowSums(dd) / m, colSums (dd) / n, \"+\") + sum(dd) / (n * m)\n  sc &lt;- svd (-cc / 2)\n  lb &lt;- diag (sqrt(sc$d))\n  xold &lt;- sc$u %*% lb\n  yold &lt;- sc$v %*% lb\n  zold &lt;- rbind (xold, yold)\n  lold &lt;- rowSums (zold ^ 2)\n  dold &lt;- outer (lold, lold, \"+\") - 2 * tcrossprod (zold)\n  \n}\n\nteqbounds &lt;- function (offdiag) {\n  n &lt;- nrow (offdiag)\n  m &lt;- ncol (offdiag)\n  a &lt;- matrix (0, n, n)\n  b &lt;- matrix (0, m, m)\n  for (i in 2:n) {\n    for (j in 1:(i - 1)) {\n      smin = Inf\n      smax = -Inf\n      for (k in 1:m) {\n        smin = min (smin, offdiag[i, k] + offdiag[j, k])\n        smax = max (smax, abs (offdiag[i, k] - offdiag[j, k]))\n      }\n      a[i, j] &lt;- a[j, i] &lt;- (smin + smax) / 2\n    }\n  }\n  for (i in 2:m) {\n    for (j in 1:(i - 1)) {\n      smin = Inf\n      smax = -Inf\n      for (k in 1:n) {\n        smin = min (smin, offdiag[k, i] + offdiag[k, j])\n        smax = max (smax, abs (offdiag[k, i] - offdiag[k, j]))\n      }\n      b[i, j] &lt;- b[j, i] &lt;- (smin + smax) / 2\n    }\n  }\n  return (list (a = a, b = b))\n}\n\n\n\nconstrained.R\n\npcircsmacof &lt;-\n  function (delta,\n            w = wdef (nrow (delta)),\n            p = 2,\n            x = smacofInitialR (delta, p),\n            itmax = 1000,\n            eps = 1e-6,\n            verbose = TRUE) {\n    labels = c(\"itel\", \"sold\", \"snew\")\n    digits = c(4, 10, 10)\n    widths = c(6, 15, 15)\n    format = c(\"d\", \"f\", \"f\")\n    n &lt;- nrow (x)\n    p &lt;- ncol (x)\n    xold &lt;- x / sqrt (rowSums (x ^ 2))\n    dold &lt;- as.matrix (dist (xold))\n    v &lt;- smacofVmatR (w)\n    e &lt;- max (eigen (v, only.values = TRUE)$values)\n    vinv &lt;- ginv(v)\n    itel &lt;- 1\n    sold &lt;- smacofLossR (dold, w, delta)\n    repeat {\n      b &lt;- smacofBmatR (dold, w, delta)\n      xgut &lt;- smacofGuttmanR(xold, b, vinv)\n      xtar &lt;- xold + v %*% (xgut - xold) / e\n      xlen &lt;- sqrt (rowSums (xtar ^ 2))\n      xrad &lt;- mean (xlen)\n      xnew &lt;- (xtar / xlen) * xrad\n      dnew &lt;- as.matrix (dist(xnew))\n      snew &lt;- smacofLossR (dnew, w, delta)\n        if (verbose) {\n          values = c(itel, sold, snew)\n          iterationWrite (labels, values, digits, width, format)\n        }\n        if (((sold - snew) &lt; eps) || (itel == itmax)) {\n          break\n        }\n        itel &lt;- itel + 1\n        xold &lt;- xnew\n        sold &lt;- snew\n      }\n      return (list (\n        x = xnew,\n        d = dnew,\n        stress = snew,\n        radius = xrad,\n        itel = itel\n      ))\n  }\n\n\npellipsmacof &lt;-\n  function (delta,\n            w = wdef (nrow (delta)),\n            p = 2,\n            x = smacofInitialR (delta, p),\n            itmax = 1000,\n            eps = 1e-6,\n            verbose = TRUE) {\n    labels = c(\"itel\", \"sold\", \"smid\", \"snew\")\n    digits = c(4, 10, 10, 10)\n    widths = c(6, 15, 15, 15)\n    format = c(\"d\", \"f\", \"f\", \"f\")\n    n &lt;- nrow (x)\n    p &lt;- ncol (x)\n    yold &lt;- x / sqrt (rowSums (x ^ 2))\n    xlbd &lt;- rep (1, p)\n    xold &lt;- yold %*% diag (xlbd)\n    dold &lt;- as.matrix (dist (xold))\n    v &lt;- smacofVmatR (w)\n    e &lt;- max (eigen (v, only.values = TRUE)$values)\n    vinv &lt;- ginv(v)\n    itel &lt;- 1\n    sold &lt;- smacofLoss(dold, w, delta)\n    repeat {\n      b &lt;- smacofBmatR (dold, w, delta)\n      xgut &lt;- smacofGuttmanR(xold, b, vinv)\n      for (s in 1:p) {\n        xlbd[s] &lt;-\n          sum (xgut[, s] * (v %*% yold[, s])) / sum (yold[, s] * (v %*% yold[, s]))\n      }\n      xmid &lt;- yold %*% diag (xlbd)\n      dmid &lt;- as.matrix (dist (xmid))\n      smid &lt;- sum (w * (delta - dmid) ^ 2) / 2\n      mlbd &lt;- max (xlbd ^ 2)\n      ytar &lt;-\n        yold + v %*% ((xgut %*% diag (1 / xlbd)) - yold) %*% diag (xlbd ^ 2) / (e * mlbd)\n      ylen &lt;- sqrt (rowSums (ytar ^ 2))\n      ynew &lt;- ytar / ylen\n      xnew &lt;- ynew %*% diag (xlbd)\n      dnew &lt;- as.matrix (dist(xnew))\n      snew &lt;- sum (w * (delta - dnew) ^ 2) / 2\n      if (verbose) {\n        values = c(itel, sold, smid, snew)\n        iterationWrite (labels, values, digits, width, format)\n      }\n      if (((sold - snew) &lt; eps) || (itel == itmax)) {\n        break\n      }\n      itel &lt;- itel + 1\n      xold &lt;- xnew\n      yold &lt;- ynew\n      sold &lt;- snew\n    }\n    return (list (\n      x = xnew,\n      d = dnew,\n      stress = snew,\n      axes = xlbd,\n      itel = itel\n    ))\n  }\n\ndcircsmacof &lt;-\n  function (delta,\n            w = wdef (nrow (delta)),\n            p = 2,\n            x = smacofInitialR (delta, p),\n            pen = 1,\n            itmax = 1000,\n            eps = 1e-6,\n            verbose = TRUE) {\n    labels = c(\"itel\", \"sold\", \"smid\", \"snew\")\n    digits = c(4, 10, 10, 10)\n    widths = c(6, 15, 15, 15)\n    format = c(\"d\", \"f\", \"f\", \"f\")\n    n &lt;- nrow (x)\n    xold &lt;-\n      rbind (0, x / sqrt (rowSums(x ^ 2)))\n    dold &lt;- as.matrix (dist (xold))\n    w &lt;- rbind (pen, cbind (pen, w))\n    delta &lt;- rbind (1, cbind (1, delta))\n    w[1, 1] &lt;- delta [1, 1] &lt;- 0\n    v &lt;- smacofVmatR (w)\n    vinv &lt;- ginv(v)\n    itel &lt;- 1\n    sold &lt;- smacofLossR(dold, w, delta)\n    repeat {\n      b &lt;- smacofBmatR(dold, w, delta)\n      xnew &lt;- smacofGuttmanR(xold, b, vinv)\n      dnew &lt;- as.matrix (dist (xnew))\n      smid &lt;- smacofLossR(dnew, w, delta)\n      a &lt;- sum (dnew[1,]) / n\n      delta[1,] &lt;- delta[, 1] &lt;- a\n      delta[1, 1] &lt;- 0\n      snew &lt;- smacofLossR(dnew, w, delta)\n      if (verbose) {\n        values = c(itel, sold, smid, snew)\n        iterationWrite (labels, values, digits, width, format)\n      }\n      if (((sold - snew) &lt; eps) || (itel == itmax)) {\n        break\n      }\n      itel &lt;- itel + 1\n      xold &lt;- xnew\n      sold &lt;- snew\n    }\n    return (list (\n      x = xnew,\n      d = dnew,\n      a = a,\n      stress = snew,\n      itel = itel\n    ))\n  }\n\ndellipsmacof &lt;-\n  function (delta,\n            w = wdef (nrow (delta)),\n            p = 2,\n            x = smacofInitialR (delta, p),\n            pen = 1,\n            itmax = 1000,\n            eps = 1e-6,\n            verbose = TRUE) {\n    labels = c(\"itel\", \"sold\", \"smid\", \"snew\")\n    digits = c(4, 10, 10, 10)\n    widths = c(6, 15, 15, 15)\n    format = c(\"d\", \"f\", \"f\", \"f\")\n    n &lt;- nrow (x)\n    set.seed(12345)\n    focal &lt;- rnorm(2)\n    xold &lt;-\n      rbind (focal, -focal, x / sqrt (rowSums(x ^ 2)))\n    dold &lt;- as.matrix (dist (xold))\n    w &lt;- rbind (pen, pen, cbind (pen, pen, w))\n    delta &lt;- rbind (1, 1, cbind (1, 1, delta))\n    w[1:2, 1:2] &lt;- delta [1:2, 1:2] &lt;- 0\n    v &lt;- smacofVmatR (w)\n    vinv &lt;- ginv(v)\n    itel &lt;- 1\n    sold &lt;- smacofLossR(dold, w, delta)\n    repeat {\n      b &lt;- smacofBmatR (dold, w, delta)\n      xnew &lt;- smacofGuttmanR (xold, b, vinv)\n      dnew &lt;- as.matrix (dist (xnew))\n      smid &lt;- smacofLossR(dnew, w, delta)\n      dsub &lt;- dnew[1:2, 2 + (1:n)]\n      asub &lt;- sum (dsub) / (2 * n)\n      dsub &lt;- ccen (dsub) + asub\n      delta[1:2, 2 + (1:n)] &lt;- dsub\n      delta[2 + (1:n), 1:2] &lt;- t(dsub)\n      snew &lt;- smacofLossR(dnew, w, delta)\n      if (verbose) {\n        values = c(itel, sold, smid, snew)\n        iterationWrite (labels, values, digits, width, format)\n      }\n      if (((sold - snew) &lt; eps) || (itel == itmax)) {\n        break\n      }\n      itel &lt;- itel + 1\n      xold &lt;- xnew\n      sold &lt;- snew\n    }\n    return (list (\n      pen = pen,\n      x = xnew,\n      d = dnew,\n      stress = snew,\n      itel = itel\n    ))\n  }\n\n\n\nnominal.R\n\nbaseplot &lt;- function (x,\n                      y,\n                      z,\n                      wx = TRUE,\n                      wy = TRUE,\n                      wz = TRUE) {\n  par(pty=\"s\")\n  plot(\n    x,\n    xlim = c(-3, 3),\n    ylim = c(-3, 3),\n    xlab = \"\",\n    ylab = \"\",\n    type  = \"n\"\n  )\n  if (wx)\n    points(x, col = \"RED\", cex = 1)\n  if (wy)\n    points(y, col = \"BLUE\", cex = 1)\n  if (wz)\n    points(z, col = \"GREEN\", cex = 1)\n  mx &lt;- apply(x, 2, mean)\n  my &lt;- apply(y, 2, mean)\n  mz &lt;- apply(z, 2, mean)\n  if (wx)\n    points(\n      matrix(mx, 1, 2),\n      col = \"RED\",\n      pch = 5,\n      cex = 2,\n      lwd = 2\n    )\n  if (wy)\n    points(\n      matrix(my, 1, 2),\n      col = \"BLUE\",\n      pch = 5,\n      cex = 2,\n      lwd = 2\n    )\n  if (wz)\n    points(\n      matrix(mz, 1, 2),\n      col = \"GREEN\",\n      pch = 5,\n      cex = 2,\n      lwd = 2\n    )\n  if (wx)\n    for (i in 1:10) {\n      lines(rbind(x[i,], mx))\n    }\n  if (wy)\n    for (i in 1:5) {\n      lines(rbind(y[i,], my))\n    }\n  if (wz)\n    for (i in 1:5) {\n      lines(rbind(z[i,], mz))\n    }\n}\n\n\n\nsstress.R\n\nstrainAdd &lt;-\n  function (delta,\n            w = rep (1, length (delta)),\n            p = 2,\n            itmax = 100,\n            eps = 1e-6,\n            verbose = TRUE) {\n    delta &lt;- as.matrix (delta ^ 2)\n    n &lt;- nrow(delta)\n    \n  }\n\nstrainWeight &lt;-\n  function (delta,\n            w = rep (1, length (delta)),\n            p = 2,\n            itmax = 100,\n            eps = 1e-6,\n            verbose = TRUE) {\n    \n  }\n\nalscal &lt;-\n  function (delta,\n            p,\n            x = torgerson (delta, p),\n            w = wdef (nrow (x)),\n            itmax = 1000,\n            eps = 1e-6,\n            verbose = TRUE,\n            check = TRUE) {\n    n &lt;- nrow (x)\n    delta &lt;- delta ^ 2\n    d &lt;- as.matrix (dist (x)) ^ 2\n    sold &lt;- sum (w * (delta - d) ^ 2)\n    wsum &lt;- rowSums (w)\n    itel &lt;- 1\n    snew &lt;- sold\n    repeat {\n      for (k in 1:n) {\n        t4 &lt;- wsum[k]\n        for (s in 1:p) {\n          u &lt;- x[, s] - x[k, s]\n          t0 &lt;- snew\n          t1 &lt;- t2 &lt;- t3 &lt;- 0\n          for (i in 1:n) {\n            t1 &lt;- t1 + 4 * w[i, k] * (d[i, k] - delta[i, k]) * u[i]\n            t2 &lt;-\n              t2 + 2 * w[i, k] * ((d[i, k] - delta[i, k]) + 2 * u[i] ^ 2)\n            t3 &lt;- t3 + 4 * w[i, k] * u[i]\n          }\n          pp &lt;- polynomial(c(t0,-t1, t2,-t3, t4))\n          qq &lt;- deriv (pp)\n          ss &lt;- solve (qq)\n          ss &lt;- Re (ss[which (abs (Im (ss)) &lt; 1e-10)])\n          tt &lt;- predict (pp, ss)\n          snew &lt;- min (tt)\n          root &lt;- ss[which.min (tt)]\n          x[k, s] &lt;- x[k, s] + root\n          for (i in (1:n)[-k]) {\n            d[i, k] &lt;- d[i, k] - 2 * root * u[i] + root ^ 2\n            d[k, i] &lt;- d[i, k]\n          }\n        }\n      }\n      if (verbose) {\n        cat(\n          \"itel \",\n          formatC(itel, width = 6, format = \"d\"),\n          \"sold \",\n          formatC(\n            sold,\n            digits = 6,\n            width = 15,\n            format = \"f\"\n          ),\n          \"snew \",\n          formatC(\n            snew,\n            digits = 6,\n            width = 15,\n            format = \"f\"\n          ),\n          \"\\n\"\n        )\n      }\n      if (((sold - snew) &lt; eps) || (itel == itmax)) {\n        break\n      }\n      sold &lt;- snew\n      itel &lt;- itel + 1\n    }\n    return (list (\n      x = x,\n      sstress = snew,\n      itel = itel\n    ))\n  }\n\njeffrey &lt;- function(a) {\n  h &lt;-\n    .C(\"jeffreyC\",\n       a = as.double (a),\n       minwhere = as.double (0),\n       minvalue = as.double (0))\n  return (list.remove (h, 1))\n}\n\n\n\ninverse.R\n\nimdsSolver &lt;- function (tau) {\n  radius &lt;- sqrt (((tau - 3) ^ 2) / 3)\n  center &lt;- c(tau, tau) / 3\n  a &lt;- matrix (c(1, .5, .5, 1), 2, 2)\n  draw_ellipse (\n    center,\n    radius,\n    a,\n    col = \"RED\",\n    lwd = 2,\n    xlim = c(0, tau),\n    ylim = c(0, tau),\n    xlab = \"alpha\",\n    ylab = \"beta\"\n  )\n  lines (matrix(c(0, tau, tau, 0), 2, 2), col = \"BLUE\", lwd = 2)\n  abline(h = 0)\n  abline(v = 0)\n}\n\nbs &lt;- function () {\n  z &lt;- matrix(c(1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, -1, 1, -1, -1, 1), 4 , 4) / 2\n  a &lt;- as.list (1:6)\n  k &lt;- 1\n  for (i in 1:3) {\n    for (j in (i + 1):4) {\n      a[[k]] &lt;- crossprod (z, aijn(i, j, 4) %*% z)\n      k &lt;- k + 1\n    }\n  }\n  return(a)\n}\n\nimdsChecker &lt;- function (a) {\n  aa &lt;- bs()\n  bb &lt;- matrix(0, 4, 4)\n  for (i in 1:6) {\n    bb &lt;- bb + a[i] * aa[[i]]\n  }\n  return (bb)\n}\n\ninverseMDS &lt;- function (x) {\n  n &lt;- nrow (x)\n  m &lt;- ncol (x)\n  x &lt;- apply (x, 2, function (y)\n    y - mean (y))\n  nm &lt;- n - (m + 1)\n  kk &lt;- cbind (1, x, matrix (rnorm (n * nm), n , nm))\n  kperp &lt;- as.matrix (qr.Q (qr (kk))[,-(1:(m + 1))])\n  dd &lt;- as.matrix (dist (x))\n  k &lt;- 1\n  base &lt;- matrix (0, n * (n - 1) / 2, nm * (nm + 1) / 2)\n  for (i in 1:nm) {\n    for (j in 1:i) {\n      oo &lt;- outer (kperp[, i], kperp[, j])\n      if (j != i) {\n        oo &lt;- oo + t(oo)\n      }\n      base[, k] &lt;- lower_triangle (dd + (1 - oo))\n      k &lt;- k + 1\n      print (c(i, j, k))\n    }\n  }\n  return (base = cbind (lower_triangle (dd), base))\n}\n\ninversePlus &lt;- function (base, affine = TRUE) {\n  if (affine) {\n    hrep &lt;- makeH (\n      a1 = d2q (-base),\n      b1 = d2q (rep (0, nrow (base))),\n      a2 = d2q (rep (1, ncol (base))),\n      b2 = d2q (1)\n    )\n  } else {\n    hrep &lt;- makeH (a1 = d2q (-base), b1 = d2q (rep (0, nrow (base))))\n  }\n  vrep &lt;- scdd (hrep)\n  hrep &lt;- q2d (hrep)\n  vrep &lt;- q2d (vrep$output)\n  pr &lt;- tcrossprod (hrep[, -c(1, 2)], vrep[, -c(1, 2)])[-1, ]\n  return (list (\n    base = base,\n    hrep = hrep,\n    vrep = vrep,\n    pr = pr\n  ))\n}\n\ntwoPoints &lt;- function (x, y, w = 1 - diag (nrow (x))) {\n  dx &lt;- lower_triangle (as.matrix (dist (x)))\n  dy &lt;- lower_triangle (as.matrix (dist (y)))\n  w &lt;- lower_triangle (w)\n  gx &lt;- makeG (x)\n  gy &lt;- makeG (y)\n  hx &lt;- (dx / w) * gx\n  hy &lt;- (dy / w) * gy\n  lxy &lt;- lm.fit (cbind (hx,-hy), dx - dy)\n  lxx &lt;- lxy$coefficients[1:ncol(hx)]\n  lyy &lt;- lxy$coefficients[-(1:ncol(hx))]\n  return (list(\n    delta1 = dx - hx %*% lxx,\n    delta2 = dy - hy %*% lyy,\n    res = sum (abs(lxy$residuals)),\n    rank = lxy$rank\n  ))\n}\n\nsecond_partials_stress &lt;-\n  function (x, delta, w = wdef (nrow (x))) {\n    n &lt;- nrow (x)\n    p &lt;- ncol (x)\n    d &lt;- as.matrix (dist (x))\n    fac &lt;- (w * delta) / (d + diag (n))\n    dd &lt;- d * d\n    v &lt;- smacofVmatR (w)\n    deri &lt;- direct_sum (repList (v, p))\n    xx &lt;- as.vector (x)\n    for (i in 1:(n - 1)) {\n      for (j in (i + 1):n) {\n        aa &lt;- direct_sum (repList (aijn (i, j, n), p))\n        ax &lt;- drop (aa %*% xx)\n        deri &lt;- deri - fac[i, j] * (aa - outer (ax, ax) / dd[i, j])\n      }\n    }\n    return (deri)\n  }\n\nsecond_partials_numerical &lt;-\n  function (x, delta, w = wdef (nrow (x))) {\n    stress &lt;- function (x, delta, w) {\n      n &lt;- nrow (delta)\n      p &lt;- length (x) / n\n      d &lt;- as.matrix(dist(matrix (x, n, p)))\n      res &lt;- delta - d\n      return (sum (w * res * res) / 2)\n    }\n    return (hessian (stress, x, delta = delta, w = w))\n  }\n\ncleanUp &lt;- function (a, eps = 1e-3) {\n  nv &lt;- nrow (a)\n  ind &lt;- rep (TRUE, nv)\n  for (i in 1:(nv - 1)) {\n    xx &lt;- a[i, ]\n    for (j in (i + 1):nv) {\n      if (!ind[j])\n        next\n      yy &lt;- a[j, ]\n      mm &lt;- max (abs (xx - yy))\n      if (mm &lt; eps)\n        ind[j] &lt;- FALSE\n    }\n  }\n  return (ind)\n}\n\nbruteForce &lt;- function (a, b, eps = 1e-3) {\n  n &lt;- nrow (a)\n  m &lt;- ncol (a)\n  cb &lt;- combn (n, m)\n  n1 &lt;- ncol (cb)\n  ind &lt;- rep(TRUE, n1)\n  ht &lt;- numeric()\n  for (i in 1:n1) {\n    gg &lt;- a[cb[, i],]\n    bg &lt;- b[cb[, i]]\n    qg &lt;- qr(gg)\n    if (qg$rank &lt; m) {\n      ind[i] &lt;- FALSE\n      next\n    }\n    hh &lt;- solve (qg, bg)\n    hg &lt;- drop (a %*% hh)\n    if (min (b - hg) &lt; -eps) {\n      ind[i] &lt;- FALSE\n      next\n    }\n    ht &lt;- c(ht, hh)\n  }\n  n2 &lt;- sum (ind)\n  ht &lt;- matrix (ht, m, n2)\n  ind &lt;-\n    .C (\n      \"cleanup\",\n      as.double(ht),\n      as.integer(n2),\n      as.integer(m),\n      as.integer(rep(1, n2)),\n      as.double (eps)\n    )[[4]]\n  n3 &lt;- sum (ind)\n  return (list (\n    x = t(ht)[which(ind == 1),],\n    n1 = n1,\n    n2 = n2,\n    n3 = n3\n  ))\n}\n\nbruteForceOne &lt;- function (a, b, p, q, v, eps = 1e-3) {\n  n &lt;- nrow (a)\n  m &lt;- ncol (a)\n  ind &lt;- which ((q - v %*% p) &gt; -eps)\n  v &lt;- v[ind,]\n  cb &lt;- combn (n, m - 1)\n  n1 &lt;- ncol (cb)\n  ind &lt;- rep(TRUE, n1)\n  ht &lt;- numeric()\n  for (i in 1:n1) {\n    gg &lt;- rbind (a[cb[, i],], p)\n    bg &lt;- c (b[cb[, i]], q)\n    qg &lt;- qr(gg)\n    if (qg$rank &lt; m) {\n      ind[i] &lt;- FALSE\n      next\n    }\n    hh &lt;- solve (qg, bg)\n    hg &lt;- drop (a %*% hh)\n    if (min (b - hg) &lt; -eps) {\n      ind[i] &lt;- FALSE\n      next\n    }\n    ht &lt;- c(ht, hh)\n  }\n  n2 &lt;- sum (ind)\n  ht &lt;- t (matrix (ht, m, n2))\n  ht &lt;- rbind (v, ht)\n  ind &lt;- cleanUp (ht, eps)\n  print (ind)\n  n3 &lt;- sum (ind)\n  return (list (\n    x = ht[ind,],\n    n1 = n1,\n    n2 = n2,\n    n3 = n3\n  ))\n}\n\nrankTest &lt;- function (x, a, b, eps = 1e-3) {\n  h &lt;- drop (a %*% x)\n  ind &lt;- which (abs (h - b) &lt; eps)\n  r &lt;- qr (a[ind, ])$rank\n  f &lt;- min (b - h) &gt; -eps\n  return (list (rank = r, feasibility = f))\n}\n\nmakeDC &lt;- function (x) {\n  y &lt;- -x\n  diag(y) &lt;- -rowSums (y)\n  return (y)\n}\n\nbmat &lt;- function (delta, w, d) {\n  n &lt;- nrow (w)\n  dd &lt;- ifelse (d == 0, 0, 1 / d)\n  return (makeDC (w * delta * dd))\n}\n\nsmacof &lt;-\n  function (delta,\n            w,\n            xini,\n            eps = 1e-6,\n            itmax = 100,\n            verbose = TRUE) {\n    n &lt;- nrow (xini)\n    xold &lt;- xini\n    dold &lt;- as.matrix (dist (xold))\n    sold &lt;- sum (w * (delta - dold) ^ 2) / 2\n    itel &lt;- 1\n    v &lt;- ginv (makeDC (w))\n    repeat {\n      b &lt;- bmat (delta, w, dold)\n      xnew &lt;- v %*% b %*% xold\n      dnew &lt;- as.matrix (dist (xnew))\n      snew &lt;- sum (w * (delta - dnew) ^ 2) / 2\n      if (verbose) {\n        cat (\n          formatC (itel, width = 4, format = \"d\"),\n          formatC (\n            sold,\n            digits = 10,\n            width = 13,\n            format = \"f\"\n          ),\n          formatC (\n            snew,\n            digits = 10,\n            width = 13,\n            format = \"f\"\n          ),\n          \"\\n\"\n        )\n      }\n      if ((itel == itmax) || (sold - snew) &lt; eps)\n        break ()\n      itel &lt;- itel + 1\n      sold &lt;- snew\n      dold &lt;- dnew\n      xold &lt;- xnew\n    }\n    return (list (\n      x = xnew,\n      d = dnew,\n      s = snew,\n      itel = itel\n    ))\n  }\n\noneMore &lt;- function (g, u) {\n  v &lt;- bruteForce (g, u)$x\n  nv &lt;- nrow (v)\n  s &lt;- matrix (0, 2, 2)\n  ev &lt;- rep (0, nv)\n  for (i in 1:nv) {\n    s[1, 1] &lt;- v[i, 1]\n    s[2, 2] &lt;- v[i, 2]\n    s[1, 2] &lt;- s[2, 1] &lt;- v[i, 3]\n    ee &lt;- eigen (s)\n    ev[i] &lt;- min (ee$values)\n    if (ev[i] &lt; 0) {\n      yy &lt;- ee$vectors[, 2]\n      hh &lt;- c (yy[1] ^ 2, yy[2] ^ 2, 2 * yy[1] * yy [2])\n      g &lt;- rbind (g,-hh)\n      u &lt;- c (u, 0)\n    }\n  }\n  return (list (\n    v = v,\n    g = g,\n    u = u,\n    e = ev\n  ))\n}\n\nmakeG &lt;- function (x) {\n  n &lt;- nrow (x)\n  p &lt;- ncol (x)\n  m &lt;- n - p - 1\n  k &lt;- qr.Q(qr(cbind(1, x, diag (n))))[,-c(1:(p + 1))]\n  g &lt;- matrix (0, n * (n - 1) / 2, m * (m + 1) / 2)\n  l &lt;- 1\n  if (m == 1) {\n    g[, 1] &lt;- lower_triangle (outer (k, k))\n  }\n  else {\n    for (i in 1:m) {\n      g[, l] &lt;- lower_triangle (outer(k[, i], k[, i]))\n      l &lt;- l + 1\n    }\n    for (i in 1:(m - 1))\n      for (j in (i + 1):m) {\n        g[, l] &lt;-\n          lower_triangle (outer(k[, i], k[, j]) + outer(k[, j], k[, i]))\n        l &lt;- l + 1\n      }\n  }\n  return (g)\n}\n\niStress &lt;-\n  function (x,\n            delta,\n            w = rep (1, length (delta)),\n            only = TRUE) {\n    m &lt;- length (delta)\n    n &lt;- (1 + sqrt (1 + 8 * m)) / 2\n    x &lt;- matrix (x, n, length (x) / n)\n    d &lt;- lower_triangle (as.matrix (dist (x)))\n    g &lt;- makeG (x)\n    h &lt;- (d / w) * makeG (x)\n    u &lt;- -colSums(w * (delta - d) * h)\n    v &lt;- crossprod (h, w * h)\n    s &lt;- solve.QP (\n      Dmat = v,\n      dvec = u,\n      Amat = -t(h),\n      bvec = -d\n    )\n    ds &lt;- d  - h %*% s$solution\n    is &lt;- sum (w * (delta - ds) ^ 2)\n    if (only)\n      return (is)\n    else\n      return (list (istress = is, delta = fill_symmetric (ds)))\n  }\n\n\n\nglobal.R\n\ncheckUni &lt;- function (w, delta, x) {\n  x &lt;- drop (x)\n  n &lt;- length (x)\n  vinv &lt;- solve (smacofVmat (w) + (1 / n)) - (1 / n)\n  return (drop (vinv %*% rowSums (w * delta * sign (outer (\n    x, x, \"-\"\n  )))))\n}\n\nmatchMe &lt;- function (x,\n                     itmax = 100,\n                     eps = 1e-10,\n                     verbose = FALSE) {\n  m &lt;- length (x)\n  y &lt;- sumList (x) / m\n  itel &lt;- 1\n  fold &lt;- sum (sapply (x, function (z)\n    (z - y) ^ 2))\n  repeat {\n    for (j in 1:m) {\n      u &lt;- crossprod (x[[j]], y)\n      s &lt;- svd (u)\n      r &lt;- tcrossprod (s$u, s$v)\n      x[[j]] &lt;- x[[j]] %*% r\n    }\n    y &lt;- sumList (x) / m\n    fnew &lt;- sum (sapply (x, function (z)\n      (z - y) ^ 2))\n    if (verbose) {\n      \n    }\n    if (((fold - fnew) &lt; eps) || (itel == itmax))\n      break\n    itel &lt;- itel + 1\n    fold &lt;- fnew\n  }\n  return (x)\n}\n\nsumList &lt;- function (x) {\n  m &lt;- length (x)\n  y &lt;- x[[1]]\n  for (j in 2:m) {\n    y &lt;- y + x[[j]]\n  }\n  return (y)\n}\n\n\nsmacofLoss &lt;- function (d, w, delta) {\n  return (sum (w * (delta - d) ^ 2) / 4)\n}\n\nsmacofBmat &lt;- function (d, w, delta) {\n  dd &lt;- ifelse (d == 0, 0, 1 / d)\n  b &lt;- -dd * w * delta\n  diag (b) &lt;- -rowSums (b)\n  return(b)\n}\n\nsmacofVmat &lt;- function (w) {\n  v &lt;- -w\n  diag(v) &lt;- -rowSums(v)\n  return (v)\n}\n\nsmacofGuttman &lt;- function (x, b, vinv) {\n  return (vinv %*% b %*% x)\n}\n\ncolumnCenter &lt;- function (x) {\n  return (apply (x, 2, function (z)\n    z - mean (z)))\n}\n\nsmacofComplement &lt;- function (y, v) {\n  return (sum (v * tcrossprod (y)) / 4)\n}\n\nsmacofPenalty &lt;-\n  function (w,\n            delta,\n            p = 2,\n            lbd = 0,\n            zold = columnCenter (diag (nrow (delta))),\n            itmax = 10000,\n            eps = 1e-10,\n            verbose = FALSE) {\n    itel &lt;- 1\n    n &lt;- nrow (zold)\n    vmat &lt;- smacofVmat (w)\n    vinv &lt;- solve (vmat + (1 / n)) - (1 / n)\n    dold &lt;- as.matrix (dist (zold))\n    mold &lt;- sum (w * delta * dold) / sum (w * dold * dold)\n    zold &lt;- zold * mold\n    dold &lt;- dold * mold\n    yold &lt;- zold [, (p + 1):n]\n    sold &lt;- smacofLoss (dold, w, delta)\n    bold &lt;- smacofBmat (dold, w, delta)\n    told &lt;- smacofComplement (yold, vmat)\n    uold &lt;- sold + lbd * told\n    repeat {\n      znew &lt;- smacofGuttman (zold, bold, vinv)\n      ynew &lt;- znew [, (p + 1):n] / (1 + lbd)\n      znew [, (p + 1):n] &lt;- ynew\n      xnew &lt;- znew [, 1:p]\n      dnew &lt;- as.matrix (dist (znew))\n      bnew &lt;- smacofBmat (dnew, w, delta)\n      tnew &lt;- smacofComplement (ynew, vmat)\n      snew &lt;- smacofLoss (dnew, w, delta)\n      unew &lt;- snew + lbd * tnew\n      if (verbose) {\n        cat(\n          \"itel \",\n          formatC(itel, width = 4, format = \"d\"),\n          \"sold \",\n          formatC(\n            sold,\n            width = 10,\n            digits = 6,\n            format = \"f\"\n          ),\n          \"snew \",\n          formatC(\n            snew,\n            width = 10,\n            digits = 6,\n            format = \"f\"\n          ),\n          \"told \",\n          formatC(\n            told,\n            width = 10,\n            digits = 6,\n            format = \"f\"\n          ),\n          \"tnew \",\n          formatC(\n            tnew,\n            width = 10,\n            digits = 6,\n            format = \"f\"\n          ),\n          \"uold \",\n          formatC(\n            uold,\n            width = 10,\n            digits = 6,\n            format = \"f\"\n          ),\n          \"unew \",\n          formatC(\n            unew,\n            width = 10,\n            digits = 6,\n            format = \"f\"\n          ),\n          \"\\n\"\n        )\n      }\n      if (((uold - unew) &lt; eps) || (itel == itmax)) {\n        break\n      }\n      itel &lt;- itel + 1\n      zold &lt;- znew\n      bold &lt;- bnew\n      sold &lt;- snew\n      told &lt;- tnew\n      uold &lt;- unew\n    }\n    zpri &lt;- znew %*% svd(znew)$v\n    xpri &lt;- zpri[, 1:p]\n    return (list (\n      x = xpri,\n      z = zpri,\n      b = bnew,\n      l = lbd,\n      s = snew,\n      t = tnew,\n      itel = itel\n    ))\n  }\n\nplotMe2 &lt;- function(hList, labels, s = 1, t = 2) {\n  n &lt;- nrow(hList[[1]]$x)\n  m &lt;- length (hList)\n  par(pty = \"s\")\n  hMatch &lt;- matchMe (lapply (hList, function(r)\n    r$x))\n  hMat &lt;- matrix (0, 0, 2)\n  for (j in 1:m) {\n    hMat &lt;- rbind(hMat, hMatch[[j]][, c(s, t)])\n  }\n  plot(\n    hMat,\n    xlab = \"dim 1\",\n    ylab = \"dim 2\",\n    col = c(rep(\"RED\", n * (m - 1)), rep(\"BLUE\", n)),\n    cex = c(rep(1, n * (m - 1)), rep(2, n))\n  )\n  for (i in 1:n) {\n    hLine &lt;- matrix (0, 0, 2)\n    for (j in 1:m) {\n      hLine &lt;- rbind (hLine, hMatch[[j]][i, c(s, t)])\n    }\n    lines(hLine)\n  }\n  text(hMatch[[m]], labels, cex = .75)\n}\n\nplotMe1 &lt;- function(hList, labels) {\n  n &lt;- length (hList[[1]]$x)\n  m &lt;- length (hList)\n  blow &lt;- function (x) {\n    n &lt;- length (x)\n    return (matrix (c(1:n, x), n, 2))\n  }\n  hMat &lt;- matrix (0, 0, 2)\n  for (j in 1:m) {\n    hMat &lt;- rbind(hMat, blow(hList[[j]]$x))\n  }\n  plot(\n    hMat,\n    xlab = \"index\",\n    ylab = \"x\",\n    col = c(rep(\"RED\", n * (m - 1)), rep(\"BLUE\", n)),\n    cex = c(rep(1, n * (m - 1)), rep(2, n))\n  )\n  for (i in 1:n) {\n    hLine &lt;- matrix (0, 0, 2)\n    for (j in 1:m) {\n      hLine &lt;- rbind (hLine, blow(hList[[j]]$x)[i,])\n      lines(hLine)\n    }\n  }\n  text(blow(hList[[m]]$x), labels, cex = 1.00)\n  for (i in 1:n) {\n    abline(h = hList[[m]]$x[i])\n  }\n}\n\n\n\n\nrunPenalty &lt;-\n  function (w,\n            delta,\n            lbd,\n            p = 2,\n            itmax = 10000,\n            eps = 1e-10,\n            cut = 1e-6,\n            write = TRUE,\n            verbose = FALSE) {\n    m &lt;- length (lbd)\n    hList &lt;- as.list (1:m)\n    hList[[1]] &lt;-\n      smacofPenalty(\n        w,\n        delta,\n        p,\n        lbd = lbd[1],\n        itmax = itmax,\n        eps = eps,\n        verbose = verbose\n      )\n    for (j in 2:m) {\n      hList[[j]] &lt;-\n        smacofPenalty(\n          w,\n          delta,\n          p,\n          zold = hList[[j - 1]]$z,\n          lbd = lbd[j],\n          itmax = itmax,\n          eps = eps,\n          verbose = verbose\n        )\n    }\n    mm &lt;- m\n    for (i in 1:m) {\n      if (write) {\n        cat(\n          \"itel\",\n          formatC(hList[[i]]$itel, width = 4, format = \"d\"),\n          \"lambda\",\n          formatC(\n            hList[[i]]$l,\n            width = 10,\n            digits = 6,\n            format = \"f\"\n          ),\n          \"stress\",\n          formatC(\n            hList[[i]]$s,\n            width = 8,\n            digits = 6,\n            format = \"f\"\n          ),\n          \"penalty\",\n          formatC(\n            hList[[i]]$t,\n            width = 8,\n            digits = 6,\n            format = \"f\"\n          ),\n          \"\\n\"\n        )\n      }\n      if (hList[[i]]$t &lt; cut) {\n        mm &lt;- i\n        break\n      }\n    }\n    return(hList[1:mm])\n  }\n\nwriteSelected &lt;- function (hList, ind) {\n  m &lt;- length (hList)\n  n &lt;- length (ind)\n  mn &lt;- sort (union (union (1:3, ind), m - (2:0)))\n  for (i in mn) {\n    if (i &gt; m) {\n      next\n    }\n    cat(\n      \"itel\",\n      formatC(hList[[i]]$itel, width = 4, format = \"d\"),\n      \"lambda\",\n      formatC(\n        hList[[i]]$l,\n        width = 10,\n        digits = 6,\n        format = \"f\"\n      ),\n      \"stress\",\n      formatC(\n        hList[[i]]$s,\n        width = 8,\n        digits = 6,\n        format = \"f\"\n      ),\n      \"penalty\",\n      formatC(\n        hList[[i]]$t,\n        width = 8,\n        digits = 6,\n        format = \"f\"\n      ),\n      \"\\n\"\n    )\n  }\n}\n\nrhofun &lt;- function (a) {\n  rhomax &lt;- 0\n  rhoval &lt;- c(0, 0)\n  n &lt;- nrow (a)\n  for (i in 1:n) {\n    z &lt;- a[i, 1] * xbase + a[i, 2] * ybase\n    rho &lt;- sum (delta * dist (z))\n    if (rho &gt; rhomax) {\n      rhomax &lt;- rho\n      rhoval &lt;- a[i,]\n    }\n  }\n  return(list(rhomax = rhomax, rhoval = rhoval))\n}\n\nrhomax2Plot &lt;- function (inpoints) {\n  par(pty = \"s\")\n  s &lt;- (0:500) / 500 * 2 * pi\n  x &lt;- sin(s)\n  y &lt;- cos(s)\n  plot(\n    x,\n    y,\n    type = \"l\",\n    col = \"RED\",\n    xlim = c(-1.25, 1.25),\n    ylim = c(-1.25, 1.25),\n    lwd = 3\n  )\n  points(0, 0, cex = 1.2)\n  n &lt;- nrow(inpoints)\n  outpoints &lt;- matrix(0, n, 2)\n  for (i in 1:n) {\n    a &lt;- inpoints[i, ]\n    if (i == n) {\n      b &lt;- inpoints[1, ]\n    } else {\n      b &lt;- inpoints[i + 1, ]\n    }\n    d &lt;- a[1] * b[2] - a[2] * b[1]\n    outpoints[i, 1] &lt;- (b[2] - a[2]) / d\n    outpoints[i, 2] &lt;- (a[1] - b[1]) / d\n  }\n  lines (\n    x = inpoints[, 1],\n    y = inpoints[, 2],\n    col = \"BLUE\",\n    lwd = 2\n  )\n  lines (\n    x = inpoints[c(1, n), 1],\n    y = inpoints[c(1, n), 2],\n    col = \"BLUE\",\n    lwd = 2\n  )\n  lines (\n    x = outpoints[, 1],\n    y = outpoints[, 2],\n    col = \"BLUE\",\n    lwd = 2\n  )\n  lines (\n    x = outpoints[c(1, n), 1],\n    y = outpoints[c(1, n), 2],\n    col = \"BLUE\",\n    lwd = 2\n  )\n}\n\n\nrhomax2Comp &lt;-\n  function (inpoints,\n            itmax = 100,\n            eps = 1e-8,\n            verbose = TRUE) {\n    itel = 1\n    repeat {\n      n &lt;- nrow(inpoints)\n      outpoints &lt;- matrix(0, n, 2)\n      for (i in 1:n) {\n        a &lt;- inpoints[i, ]\n        if (i == n) {\n          b &lt;- inpoints[1, ]\n        } else {\n          b &lt;- inpoints[i + 1, ]\n        }\n        d &lt;- a[1] * b[2] - a[2] * b[1]\n        outpoints[i, 1] &lt;- (b[2] - a[2]) / d\n        outpoints[i, 2] &lt;- (a[1] - b[1]) / d\n      }\n      infun &lt;- rhofun (inpoints)\n      oufun &lt;- rhofun (outpoints)\n      inmax &lt;- infun$rhomax\n      oumax &lt;- oufun$rhomax\n      inval &lt;- infun$rhoval\n      ouval &lt;- oufun$rhoval\n      for (i in 1:n) {\n        outpoints[i, ] &lt;- outpoints[i, ] / sqrt (sum (outpoints[i, ] ^ 2))\n      }\n      impoints &lt;- matrix (0, 2 * n, 2)\n      impoints[seq.int(1, (2 * n) - 1, by = 2),] &lt;- inpoints\n      impoints[seq.int(2, 2 * n, by = 2),] &lt;- outpoints\n      if (verbose) {\n        cat(\n          \"itel \",\n          formatC(itel, width = 6, format = \"d\"),\n          \"vertices \",\n          formatC(n, width = 6, format = \"d\"),\n          \"innermax \",\n          formatC(\n            inmax,\n            digits = 8,\n            width = 15,\n            format = \"f\"\n          ),\n          \"outermax \",\n          formatC(\n            oumax,\n            digits = 8,\n            width = 15,\n            format = \"f\"\n          ),\n          \"\\n\"\n        )\n      }\n      if ((itel == itmax) || ((oumax - inmax) &lt; eps)) {\n        break\n      }\n      itel &lt;- itel + 1\n      inpoints &lt;- impoints\n    }\n    return (list (\n      itel = itel,\n      vertices = n,\n      inmax = inmax,\n      oumax = oumax,\n      inval = inval,\n      ouval = ouval\n    ))\n  }\n\n\n\nmathadd.R\n\nlsuw &lt;- function (y,\n                  w,\n                  proj,\n                  xold = rep (1, length (y)),\n                  v = max (eigen (w)$values)  * diag (length (y)),\n                  itmax = 100,\n                  eps = 1e-6,\n                  verbose = FALSE,\n                  add = 1e-6) {\n  f &lt;- function (x, y, w) {\n    return (sum ((x - y) * w %*% (x - y)))\n  }\n  n &lt;- length (y)\n  labels &lt;- c(\"itel\", \"fold\", \"fnew\")\n  digits &lt;- c(0, 6, 6)\n  widths &lt;- c(3, 10, 10)\n  formats &lt;- c(\"d\", \"f\", \"f\")\n  fold &lt;- f (xold, y, w)\n  itel &lt;- 1\n  repeat {\n    u &lt;- drop (solve (v, w %*% (xold - y)))\n    xnew &lt;- proj (xold - u, v)\n    fnew &lt;- f (xnew, y, w)\n    if (verbose) {\n      values &lt;- c(itel, fold, fnew)\n      iterWrite (labels, values, digits, widths, formats)\n    }\n    if ((itel == itmax) || ((fold - fnew) &lt; eps)) {\n      break\n    }\n    fold &lt;- fnew\n    xold &lt;- xnew\n    itel &lt;- itel + 1\n  }\n  return (list (x = xnew, f = fnew, itel = itel))\n}\n\nprojeq &lt;- function (x, v) {\n  s &lt;- sum (v)\n  h &lt;- sum (x * rowSums (v))\n  return (rep (h / s, length (x)))\n}\n\nprojplus &lt;- function (x, v) {\n  if (!all(v == diag(diag(v)))) {\n    stop (\"V must be diagonal\")\n  }\n  if (min (diag (v)) &lt; 0) {\n    stop (\"V must be positive semidefinite\")\n  }\n  return (pmax(x, 0))\n}\n\nqpmaj &lt;-\n  function (z,\n            v = diag (length (z)),\n            a = diff (diag (length(z))),\n            b = ifelse (!is.null(a), rep(0, nrow(a)), NULL),\n            c = NULL,\n            d = ifelse (!is.null(c), rep(0, nrow(c)), NULL),\n            h = NULL,\n            itmax = 1000,\n            eps = 1e-15,\n            verbose = FALSE) {\n    labs &lt;- c(\"itel\", \"fold\", \"fnew\")\n    digs &lt;- c(0, 6, 6)\n    wids &lt;- c(3, 10, 10)\n    fors &lt;- c(\"d\", \"f\", \"f\")\n    if (is.null(h)) {\n      w &lt;- v\n      y &lt;- z\n      rsum &lt;- 0\n    } else {\n      w &lt;- crossprod(h, v %*% h)\n      y &lt;- drop (solve (w, crossprod (h, v %*% z)))\n      rsum &lt;- sum (z * (v %*% (z - h %*% y))) / 2\n    }\n    winv &lt;- solve (w)\n    if (!is.null(a)) {\n      nin &lt;- nrow(a)\n      dualaa &lt;- a %*% winv %*% t(a)\n      feasa &lt;- drop ((a %*% y) - b)\n    }\n    if (!is.null(c)) {\n      neq &lt;- nrow (c)\n      dualcc &lt;- c %*% winv %*% t(c)\n      feasc &lt;- drop((c %*% y) - d)\n    }\n    if ((!is.null(a)) && (!is.null(c))) {\n      dualac &lt;- a %*% winv %*% t(c)\n      feas &lt;- c(feasa, feasc)\n      dual &lt;- rbind(cbind(dualaa, dualac), cbind(t(dualac), dualcc))\n    }\n    if ((!is.null(a)) && (is.null(c))) {\n      feas &lt;- feasa\n      dual &lt;- dualaa\n    }\n    if ((is.null(a)) && (!is.null(c))) {\n      feas &lt;- feasc\n      dual &lt;- dualcc\n    }\n    vmax &lt;- max(eigen(dual)$values)\n    itel &lt;- 1\n    lold &lt;- rep (0, nrow (dual))\n    fold &lt;-\n      -(sum (lold * drop (dual %*% lold)) / 2 +\n          sum (lold * feas))\n    repeat {\n      lnew &lt;- lold - (drop(dual %*% lold) + feas) / vmax\n      if (!is.null(a)) {\n        lnew[1:nin] &lt;- pmax(lnew[1:nin], 0)\n      }\n      fnew &lt;-\n        -(sum (lnew * drop (dual %*% lnew)) / 2 + sum (lnew * feas))\n      if (verbose) {\n        vals &lt;- c(itel, fold, fnew)\n        iterWrite (labs, vals, digs, wids, fors)\n      }\n      if ((itel == itmax) || ((fnew - fold) &lt; eps)) {\n        break\n      }\n      fold &lt;- fnew\n      lold &lt;- lnew\n      itel &lt;- itel + 1\n    }\n    fdua &lt;- fnew\n    if ((!is.null(c) && (!is.null(a)))) {\n      x &lt;- y + drop (winv %*% drop(cbind(t(a), t(c)) %*% lnew))\n      lb &lt;- lnew[1:nin]\n      mu &lt;- lnew[-(1:nin)]\n    }\n    if ((is.null(c) && (!is.null(a)))) {\n      x &lt;- y + drop (winv %*% drop (t(a) %*% lnew))\n      lb &lt;- lnew\n    }\n    if ((!is.null(c) && (is.null(a)))) {\n      x &lt;- y + drop (winv %*% drop (t(c) %*% lnew))\n      mu &lt;- lnew\n    }\n    fprs &lt;- sum ((x - y) * drop (w %*% (x - y))) / 2\n    out &lt;- list(x = x,\n                fprimal = fprs,\n                fdual = fdua)\n    if (!is.null(h)) {\n      out &lt;-\n        list.append(out, ftotal = fprs + rsum, predict = drop (h %*% x))\n    }\n    if (!is.null(a)) {\n      out &lt;-\n        list.append(out, lambda = lb, inequalities = drop (a %*% x - b))\n    }\n    if (!is.null(c)) {\n      out &lt;- list.append(out, mu = mu, equations = drop (c %*% x - d))\n    }\n    return (list.append(out, itel = itel))\n  }\n\n\ncheckIncreasing &lt;- function (innerknots, lowend, highend) {\n  h &lt;- .C(\n    \"checkIncreasing\",\n    as.double (innerknots),\n    as.double (lowend),\n    as.double (highend),\n    as.integer (length (innerknots)),\n    fail = as.integer (0)\n  )\n  return (h$fail)\n}\n\nextendPartition &lt;-\n  function (innerknots,\n            multiplicities,\n            order,\n            lowend,\n            highend) {\n    ninner &lt;- length (innerknots)\n    kk &lt;- sum(multiplicities)\n    nextended &lt;- kk + 2 * order\n    if (max (multiplicities) &gt; order)\n      stop (\"multiplicities too high\")\n    if (min (multiplicities) &lt; 1)\n      stop (\"multiplicities too low\")\n    if (checkIncreasing (innerknots, lowend, highend))\n      stop (\"knot sequence not increasing\")\n    h &lt;-\n      .C(\n        \"extendPartition\",\n        as.double (innerknots),\n        as.integer (multiplicities),\n        as.integer (order),\n        as.integer (ninner),\n        as.double (lowend),\n        as.double (highend),\n        knots = as.double (rep (0, nextended))\n      )\n    return (h)\n  }\n\nbisect &lt;-\n  function (x,\n            knots,\n            lowindex = 1,\n            highindex = length (knots)) {\n    h &lt;- .C(\n      \"bisect\",\n      as.double (x),\n      as.double (knots),\n      as.integer (lowindex),\n      as.integer (highindex),\n      index = as.integer (0)\n    )\n    return (h$index)\n  }\n\nbsplines &lt;- function (x, knots, order) {\n  if ((x &gt; knots[length(knots)]) || (x &lt; knots[1]))\n    stop (\"argument out of range\")\n  h &lt;- .C(\n    \"bsplines\",\n    as.double (x),\n    as.double (knots),\n    as.integer (order),\n    as.integer (length (knots)),\n    index = as.integer (0),\n    q = as.double (rep(0, order))\n  )\n  return (list (q = h$q, index = h$ind))\n}\n\n\nbsplineBasis &lt;- function (x, knots, order) {\n  n &lt;- length (x)\n  k &lt;- length (knots)\n  m &lt;- k - order\n  result &lt;- rep (0, n * m)\n  h &lt;- .C(\n    \"bsplineBasis\",\n    as.double (x),\n    as.double (knots),\n    as.integer (order),\n    as.integer (k),\n    as.integer (n),\n    result = as.double (result)\n  )\n  return (matrix (h$result, n, m))\n}\n\nisplineBasis &lt;-  function (x, knots, order) {\n  n &lt;- length (x)\n  k &lt;- length (knots)\n  m &lt;- k - order\n  result &lt;- rep (0, n * m)\n  h &lt;- .C(\n    \"isplineBasis\",\n    as.double (x),\n    as.double (knots),\n    as.integer (order),\n    as.integer (k),\n    as.integer (n),\n    result = as.double (result)\n  )\n  return (matrix (h$result, n, m))\n}",
    "crumbs": [
      "(APPENDIX) Appendices"
    ]
  },
  {
    "objectID": "backmatter.html#c-code",
    "href": "backmatter.html#c-code",
    "title": "(APPENDIX) Appendices",
    "section": "C code",
    "text": "C code\n\ndeboor.c\n\n\n#include &lt;math.h&gt;\n#include &lt;stdbool.h&gt;\n#include &lt;stdlib.h&gt;\n\ninline int VINDEX(const int);\ninline int MINDEX(const int, const int, const int);\ninline int IMIN(const int, const int);\ninline int IMIN(const int, const int);\n\nvoid checkIncreasing(const double *, const double *, const double *,\n                     const int *, bool *);\nvoid extendPartition(const double *, const int *, const int *, const int *,\n                     const double *, const double *, double *);\nvoid bisect(const double *, const double *, const int *, const int *, int *);\nvoid bsplines(const double *, const double *, const int *, const int *, int *,\n              double *);\nvoid bsplineBasis(const double *, const double *, const int *, const int *,\n                  const int *, double *);\nvoid isplineBasis(const double *, const double *, const int *, const int *,\n                  const int *, double *);\nvoid bsplineSparse(const double *, const double *, const int *, const int *,\n                   const int *, int *, double *);\nvoid isplineSparse(const double *, const double *, const int *, const int *,\n                   const int *, int *, double *);\n\ninline int VINDEX(const int i) { return i - 1; }\n\ninline int MINDEX(const int i, const int j, const int n) {\n    return (i - 1) + (j - 1) * n;\n}\n\ninline int IMIN(const int a, const int b) {\n    if (a &gt; b) return b;\n    return a;\n}\n\ninline int IMAX(const int a, const int b) {\n    if (a &lt; b) return b;\n    return a;\n}\n\nvoid checkIncreasing(const double *innerknots, const double *lowend,\n                     const double *highend, const int *ninner, bool *fail) {\n    *fail = false;\n    if (*lowend &gt;= innerknots[VINDEX(1)]) {\n        *fail = true;\n        return;\n    }\n    if (*highend &lt;= innerknots[VINDEX(*ninner)]) {\n        *fail = true;\n        return;\n    }\n    for (int i = 1; i &lt; *ninner; i++) {\n        if (innerknots[i] &lt;= innerknots[i - 1]) {\n            *fail = true;\n            return;\n        }\n    }\n}\n\nvoid extendPartition(const double *innerknots, const int *multiplicities,\n                     const int *order, const int *ninner, const double *lowend,\n                     const double *highend, double *extended) {\n    int k = 1;\n    for (int i = 1; i &lt;= *order; i++) {\n        extended[VINDEX(k)] = *lowend;\n        k++;\n    }\n    for (int j = 1; j &lt;= *ninner; j++)\n        for (int i = 1; i &lt;= multiplicities[VINDEX(j)]; i++) {\n            extended[VINDEX(k)] = innerknots[VINDEX(j)];\n            k++;\n        }\n    for (int i = 1; i &lt;= *order; i++) {\n        extended[VINDEX(k)] = *highend;\n        k++;\n    }\n}\n\nvoid bisect(const double *x, const double *knots, const int *lowindex,\n            const int *highindex, int *index) {\n    int l = *lowindex, u = *highindex, mid = 0;\n    while ((u - l) &gt; 1) {\n        mid = (int)floor((u + l) / 2);\n        if (*x &lt; knots[VINDEX(mid)])\n            u = mid;\n        else\n            l = mid;\n    }\n    *index = l;\n    return;\n}\n\nvoid bsplines(const double *x, const double *knots, const int *order,\n              const int *nknots, int *index, double *q) {\n    int lowindex = 1, highindex = *nknots, m = *order, j, jp1;\n    double drr, dll, saved, term;\n    double *dr = (double *)calloc((size_t)m, sizeof(double));\n    double *dl = (double *)calloc((size_t)m, sizeof(double));\n    (void)bisect(x, knots, &lowindex, &highindex, index);\n    int l = *index;\n    for (j = 1; j &lt;= m; j++) {\n        q[VINDEX(j)] = 0.0;\n    }\n    if (*x == knots[VINDEX(*nknots)]) {\n        q[VINDEX(m)] = 1.0;\n        return;\n    }\n    q[VINDEX(1)] = 1.0;\n    j = 1;\n    if (j &gt;= m) return;\n    while (j &lt; m) {\n        dr[VINDEX(j)] = knots[VINDEX(l + j)] - *x;\n        dl[VINDEX(j)] = *x - knots[VINDEX(l + 1 - j)];\n        jp1 = j + 1;\n        saved = 0.0;\n        for (int r = 1; r &lt;= j; r++) {\n            drr = dr[VINDEX(r)];\n            dll = dl[VINDEX(jp1 - r)];\n            term = q[VINDEX(r)] / (drr + dll);\n            q[VINDEX(r)] = saved + drr * term;\n            saved = dll * term;\n        }\n        q[VINDEX(jp1)] = saved;\n        j = jp1;\n    }\n    free(dr);\n    free(dl);\n    return;\n}\n\nvoid bsplineBasis(const double *x, const double *knots, const int *order,\n                  const int *nknots, const int *nvalues, double *result) {\n    int m = *order, l = 0;\n    double *q = (double *)calloc((size_t)m + 1, sizeof(double));\n    for (int i = 1; i &lt;= *nvalues; i++) {\n        (void)bsplines(x + VINDEX(i), knots, order, nknots, &l, q);\n        for (int j = 1; j &lt;= m; j++) {\n            int r = IMIN(l - m + j, *nknots - m);\n            result[MINDEX(i, r, *nvalues)] = q[VINDEX(j)];\n        }\n    }\n    free(q);\n    return;\n}\n\nvoid isplineBasis(const double *x, const double *knots, const int *order,\n                  const int *nknots, const int *nvalues, double *result) {\n    int m = *nknots - *order, n = *nvalues;\n    (void)bsplineBasis(x, knots, order, nknots, nvalues, result);\n    for (int i = 1; i &lt;= n; i++) {\n        for (int j = m - 1; j &gt;= 1; j--) {\n            result[MINDEX(i, j, n)] += result[MINDEX(i, j + 1, n)];\n        }\n    }\n    return;\n}\n\nvoid bsplineSparse(const double *x, const double *knots, const int *order,\n                   const int *nknots, const int *nvalues, int *columns,\n                   double *result) {\n    int m = *order, l = 0;\n    double *q = (double *)calloc((size_t)m + 1, sizeof(double));\n    for (int i = 1; i &lt;= *nvalues; i++) {\n        (void)bsplines(x + VINDEX(i), knots, order, nknots, &l, q);\n        columns[VINDEX(i)] = l;\n        for (int j = 1; j &lt;= m; j++) {\n            result[MINDEX(i, j, *nvalues)] = q[VINDEX(j)];\n        }\n    }\n    free(q);\n    return;\n}\n\nvoid isplineSparse(const double *x, const double *knots, const int *order,\n                   const int *nknots, const int *nvalues, int *columns,\n                   double *result) {\n    int m = *nknots - *order, n = *nvalues;\n    for (int i = 1; i &lt;= *nvalues; i++) {\n        (void)bsplineSparse(x + VINDEX(i), knots, order, nknots, nvalues,\n                            columns, result);\n        for (int j = m - 1; j &gt;= 1; j--) {\n            result[MINDEX(i, j, n)] += result[MINDEX(i, j + 1, n)];\n        }\n    }\n    return;\n}\n\n\n\ncleanup.c\n\n#include &lt;math.h&gt;\n\n#define MAX(a, b) (((a) &gt; (b)) ? (a) : (b))\n\nvoid cleanup(double *a, int *n, int *m, int *ind, double *eps) {\n    int i, j, l, ii, jj, nn = *n, mm = *m;\n    double s;\n    for (i = 0; i &lt; (nn - 1); i++) {\n        ii = i * mm;\n        for (j = (i + 1); j &lt; nn; j++) {\n            s = 0.0;\n            jj = j * mm;\n            if (ind[j] == 0) continue;\n            for (l = 0; l &lt; mm; l++) {\n                s = MAX(s, fabs(*(a + ii + l) - *(a + jj + l)));\n            }\n            if (s &lt; *eps) {\n              ind[j] = 0;\n            }\n        }\n    }\n}\n\n\n\njacobi.c\n\n#include \"jacobi.h\"\n\nvoid jacobiC(const int *nn, double *a, double *evec, double *oldi, double *oldj,\n             int *itmax, double *eps) {\n    int n = *nn, itel = 1;\n    double d = 0.0, s = 0.0, t = 0.0, u = 0.0, v = 0.0, p = 0.0, q = 0.0,\n           r = 0.0;\n    double fold = 0.0, fnew = 0.0;\n    for (int i = 1; i &lt;= n; i++) {\n        for (int j = 1; j &lt;= n; j++) {\n            evec[MINDEX(i, j, n)] = (i == j) ? 1.0 : 0.0;\n        }\n    }\n    for (int i = 1; i &lt;= n; i++) {\n        fold += SQUARE(a[TINDEX(i, i, n)]);\n    }\n    while (true) {\n        for (int j = 1; j &lt;= n - 1; j++) {\n            for (int i = j + 1; i &lt;= n; i++) {\n                p = a[TINDEX(i, j, n)];\n                q = a[TINDEX(i, i, n)];\n                r = a[TINDEX(j, j, n)];\n                if (fabs(p) &lt; 1e-10) continue;\n                d = (q - r) / 2.0;\n                s = (p &lt; 0) ? -1.0 : 1.0;\n                t = -d / sqrt(SQUARE(d) + SQUARE(p));\n                u = sqrt((1 + t) / 2);\n                v = s * sqrt((1 - t) / 2);\n                for (int k = 1; k &lt;= n; k++) {\n                    int ik = IMIN(i, k);\n                    int ki = IMAX(i, k);\n                    int jk = IMIN(j, k);\n                    int kj = IMAX(j, k);\n                    oldi[VINDEX(k)] = a[TINDEX(ki, ik, n)];\n                    oldj[VINDEX(k)] = a[TINDEX(kj, jk, n)];\n                }\n                for (int k = 1; k &lt;= n; k++) {\n                    int ik = IMIN(i, k);\n                    int ki = IMAX(i, k);\n                    int jk = IMIN(j, k);\n                    int kj = IMAX(j, k);\n                    a[TINDEX(ki, ik, n)] =\n                        u * oldi[VINDEX(k)] - v * oldj[VINDEX(k)];\n                    a[TINDEX(kj, jk, n)] =\n                        v * oldi[VINDEX(k)] + u * oldj[VINDEX(k)];\n                }\n                for (int k = 1; k &lt;= n; k++) {\n                    oldi[VINDEX(k)] = evec[MINDEX(k, i, n)];\n                    oldj[VINDEX(k)] = evec[MINDEX(k, j, n)];\n                    evec[MINDEX(k, i, n)] =\n                        u * oldi[VINDEX(k)] - v * oldj[VINDEX(k)];\n                    evec[MINDEX(k, j, n)] =\n                        v * oldi[VINDEX(k)] + u * oldj[VINDEX(k)];\n                }\n                a[TINDEX(i, i, n)] =\n                    SQUARE(u) * q + SQUARE(v) * r - 2 * u * v * p;\n                a[TINDEX(j, j, n)] =\n                    SQUARE(v) * q + SQUARE(u) * r + 2 * u * v * p;\n                a[TINDEX(i, j, n)] =\n                    u * v * (q - r) + (SQUARE(u) - SQUARE(v)) * p;\n            }\n        }\n        fnew = 0.0;\n        for (int i = 1; i &lt;= n; i++) {\n            fnew += SQUARE(a[TINDEX(i, i, n)]);\n        }\n        if (((fnew - fold) &lt; *eps) || (itel == *itmax)) break;\n        fold = fnew;\n        itel++;\n    }\n    return;\n}\n\nvoid primat(const int *n, const int *m, const int *w, const int *p,\n            const double *x) {\n    for (int i = 1; i &lt;= *n; i++) {\n        for (int j = 1; j &lt;= *m; j++) {\n            printf(\" %*.*f \", *w, *p, x[MINDEX(i, j, *n)]);\n        }\n        printf(\"\\n\");\n    }\n    printf(\"\\n\\n\");\n    return;\n}\n\nvoid pritru(const int *n, const int *w, const int *p, const double *x) {\n    for (int i = 1; i &lt;= *n; i++) {\n        for (int j = 1; j &lt;= i; j++) {\n            printf(\" %*.*f \", *w, *p, x[TINDEX(i, j, *n)]);\n        }\n        printf(\"\\n\");\n    }\n    printf(\"\\n\\n\");\n    return;\n}\n\nvoid trimat(const int *n, const double *x, double *y) {\n    int nn = *n;\n    for (int i = 1; i &lt;= nn; i++) {\n        for (int j = 1; j &lt;= nn; j++) {\n            y[MINDEX(i, j, nn)] =\n                (i &gt;= j) ? x[TINDEX(i, j, nn)] : x[TINDEX(j, i, nn)];\n        }\n    }\n    return;\n}\n\nvoid mattri(const int *n, const double *x, double *y) {\n    int nn = *n;\n    for (int j = 1; j &lt;= nn; j++) {\n        for (int i = j; i &lt;= nn; i++) {\n            y[TINDEX(i, j, nn)] = x[MINDEX(i, j, nn)];\n        }\n    }\n    return;\n}\n\n\n\njbkTies.c\n\n\n#include &lt;math.h&gt;\n#include &lt;stdbool.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n#define DEBUG false\n\nstruct block {\n    double value;\n    double weight;\n    int size;\n    int previous;\n    int next;\n};\n\nstruct quadruple {\n    double value;\n    double result;\n    double weight;\n    int index;\n};\n\nstruct triple {\n    double value;\n    double weight;\n    int index;\n};\n\nstruct pair {\n    int value;\n    int index;\n};\n\nint myCompDouble(const void *, const void *);\nint myCompInteger(const void *, const void *);\nvoid mySortDouble(double *, double *, double *, int *, const int *);\nvoid mySortInteger(int *, int *, const int *);\nvoid mySortInBlock(double *, double *, int *, int *);\nvoid tieBlock(double *, int *, const int *, int *);\nvoid makeBlocks(double *, double *, double *, double *, int *, const int *,\n                const int *);\nvoid sortBlocks(double *, double *, int *, const int *, const int *,\n                const int *);\nvoid jbkPava(double *, double *, const int *);\nvoid primary(double *, double *, int *, int *, const int *, const int *);\nvoid secondary(double *, double *, int *, const int *, const int *);\nvoid tertiary(double *, double *, int *, const int *, const int *);\nvoid monreg(double *, double *, double *, const int *, const int *);\n\nint myCompDouble(const void *px, const void *py) {\n    double x = ((struct quadruple *)px)-&gt;value;\n    double y = ((struct quadruple *)py)-&gt;value;\n    return (int)copysign(1.0, x - y);\n}\n\nint myCompInteger(const void *px, const void *py) {\n    int x = ((struct pair *)px)-&gt;value;\n    int y = ((struct pair *)py)-&gt;value;\n    return (int)copysign(1.0, x - y);\n}\n\nvoid mySortInBlock(double *x, double *w, int *xind, int *n) {\n    int nn = *n;\n    struct triple *xi =\n        (struct triple *)calloc((size_t)nn, (size_t)sizeof(struct triple));\n    for (int i = 0; i &lt; nn; i++) {\n        xi[i].value = x[i];\n        xi[i].weight = w[i];\n        xi[i].index = xind[i];\n    }\n    (void)qsort(xi, (size_t)nn, (size_t)sizeof(struct triple), myCompDouble);\n    for (int i = 0; i &lt; nn; i++) {\n        x[i] = xi[i].value;\n        w[i] = xi[i].weight;\n        xind[i] = xi[i].index;\n    }\n    free(xi);\n    return;\n}\n\nvoid mySortDouble(double *x, double *y, double *w, int *xind, const int *n) {\n    int nn = *n;\n    struct quadruple *xi = (struct quadruple *)calloc(\n        (size_t)nn, (size_t)sizeof(struct quadruple));\n    for (int i = 0; i &lt; nn; i++) {\n        xi[i].value = x[i];\n        xi[i].result = y[i];\n        xi[i].weight = w[i];\n        xi[i].index = i + 1;\n    }\n    (void)qsort(xi, (size_t)nn, (size_t)sizeof(struct quadruple), myCompDouble);\n    for (int i = 0; i &lt; nn; i++) {\n        x[i] = xi[i].value;\n        y[i] = xi[i].result;\n        w[i] = xi[i].weight;\n        xind[i] = xi[i].index;\n    }\n    free(xi);\n    return;\n}\n\nvoid mySortInteger(int *x, int *k, const int *n) {\n    int nn = *n;\n    struct pair *xi =\n        (struct pair *)calloc((size_t)nn, (size_t)sizeof(struct pair));\n    for (int i = 0; i &lt; nn; i++) {\n        xi[i].value = x[i];\n        xi[i].index = i + 1;\n    }\n    (void)qsort(xi, (size_t)nn, (size_t)sizeof(struct pair), myCompInteger);\n    for (int i = 0; i &lt; nn; i++) {\n        x[i] = xi[i].value;\n        k[i] = xi[i].index;\n    }\n    free(xi);\n    return;\n}\n\nvoid tieBlock(double *x, int *iblks, const int *n, int *nblk) {\n    iblks[0] = 1;\n    for (int i = 1; i &lt; *n; i++) {\n        if (x[i - 1] == x[i]) {\n            iblks[i] = iblks[i - 1];\n        } else {\n            iblks[i] = iblks[i - 1] + 1;\n        }\n    }\n    *nblk = iblks[*n - 1];\n    return;\n}\n\nvoid makeBlocks(double *x, double *w, double *xblks, double *wblks, int *iblks,\n                const int *n, const int *nblk) {\n    for (int i = 0; i &lt; *nblk; i++) {\n        xblks[i] = 0.0;\n        wblks[i] = 0.0;\n    }\n    for (int i = 0; i &lt; *n; i++) {\n        xblks[iblks[i] - 1] += w[i] * x[i];\n        wblks[iblks[i] - 1] += w[i];\n    }\n    for (int i = 0; i &lt; *nblk; i++) {\n        xblks[i] = xblks[i] / wblks[i];\n    }\n    return;\n}\n\nvoid sortBlocks(double *y, double *w, int *xind, const int *iblks, const int *n,\n                const int *nblk) {\n    int *nblks = (int *)calloc((size_t)*nblk, sizeof(int));\n    for (int i = 0; i &lt; *n; i++) {\n        nblks[iblks[i] - 1]++;\n    }\n    int k = 0;\n    for (int i = 0; i &lt; *nblk; i++) {\n        int nn = nblks[i];\n        (void)mySortInBlock(y + k, w + k, xind + k, &nn);\n        k += nn;\n    }\n    free(nblks);\n    return;\n}\n\nvoid jbkPava(double *x, double *w, const int *n) {\n    struct block *blocks = calloc((size_t)*n, sizeof(struct block));\n    for (int i = 0; i &lt; *n; i++) {\n        blocks[i].value = x[i];\n        blocks[i].weight = w[i];\n        blocks[i].size = 1;\n        blocks[i].previous = i - 1;  // index first element previous block\n        blocks[i].next = i + 1;      // index first element next block\n    }\n    int active = 0;\n    do {\n        bool upsatisfied = false;\n        int next = blocks[active].next;\n        if (next == *n)\n            upsatisfied = true;\n        else if (blocks[next].value &gt; blocks[active].value)\n            upsatisfied = true;\n        if (!upsatisfied) {\n            double ww = blocks[active].weight + blocks[next].weight;\n            int nextnext = blocks[next].next;\n            blocks[active].value =\n                (blocks[active].weight * blocks[active].value +\n                 blocks[next].weight * blocks[next].value) /\n                ww;\n            blocks[active].weight = ww;\n            blocks[active].size += blocks[next].size;\n            blocks[active].next = nextnext;\n            if (nextnext &lt; *n) blocks[nextnext].previous = active;\n            blocks[next].size = 0;\n        }\n        bool downsatisfied = false;\n        int previous = blocks[active].previous;\n        if (previous == -1)\n            downsatisfied = true;\n        else if (blocks[previous].value &lt; blocks[active].value)\n            downsatisfied = true;\n        if (!downsatisfied) {\n            double ww = blocks[active].weight + blocks[previous].weight;\n            int previousprevious = blocks[previous].previous;\n            blocks[active].value =\n                (blocks[active].weight * blocks[active].value +\n                 blocks[previous].weight * blocks[previous].value) /\n                ww;\n            blocks[active].weight = ww;\n            blocks[active].size += blocks[previous].size;\n            blocks[active].previous = previousprevious;\n            if (previousprevious &gt; -1) blocks[previousprevious].next = active;\n            blocks[previous].size = 0;\n        }\n        if ((blocks[active].next == *n) && downsatisfied) break;\n        if (upsatisfied && downsatisfied) active = next;\n    } while (true);\n    int k = 0;\n    for (int i = 0; i &lt; *n; i++) {\n        int blksize = blocks[i].size;\n        if (blksize &gt; 0.0) {\n            for (int j = 0; j &lt; blksize; j++) {\n                x[k] = blocks[i].value;\n                k++;\n            }\n        }\n    }\n    free(blocks);\n}\n\n\n\nmatrix.c\n\n#include \"smacof.h\"\n\nvoid gsC(double *x, double *r, int *n, int *m, int *rank, int *pivot,\n         double *eps) {\n    int i, j, ip, nn = *n, mm = *m, rk = *m, jwork = 1;\n    double s = 0.0, p;\n    for (j = 1; j &lt;= mm; j++) {\n        pivot[j - 1] = j;\n    }\n    while (jwork &lt;= rk) {\n        for (j = 1; j &lt; jwork; j++) {\n            s = 0.0;\n            for (i = 1; i &lt;= nn; i++) {\n                s += x[MINDEX(i, jwork, nn)] * x[MINDEX(i, j, nn)];\n            }\n            r[MINDEX(j, jwork, mm)] = s;\n            for (i = 1; i &lt;= nn; i++) {\n                x[MINDEX(i, jwork, nn)] -= s * x[MINDEX(i, j, nn)];\n            }\n        }\n        s = 0.0;\n        for (i = 1; i &lt;= nn; i++) {\n            s += x[MINDEX(i, jwork, nn)] * x[MINDEX(i, jwork, nn)];\n        }\n        if (s &gt; *eps) {\n            s = sqrt(s);\n            r[MINDEX(jwork, jwork, mm)] = s;\n            for (i = 1; i &lt;= nn; i++) {\n                x[MINDEX(i, jwork, nn)] /= s;\n            }\n            jwork += 1;\n        }\n        if (s &lt;= *eps) {\n            ip = pivot[rk - 1];\n            pivot[rk - 1] = pivot[jwork - 1];\n            pivot[jwork - 1] = ip;\n            for (i = 1; i &lt;= nn; i++) {\n                p = x[MINDEX(i, rk, nn)];\n                x[MINDEX(i, rk, nn)] = x[MINDEX(i, jwork, nn)];\n                x[MINDEX(i, jwork, nn)] = p;\n            }\n            for (j = 1; j &lt;= mm; j++) {\n                p = r[MINDEX(j, rk, mm)];\n                r[MINDEX(j, rk, mm)] = r[MINDEX(j, jwork, mm)];\n                r[MINDEX(j, jwork, mm)] = p;\n            }\n            rk -= 1;\n        }\n    }\n    *rank = rk;\n}\n\n\n\njeffrey.c\n\n// This implements the formulas in\n//\n// D.J. Jeffrey\n// Formulae, Algorithms, and Quartic Extrema\n// Mathematics Magazine, 1997, 70(5), 341-348\n//\n// to find the minimum of a quartic polynomial.\n\n#include &lt;math.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n#define SQ(x) ((x) * (x))\n#define CU(x) ((x) * (x) * (x))\n#define QU(x) ((x) * (x) * (x) * (x))\n\nvoid smacofJeffrey(double *a, double *minwhere, double *minvalue) {\n    if (a[4] &lt;= 0.0) {\n        printf(\"Quartic term must be positive. Exiting...\\n\");\n        exit (EXIT_FAILURE);\n    }\n    double b0 = 0.0, b1 = 0.0, b2 = 0.0;\n    b2 += a[2] / (3.0 * a[4]);\n    b2 -= (SQ(a[3])) / (8.0 * SQ(a[4]));\n    b1 += a[1] / (2.0 * a[4]);\n    b1 += CU(a[3]) / (16.0 * CU(a[4]));\n    b1 -= (a[2] * a[3]) / (4.0 * SQ(a[4]));\n    b0 += (a[2] * SQ(a[3])) / (16.0 * SQ(a[4]));\n    b0 -= (a[1] * a[3]) / (4.0 * a[4]);\n    b0 -= (3.0 * QU(a[3])) / (256.0 * CU(a[4]));\n    if (fabs(b1) &gt; 1e-15) {\n        double s = SQ(b1) + CU(b2) + sqrt(QU(b1) + 2 * SQ(b1) * CU(b2));\n        double t = pow(s, (double) 1 / 3);\n        double k = t + (SQ(b2) / t) + b2;\n        double infp = -0.75 * (k - b2) * (k - 3.0 * b2);\n        *minwhere = -b1 / k - 0.25 * a[3] / a[4];\n        *minvalue = infp * a[4] + a[0] + b0;\n    } else {\n        double infp = -2.25 * SQ(fmin(0.0, b2));;\n        *minwhere = sqrt(-fmin(0.0, 1.5 * b2));\n        *minvalue = infp * a[4] + a[0] + b0;\n    }\n    return;\n}\n\n\n\nsmacofBlockSort.c\n\n/*\n\nsmacofBlockSort() is a routine to transform a matrix of dissimilarities (in\nlower-triangular column-major storage) into a ordinal multidimensional scaling\nstructure (OMDS structure). An OMDS structure is an array of tie-blocks, where\neach tie-block corresponds with a unique dissimilarity value. Tie-blocks have a\nvalue, a size, a vector of weights, and a vector of indices. They are strictly \nordered by increasing value. The routine is written for the smacof project, \nbut it can be used as a preprocessor for any monotone regression problem.\n\n*/\n\n#include \"../include/smacof.h\"\n\n\nint sortComp(const void *px, const void *py) {\n    double x = ((struct triple *)px)-&gt;value;\n    double y = ((struct triple *)py)-&gt;value;\n    return (int)copysign(1.0, x - y);\n}\n\nvoid smacofBlockSort(const double *x, const double *w, const int n, int nblock,\n                 block *yi) {\n    triple *xi = (triple *)calloc((size_t)n, sizeof(triple));\n    yi = (struct block *)calloc((size_t)n, (size_t)sizeof(block));\n    for (int i = 0; i &lt; n; i++) {\n        xi[i].value = x[i];\n        xi[i].weight = w[i];\n        xi[i].index = i;\n    }\n    (void)qsort(xi, (size_t)n, (size_t)sizeof(triple), sortComp);\n    int counter = 0;\n    while (counter &lt; n) {\n        double value = xi[counter].value;\n        int size = 0;\n        for (int j = counter; j &lt; n; j++) {\n            if (xi[j].value == value) {\n                size += 1;\n            } else {\n                break;\n            }\n        }\n        yi[nblock].size = size;\n        yi[nblock].value = value;\n        yi[nblock].indices = (int *)calloc((size_t)size, sizeof(int));\n        yi[nblock].weights = (double *)calloc((size_t)size, sizeof(double));\n        for (int i = 0; i &lt; size; i++) {\n            yi[nblock].indices[i] = xi[counter + i].index;\n            yi[nblock].weights[i] = xi[counter + i].weight;\n        }\n        counter += size;\n        printf(\"nblock %4d value %4.1f size %4d counter %4d\", nblock,\n               yi[nblock].value, yi[nblock].size, counter);\n        printf(\"\\nindices\");\n        for (int i = 0; i &lt; size; i++) {\n            printf(\"%4d\", yi[nblock].indices[i] + 1);\n        }\n        printf(\"\\nweights\");\n        for (int i = 0; i &lt; size; i++) {\n            printf(\"%4.1f\", yi[nblock].weights[i]);\n        }\n        printf(\"\\n\");\n        if (counter == n) {\n            break;\n        } else {\n            nblock++;\n        }\n    }\n    yi = (block *)realloc(yi, (size_t)nblock * sizeof(block));\n    free(xi);\n    return;\n}\n\n/*\ndouble x1[10] = {3.0, 1.0, 1.0, 5.0, 1.0, 5.0, 1.0, 2.0, 5.0, 2.0};\ndouble x2[10] = {1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0};\ndouble x3[10] = {0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0};\ndouble ww[10] = {1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0};\nint n = 10;\n\nint main() {\n    block *yi = NULL;\n    int nblock = 1;\n    (void)smacofBlockSort(x1, ww, n, nblock, yi);\n    printf(\"**********************************\\n\");\n    (void)smacofBlockSort(x2, ww, n, nblock, yi);\n    printf(\"**********************************\\n\");\n    (void)smacofBlockSort(x3, ww, n, nblock, yi);\n    free(yi);\n}\n*/\n\n\n\nsmacofConvert.c\n\n#include \"../include/smacof.h\"\n\nvoid smacofEncode(const int *ip, const int *jp, const int *np, int *kp) {\n    int i = *ip, j = *jp, n = *np;\n    *kp = i + (j - 1) * n - j * (j + 1) / 2;\n    return;\n}\n\nvoid smacofDecode(const int *kp, const int *np, int *ip, int *jp) {\n    int j = 1, m = 1, k = *kp, n = *np;\n    while (k &gt;= ((j * n) - m + 1)) {\n        j += 1;\n        m += j;\n    }\n    *ip = k - (j - 1) * n + m;\n    *jp = j;\n    return;\n}\n\n\n\n\n\n\n\n\n/*\nint main(void) {\n    int i = 0, j = 0, k = 0, n = 6;\n    printf(\"ENCODE\\n\\n\");\n    for (j = 1; j &lt; n; j++) {\n        for (i = (j + 1); i &lt;= n; i++) {\n            (void)dencode(&i, &j, &n, &k);\n            printf(\" %4d \", k);\n        }\n    }\n    printf(\"\\n\\n\");\n    printf(\"DECODE\\n\\n\");\n    for (int k = 1; k &lt;= n * (n - 1) / 2; k++) {\n        (void)ddecode(&k, &n, &i, &j);\n        printf(\"%4d %4d\\n\", i, j);\n    }\n    return EXIT_SUCCESS;\n}\n*/\n\n\n\nnextPC.c\n\n\nvoid swap(int* x, int i, int j) {\n    int temp;\n    temp = x[i];\n    x[i] = x[j];\n    x[j] = temp;\n}\n\nvoid nextPermutation(int* x, int* nn) {\n    int i, j, n = *nn;\n    i = n - 1;\n    while (x[i - 1] &gt;= x[i]) i--;\n    if (i == 0) return;\n    j = n;\n    while (x[j - 1] &lt;= x[i - 1]) j--;\n    swap(x, i - 1, j - 1);\n    j = n;\n    i++;\n    while (i &lt; j) {\n        swap(x, i - 1, j - 1);\n        j--;\n        i++;\n    }\n}\n\nvoid nextCombination(int* n, int* m, int* next) {\n    int i, j, mm = *m - 1, nn = *n;\n    for (i = mm; i &gt;= 0; i--) {\n        if (next[i] != nn - mm + i) {\n            next[i]++;\n            if (i &lt; mm) {\n                for (j = i + 1; j &lt;= mm; j++) next[j] = next[j - 1] + 1;\n            }\n            return;\n        }\n    }\n}",
    "crumbs": [
      "(APPENDIX) Appendices"
    ]
  },
  {
    "objectID": "backmatter.html#apdatadsmall",
    "href": "backmatter.html#apdatadsmall",
    "title": "(APPENDIX) Appendices",
    "section": "Small",
    "text": "Small\n\n\n  1 2 3\n2 1    \n3 2 4  \n4 5 2 1",
    "crumbs": [
      "(APPENDIX) Appendices"
    ]
  },
  {
    "objectID": "backmatter.html#apdatagruijter",
    "href": "backmatter.html#apdatagruijter",
    "title": "(APPENDIX) Appendices",
    "section": "De Gruijter",
    "text": "De Gruijter\n\n\n      KVP PvdA  VVD  ARP  CHU  CPN  PSP   BP\nPvdA 5.63                                   \nVVD  5.27 6.72                              \nARP  4.60 5.64 5.46                         \nCHU  4.80 6.22 4.97 3.20                    \nCPN  7.54 5.12 8.13 7.84 7.80               \nPSP  6.73 4.59 7.55 6.73 7.08 4.08          \nBP   7.18 7.22 6.90 7.28 6.96 6.34 6.88     \nD66  6.17 5.47 4.67 6.13 6.04 7.42 6.36 7.36",
    "crumbs": [
      "(APPENDIX) Appendices"
    ]
  },
  {
    "objectID": "backmatter.html#apdataekman",
    "href": "backmatter.html#apdataekman",
    "title": "(APPENDIX) Appendices",
    "section": "Ekman",
    "text": "Ekman",
    "crumbs": [
      "(APPENDIX) Appendices"
    ]
  },
  {
    "objectID": "backmatter.html#apdataveg",
    "href": "backmatter.html#apdataveg",
    "title": "(APPENDIX) Appendices",
    "section": "Vegetables",
    "text": "Vegetables\n\nveg &lt;- abs (qnorm (matrix (c(.500,.818,.770,.811,.878,.892,.899,.892,.926,\n.182,.500,.601,.723,.743,.736,.811,.845,.858,\n.230,.399,.500,.561,.736,.676,.845,.797,.818,\n.189,.277,.439,.500,.561,.588,.676,.601,.730,\n.122,.257,.264,.439,.500,.493,.574,.709,.764,\n.108,.264,.324,.412,.507,.500,.628,.682,.628,\n.101,.189,.155,.324,.426,.372,.500,.527,.642,\n.108,.155,.203,.399,.291,.318,.473,.500,.628,\n.074,.142,.182,.270,.236,.372,.358,.372,.500), 9, 9)))",
    "crumbs": [
      "(APPENDIX) Appendices"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Avis, D. 2015. User’s Guide for lrs - Version\n6.1. The Computational Geometry Lab at McGill.\n\n\nBacak, M., and J. M. Borwein. 2011. “On\nDifference Convexity of Locally Lipschitz Functions.”\nOptimization 60 (8-9): 961–78.\n\n\nBailey, R. A., and J. C. Gower. 1990. “Approximating a Symmetric Matrix.”\nPsychometrika 55 (4): 665–75.\n\n\nBauschke, H. H., M. N. Bui, and X. Wang. 2018. “Projecting onto the Intersection of a Cone and a\nSphere.” SIAM Journal on Optimization 28:\n2158–88.\n\n\nBavaud, F. 2011. “On the Schoenberg\nTransformations in Data Analysis: Theory and\nIllustrations.” Journal of Classification 28:\n297–314.\n\n\nBerge, C. 1963. Topological Spaces. Oliver & Boyd.\n\n\nBerkelaar, M. and others. 2015. lpSolve:\nInterface to ’Lp_solve’ v. 5.5 to Solve Linear/Integer\nPrograms.\n\n\nBest, M. J. 2017. Quadratic Programming with Computer Programs.\nAdvances in Applied Mathematics. CRC Press.\n\n\nBlumenthal, L. M. 1953. Theory and Applications of Distance\nGeometry. Oxford University Press.\n\n\nBorg, I., and P. J. F. Groenen. 2005. Modern Multidimensional\nScaling. Second Edition. Springer.\n\n\nBorg, I., and D. Leutner. 1983. “Dimensional\nModels for the Perception of Rectangles.” Perception\nand Psychophysics 34: 257–69.\n\n\nBorg, I., and J. C. Lingoes. 1980. “A Model\nand Algorithm for Multidimensional Scaling with External Constraints on\nthe Distances.” Psychometrika 45 (1): 25–38.\n\n\nBrowne, M. W. 1987. “The Young-Householder\nAlgorithm and the Least Squares Multidimensional Scaling of Squared\nDistances.” Journal of Classification 4: 175–90.\n\n\nCailliez, F. 1983. “The Analytical Solution\nto the Additive Constant Problem.” Psychometrika\n48 (2): 305–8.\n\n\nCarroll, J. D., and J. J. Chang. 1970. “Analysis of Individual Differences in Multidimensional\nscaling via an N-way generalization of \"Eckart-Young\"\nDecomposition.” Psychometrika 35: 283–319.\n\n\nCiomek, K. 2017. hasseDiagram: Drawing Hasse\nDiagram.\n\n\nCoombs, C. H. 1964. A Theory of\nData. Wiley.\n\n\nCooper, L. G. 1972. “A New Solution to the\nAdditive Constant Problem in Metric Multidimensional\nScaling.” Psychometrika 37 (3): 311–22.\n\n\nCox, D. R., and L. Brandwood. 1959. “On a\nDiscriminatory Problem Connected with the Works of Plato.”\nJournal of the Royal Statistical Society, Series B 21: 195–200.\n\n\nCox, M. G. 1972. “The Numerical Evaluation of\nB-splines.” Journal of the Institute of Mathematics\nand Its Applications 10: 134–49.\n\n\nCox, T. F., and M. A. A. Cox. 1991. “Multidimensional Scaling on a Sphere.”\nCommunications in Statistics 20: 2943–53.\n\n\n———. 2001. Multidimensional Scaling. Second Edition. Monographs\non Statistics and Applied Probability 88. Chapman & Hall.\n\n\nCritchley, F. 1988. “On Certain Linear\nMappings Between Inner Product and Squared Distance\nMatrices.” Linear Algebra and Its Applications\n105: 91–107.\n\n\nDanskin, J. M. 1967. The Theory of Max-Min and Its Application to\nWeapons Allocation Problems. Springer.\n\n\nDatorro, J. 2018. Convex Optimization and Euclidean Distance\nGeometry. Second Edition. Palo Alto, CA: Meebo Publishing. https://ccrma.stanford.edu/~dattorro/0976401304.pdf.\n\n\nDe Boor, C. 1972. “On Calculating with\nB-splines. II. Integration.” Journal of Approximation\nTheory 6: 50–62.\n\n\n———. 1976. “Splines as Linear Combination of\nB-splines. A Survey.” In Approximation Theory\nII, edited by G. G. Lorentz, C. K. Chui, and L. L.\nSchumaker, 1–47. Academic Press.\n\n\n———. 2001. A Practical Guide to Splines. Revised Edition. New\nYork: Springer-Verlag.\n\n\nDe Boor, C., and K. Höllig. 1985. “B-splines\nwithout Divided Differences.” Technical Report 622.\nDepartment of Computer Science, University of Wisconsin-Madison.\n\n\nDe Boor, C., T. Lyche, and L. L. Schumaker. 1976. “On Calculating with B-splines. II.\nIntegration.” In Numerische\nMethoden der Approximationstheorie, edited by L. Collatz, G.\nMeinardus, and H. Werner, 123–46. Basel: Birkhauser.\n\n\nDe Gruijter, D. N. M. 1967. “The Cognitive\nStructure of Dutch Political Parties in 1966.” Report\nE019-67. Psychological Institute, University of Leiden.\n\n\nDe Leeuw, J. 1968a. “Canonical Discriminant Analysis of Relational\nData.” Research Note 007-68. Department of Data Theory FSW/RUL.\n\n\n———. 1968b. “Nonmetric Discriminant Analysis.” Research\nNote 06-68. Department of Data Theory, University of Leiden.\n\n\n———. 1968c. “Nonmetric Multidimensional Scaling.” Research\nNote 010-68. Department of Data Theory FSW/RUL. https://jansweb.netlify.app/publication/deleeuw-r-68-9/deleeuw-r-68-g.pdf.\n\n\n———. 1969. “Some Contributions to the\nAnalysis of Categorical Data.” Research Note 004-69.\nLeiden, The Netherlands: Department of Data Theory FSW/RUL.\n\n\n———. 1970. “The Euclidean Distance Model.”\nResearch Note 002-70. Department of Data Theory FSW/RUL.\n\n\n———. 1973a. “Canonical Analysis of Categorical Data.” PhD\nthesis, University of Leiden, The Netherlands. https://jansweb.netlify.app/publication/deleeuw-b-73/deleeuw-b-73.pdf.\n\n\n———. 1973b. “Smoothness Properties of\nNonmetric Loss Functions.” Technical Memorandum. Murray\nHill, N.J.: Bell Telephone Laboratories.\n\n\n———. 1974. “Approximation of a Real Symmetric Matrix by a Positive\nSemidefinite Matrix of Rank r.” Technical Memorandum\nTM-74-1229-10. Murray Hill, N.J.: Bell Telephone Laboratories.\n\n\n———. 1975a. “A Normalized Cone Regression\nApproach to Alternating Least Squares Algorithms.”\nDepartment of Data Theory FSW/RUL.\n\n\n———. 1975b. “An Alternating Least Squares\nApproach to Squared Distance Scaling.” Department of Data\nTheory FSW/RUL.\n\n\n———. 1977a. “Applications of Convex Analysis to Multidimensional\nScaling.” In Recent Developments in Statistics, edited\nby J. R. Barra, F. Brodeau, G. Romier, and B. Van Cutsem, 133–45.\nAmsterdam, The Netherlands: North Holland Publishing Company.\n\n\n———. 1977b. “Correctness of Kruskal’s Algorithms for Monotone\nRegression with Ties.” Psychometrika 42: 141–44.\n\n\n———. 1984a. Canonical Analysis of Categorical Data. Leiden, The\nNetherlands: DSWO Press. https://jansweb.netlify.app/publication/deleeuw-b-84/deleeuw-b-84.pdf.\n\n\n———. 1984b. “Differentiability of Kruskal’s\nStress at a Local Minimum.” Psychometrika 49:\n111–13.\n\n\n———. 1984c. “Fixed-Rank Approximation with Singular Weight\nMatrices.” Computational Statistics Quarterly 1: 3–12.\n\n\n———. 1988. “Convergence of the Majorization Method for\nMultidimensional Scaling.” Journal of Classification 5:\n163–80.\n\n\n———. 1993. “Fitting Distances by Least Squares.” Preprint\nSeries 130. Los Angeles, CA: UCLA Department of Statistics. https://jansweb.netlify.app/publication/deleeuw-r-93-c/deleeuw-r-93-c.pdf.\n\n\n———. 1994. “Block Relaxation Algorithms in\nStatistics.” In Information Systems and Data\nAnalysis, edited by H. H. Bock, W. Lenski, and M. M. Richter,\n308–24. Berlin: Springer Verlag. https://jansweb.netlify.app/publication/deleeuw-c-94-c/deleeuw-c-94-c.pdf.\n\n\n———. 2005a. “Fitting Ellipsoids by Least\nSquares.” UCLA Department of Statistics. 2005. https://jansweb.netlify.app/publication/deleeuw-u-05-j/deleeuw-u-05-j.pdf.\n\n\n———. 2005b. “Unidimensional Scaling.” In\nThe Encyclopedia of Statistics in Behavioral Science, edited by\nB. S. Everitt and D. Howell, 4:2095–97. New York, N.Y.: Wiley.\n\n\n———. 2006. “Accelerated Least Squares Multidimensional\nScaling.” Preprint Series 493. Los Angeles, CA: UCLA\nDepartment of Statistics. https://jansweb.netlify.app/publication/deleeuw-r-06-b/deleeuw-r-06-b.pdf.\n\n\n———. 2007a. “A Horseshoe for Multidimensional\nScaling.” Preprint Series 530. Los Angeles, CA: UCLA\nDepartment of Statistics.\n\n\n———. 2007b. “Derivatives of Generalized Eigen Systems with\nApplications.” Preprint Series 528. Los Angeles, CA: UCLA\nDepartment of Statistics.\n\n\n———. 2007c. “Quadratic Surface Embedding.”\nUCLA Department of Statistics. 2007. https://jansweb.netlify.app/publication/deleeuw-u-07-h/deleeuw-u-07-h.pdf.\n\n\n———. 2008a. “Accelerating Majorization Algorithms.”\nPreprint Series 543. Los Angeles, CA: UCLA Department of Statistics.\n\n\n———. 2008b. “Derivatives of Fixed-Rank\nApproximations.” Preprint Series 547. Los Angeles, CA:\nUCLA Department of Statistics.\n\n\n———. 2008c. “Polynomial Extrapolation to Accelerate Fixed Point\nAlgorithms.” Preprint Series 542. Los Angeles, CA: UCLA\nDepartment of Statistics.\n\n\n———. 2012. “On Inverse Multidimensional\nScaling.” UCLA Department of Statistics. https://jansweb.netlify.app/publication/deleeuw-u-12-b/deleeuw-u-12-b.pdf.\n\n\n———. 2014a. “Bounding, and Sometimes Finding,\nthe Global Minimum in Multidimensional Scaling.” UCLA\nDepartment of Statistics. https://jansweb.netlify.app/publication/deleeuw-u-14-b/deleeuw-u-14-b.pdf.\n\n\n———. 2014b. “Minimizing rStress Using Nested\nMajorization.” UCLA Department of Statistics. https://jansweb.netlify.app/publication/deleeuw-u-14-c/deleeuw-u-14-c.pdf.\n\n\n———. 2015. “Regression with Linear Inequality Restrictions on\nPredicted Values.”\n\n\n———. 2016a. “Derivatives of Low Rank PSD\nApproximation.” 2016.\n\n\n———. 2016b. “Gower Rank.” 2016. https://jansweb.netlify.app/publication/deleeuw-e-16-k/deleeuw-e-16-k.pdf.\n\n\n———. 2016c. “Pictures of Stress.” 2016.\n\n\n———. 2017a. “Computing and Fitting Monotone\nSplines.” 2017. https://jansweb.netlify.app/publication/deleeuw-e-17-i/deleeuw-e-17-i.pdf.\n\n\n———. 2017b. “Multidimensional Scaling with\nDistance Bounds.” 2017.\n\n\n———. 2017c. “Multidimensional Scaling with\nLower Bounds.” 2017.\n\n\n———. 2017d. “Multidimensional Scaling with\nUpper Bounds.” 2017.\n\n\n———. 2017e. “Shepard Non-metric\nMultidimensional Scaling.” 2017. https://jansweb.netlify.app/publication/deleeuw-e-17-e/deleeuw-e-17-e.pdf.\n\n\n———. 2017f. “Tweaking the SMACOF\nEngine.” 2017.\n\n\n———. 2018a. “Differentiability of Stress at\nLocal Minima.” 2018.\n\n\n———. 2018b. “MM Algorithms for Smoothed\nAbsolute Values.” 2018.\n\n\n———. 2019. “Normalized Cone Regression.” 2019. https://jansweb.netlify.app/publication/deleeuw-e-19-d/deleeuw-e-19-d.pdf.\n\n\nDe Leeuw, J., and B. Bettonvil. 1986. “An Upper Bound for\nSSTRESS.” Psychometrika 51: 149–53.\n\n\nDe Leeuw, J., P. Groenen, and P. Mair. 2016a. “Full-Dimensional\nScaling.” 2016. https://jansweb.netlify.app/publication/deleeuw-groenen-mair-e-16-e/deleeuw-groenen-mair-e-16-e.pdf.\n\n\n———. 2016b. “Minimizing qStress for Small q.” 2016. https://jansweb.netlify.app/publication/deleeuw-groenen-mair-e-16-h/deleeuw-groenen-mair-e-16-h.pdf.\n\n\n———. 2016c. “Minimizing rStress Using\nMajorization.” 2016. https://jansweb.netlify.app/publication/deleeuw-groenen-mair-e-16-a/deleeuw-groenen-mair-e-16-a.pdf.\n\n\n———. 2016d. “More on Inverse Multidimensional Scaling.”\n2016. https://jansweb.netlify.app/publication/deleeuw-groenen-mair-e-16-f/deleeuw-groenen-mair-e-16-f.pdf.\n\n\n———. 2016e. “Second Derivatives of rStress,\nwith Applications.” 2016. https://jansweb.netlify.app/publication/deleeuw-groenen-mair-e-16-c/deleeuw-groenen-mair-e-16-c.pdf.\n\n\n———. 2016f. “Singularities and Zero Distances in Multidimensional\nScaling.” 2016.\n\n\nDe Leeuw, J., P. Groenen, and R. Pietersz. 2016. “An Alternating\nLeast Squares Approach to Squared Distance Scaling.” 2016. https://jansweb.netlify.app/publication/deleeuw-groenen-pietersz-e-16-m/deleeuw-groenen-pietersz-e-16-m.pdf.\n\n\nDe Leeuw, J., and W. J. Heiser. 1977. “Convergence of Correction\nMatrix Algorithms for Multidimensional Scaling.” In Geometric\nRepresentations of Relational Data, edited by J. C. Lingoes,\n735–53. Ann Arbor, Michigan: Mathesis Press.\n\n\n———. 1980. “Multidimensional Scaling with Restrictions on the\nConfiguration.” In Multivariate Analysis, Volume\nV, edited by P. R. Krishnaiah, 501–22. Amsterdam, The\nNetherlands: North Holland Publishing Company.\n\n\n———. 1982. “Theory of Multidimensional Scaling.” In\nHandbook of Statistics, Volume II, edited by P. R.\nKrishnaiah and L. Kanal. Amsterdam, The Netherlands: North Holland\nPublishing Company.\n\n\nDe Leeuw, J., K. Hornik, and P. Mair. 2009. “Isotone Optimization in R: Pool-Adjacent-Violators\nAlgorithm (PAVA) and Active Set Methods.” Journal of\nStatistical Software 32 (5): 1–24.\n\n\nDe Leeuw, J., and P. Mair. 2009. “Multidimensional Scaling Using Majorization: SMACOF in\nR.” Journal of Statistical Software 31 (3): 1–30.\nhttps://www.jstatsoft.org/article/view/v031i03.\n\n\nDe Leeuw, J., and J. J. Meulman. 1986. “Principal Component\nAnalysis and Restricted Multidimensional Scaling.” In\nClassification as a Tool of Research, edited by W. Gaul and M.\nSchader, 83–96. Amsterdam, London, New York, Tokyo: North-Holland.\n\n\nDe Leeuw, J., and K. Sorenson. 2012. “Derivatives of the Procrustus Transformation with\nApplications.”\n\n\nDe Leeuw, J., and I. Stoop. 1984. “Upper Bounds for Kruskal’s\nStress.” Psychometrika 49: 391–402.\n\n\nDefays, D. 1978. “A Short Note on a Method of\nSeriation.” British Journal of Mathematical and\nStatistical Psychology 31: 49–53.\n\n\nDelfour, M. C. 2012. Introduction to Optimization and\nSemidifferential Calculus. SIAM.\n\n\nDempster, A. P., N. M. Laird, and D. B. Rubin. 1977. “Maximum Likelihood for Incomplete Data via the EM\nAlgorithm.” Journal of the Royal Statistical\nSociety B39: 1–38.\n\n\nDemyanov, V. F., and V. N. Malozemov. 1990. Introduction to\nMinimax. Dover.\n\n\nDinkelbach, W. 1967. “On Nonlinear Fractional\nProgramming.” Management Science 13: 492–98.\n\n\nDu, S., C. Jin, J. D. Lee, M. I. Jordan, B. Póczos, and A. Singh. 2017.\n“Gradient Descent Can Take Exponential Time\nto Escape Saddle Points.” In NIPS’17: Proceedings of\nthe 31st International Conference on Neural Information Processing\nSystems, 1067—1077.\n\n\nDür, M., R. Horst, and M. Locatelli. 1998. “Necessary and Sufficient Global Optimality Conditions for\nConvex Maximization Revisited.” Journal of\nMathematical Analysis and Applications 217: 637–49.\n\n\nEckart, C., and G. Young. 1936. “The\nApproximation of One Matrix by Another of Lower Rank.”\nPsychometrika 1 (3): 211–18.\n\n\nEkman, G. 1954. “Dimensions of Color\nVision.” Journal of Psychology 38: 467–74.\n\n\nGaffney, P. W. 1976. “The Calculation of\nIndefinite Integrals of B-splines.” Journal of the\nInstitute of Mathematics and Its Applications 17: 37–41.\n\n\nGifi, A. 1990. Nonlinear Multivariate Analysis. New York, N.Y.:\nWiley.\n\n\nGilbert, P., and R. Varadhan. 2019. numDeriv:\nAccurate Numerical Derivatives. https://CRAN.R-project.org/package=numDeriv.\n\n\nGohberg, I., P. Lancaster, and L. Rodman. 2009. Matrix\nPolynomials. Classic in Applied Mathematics. SIAM.\n\n\nGold, E. Mark. 1973. “Metric Unfolding: Data\nRequirement for Unique Solution & Clarification of Schönemann’s Algorithm.”\nPsychometrika 38 (4): 555–69.\n\n\nGoldman, A. J. 1956. “Resolution and\nSeparation Theorems for Polyhedral Convex Sets.” In\nLinear Inequalities and Related Systems, edited by Kuhn H. W.\nand A. W. Tucker, 41–51. Annals of Mathematics Studies 38. Princeton\nUniversity Press.\n\n\nGower, J. C. 1966. “Some Distance Properties\nof Latent Root and Vector Methods Used in Multivariate\nAnalysis.” Biometrika 53: 325–38.\n\n\nGower, J. C., and G. B. Dijksterhuis. 2004. Procrustus\nProblems. Oxford University Press.\n\n\nGreenacre, M. J., and M. W. Browne. 1986. “An\nEfficient Alternating Least Squares Algorithm to Perform\nMultidimensional Unfolding.” Psychometrika 51\n(2): 241–50.\n\n\nGroenen, P. J. F., and J. De Leeuw. 2010. “Power-Stress for Multidimensional Scaling.”\nhttps://jansweb.netlify.app/publication/groenen-deleeuw-u-10/groenen-deleeuw-u-10.pdf.\n\n\nGroenen, P. J. F., P. Giaquinto, and H. A. L Kiers. 2003. “Weighted Majorization Algorithms for Weighted Least\nSquares Decomposition Models.” Econometric Institute\nReport EI 2003-09. Econometric Institute, Erasmus University Rotterdam.\nhttps://repub.eur.nl/pub/1700.\n\n\nGroenen, P. J. F., W. J. Heiser, and J. J. Meulman. 1998. “City-Block Scaling: Smoothing Strategies for Avoiding\nLocal Minima.” In Classification, Data Analysis, and\nData Highways, edited by I. Balderjahn, R. Mathar, and M. Schader.\nSpringer.\n\n\n———. 1999. “Global Optimization in\nLeast-Squares Multidimensional Scaling by Distance\nSmoothing.” Journal of Classification 16: 225–54.\n\n\nGroenen, P. J. F., R. Mathar, and W. J. Heiser. 1995. “The Majorization Approach to Multidimensional Scaling for\nMinkowski Distances.” Journal of Classification\n12: 3–19.\n\n\nGroenen, P. J. F., and M. Van de Velden. 2016. “Multidimensional Scaling by Majorization: A\nReview.” Journal of Statistical Software 73 (8):\n1–26. https://www.jstatsoft.org/index.php/jss/article/view/v073i08.\n\n\nGuilford, J. P. 1954. Psychometric Methods. McGraw-Hill.\n\n\nGuttman, L. 1941. “The Quantification of a\nClass of Attributes: A Theory and Method of Scale\nConstruction.” In The Prediction of Personal\nAdjustment, edited by P. Horst, 321–48. New York: Social Science\nResearch Council.\n\n\n———. 1967. “The Development of Nonmetric\nSpace Analysis: A Letter to Professor John Ross.”\nMultivariate Behavioral Research 2 (1): 71–82.\n\n\n———. 1968. “A General Nonmetric Technique for\nFitting the Smallest Coordinate Space for a Configuration of\nPoints.” Psychometrika 33: 469–506.\n\n\nHarman, R., and V. Lacko. 2010. “On\nDecompositional aAgorithms for Uniform Sampling from n-Spheres and\nn-Balls.” Journal of Multivariate Analysis 101:\n2297–2304.\n\n\nHarshman, R. A. 1970. “Foundations of the\nPARAFAC Procedure.” Working Papers in Phonetics 16. UCLA.\n\n\nHeiser, W. J. 1988. “Multidimensional Scaling\nwith Least Absolute Residuals.” In Classification and\nRelated Methods of Data Analysis, edited by H. H. Bock, 455–62.\nNorth-Holland Publishing Co.\n\n\n———. 1991. “A Generalized Majorization Method\nfor Least Squares Multidimensional Scaling of Pseudodistances that May\nBe Negative.” Psychometrika 56 (1): 7–27.\n\n\n———. 1995. “Convergent Computing by Iterative\nMajorization: Theory and Applications in Multidimensional Data\nAnalysis.” In Recent Advantages in Descriptive\nMultivariate Analysis, edited by W. J. Krzanowski, 157–89. Oxford:\nClarendon Press.\n\n\nHeiser, W. J., and J. De Leeuw. 1977. “How to Use\nSMACOF-I.” Department of Data Theory FSW/RUL.\n\n\n———. 1979. “Metric Multidimensional Unfolding.”\nMethoden En Data Nieuwsbrief SWS/VVS 4: 26–50.\n\n\nHiriart-Urruty, J.-B. 1988. “Generalized Differentiability /\nDuality and Optimization for Problems Dealing with Differences of Convex\nFunctions.” In Convexity and Duality in Optimization,\nedited by Ponstein. J., 37–70. Lecture Notes in Economics and\nMathematical Systems 256. Springer.\n\n\n“Inverse Multidimensional Scaling.” 2007.\nJournal of Classification 14: 3–21.\n\n\nKeller, J. B. 1962. “Factorization of\nMatrices by Least Squares.” Biometrika 49:\n239–42.\n\n\nKiers, H. A. L. 1997. “Weighted Least Squares Fitting Using\nIterative Ordinary Least Squares Algorithms.”\nPsychometrika 62: 251–66.\n\n\nKruskal, J. B. 1964a. “Multidimensional\nScaling by Optimizing Goodness of Fit to a Nonmetric\nHypothesis.” Psychometrika 29: 1–27.\n\n\n———. 1964b. “Nonmetric Multidimensional\nScaling: a Numerical Method.” Psychometrika 29:\n115–29.\n\n\n———. 1971. “Monotone Regression: Continuity\nand Differentiability Properties.” Psychometrika\n36 (1): 57–62.\n\n\nKruskal, J. B., and J. D. Carroll. 1969. “Geometrical Models and Badness of Fit\nFunctions.” In Multivariate Analysis, Volume II,\nedited by P. R. Krishnaiah, 639–71. North Holland Publishing Company.\n\n\nLange, K. 2016. MM Optimization Algorithms. SIAM.\n\n\nLawlor, G. R. 2020. “l’Hôpital’s Rule for Multivariable\nFunctions.” American Mathematical Monthly 127\n(8): 717–25.\n\n\nLee, J. D., M. Simchowitz, M. I. Jordan, and B. Recht. 2016.\n“Gradient Descent Converges to\nMinimizers.”\n\n\nLevelt, W. J. M., J. P. Van De Geer, and R. Plomp. 1966. “Triadic Comparions of Musical Intervals.”\nBritish Journal of Mathematical and Statistical Psychology 19:\n163–79.\n\n\nLingoes, J. C. 1968a. “The Multivariate Analysis Of\nQualitative Data.” Multivariate Behavioral\nResearch 3 (1): 61–94.\n\n\n———. 1968b. “The Rationale of the\nGuttman-Lingoes Nonmetric Series: A Letter to Doctor Philip\nRunkel.” Multivariate Behavioral Research 3 (4):\n495–507.\n\n\n———. 1971. “Some Boundary Conditions for a\nMonotone Analysis of Symmetric Matrices.”\nPsychometrika 36 (2): 195–203.\n\n\nLingoes, J. C., and E. E. Roskam. 1973. “A\nMathematical and Empirical Analysis of Two Multidimensional Scaling\nAlgorithms.” Psychometrika 38: Monograph\nSupplement.\n\n\nLyusternik, L., and L. Schnirelmann. 1934. Méthodes\nTopologiues Dans Les Problèmes Variationelle. Hermann.\n\n\nMaehara, H. 1986. “Metric Transforms of\nFinite Spaces and Connected Graphs.” Discrete\nMathematics 61: 235–46.\n\n\nMair, P., P. J. F. Groenen, and J. De Leeuw. 2022. “More on Multidimensional Scaling in R: smacof Version\n2.” Journal of Statistical Software 102 (10):\n1–47. https://www.jstatsoft.org/article/view/v102i10.\n\n\nMessick, S. J., and R. P. Abelson. 1956. “The\nAdditive Constant Problem in Multidimensional Scaling.”\nPsychometrika 21 (1–17).\n\n\nMeulman, J. J. 1986. “A Distance Approach to\nNonlinear Multivariate Analysis.” PhD thesis, Leiden\nUniversity.\n\n\n———. 1992. “The Integration of\nMultidimensional Scaling and Multivariate Analysis with Optimal\nTransformations.” Psychometrika 57 (4): 539–65.\n\n\nMeulman, J. J., and W. J. Heiser. 2012. IBM SPSS Categories 21.\nIBM Corporation.\n\n\nOrtega, J. M., and W. C. Rheinboldt. 1970. Iterative Solution of Nonlinear Equations in Several\nVariables. New York, N.Y.: Academic Press.\n\n\nPalubeckis, G. 2013. “An Improved Exact\nAlgorithm for Least-Squares Unidimensional Scaling.”\nJournal of Applied Mathematics, 1–15.\n\n\nPapazoglou, S., and K. Mylonas. 2017. “An\nExamination of Alternative Multidimensional Scaling\nTechniques.” Educational and Psychological\nMeasurement 77 (3): 429–48.\n\n\nPliner, V. 1984. “A Class of Metric Scaling\nModels.” Automation and Remote Control 45:\n789–94.\n\n\n———. 1986. “The Problem of Multidimensional\nMetric Scaling.” Automation and Remote Control\n47: 560–67.\n\n\n———. 1996. “Metric Unidimensional Scaling and\nGlobal Optimization.” Journal of Classification\n13: 3–18.\n\n\nRamirez, C., R. Sanchez, V. Kreinovich, and M. Argaez. 2014.\n“$\\sqrt{x^2+\\mu}$ is the Most Computationally\nEfficient Smooth Approximation to x.” Journal of\nUncertain Systems 8: 205–10.\n\n\nRamsay, J. O. 1975. “Solving Implicit Equations in Psychometric\nData Analysis.” Psychometrika 40: 337–60.\n\n\n———. 1977. “Maximum Likelihood Estimation in\nMultidimensional Scaling.” Psychometrika 42:\n241–66.\n\n\n———. 1988. “Monotone Regression Splines in\nAction.” Statistical Science 3: 425–61.\n\n\nRobert, F. 1967. “Calcul du Rapport Maximal\nde Deux Normes sur ℝn.”\nRevue Francaise d’Automatique, d’Informatique Et De Recherche\nOperationelle 1: 97–118.\n\n\nRockafellar, R. T. 1970. Convex Analysis. Princeton University\nPress.\n\n\nRoskam, E. E. 1968. “Metric Analysis of\nOrdinal Data in Psychology.” PhD thesis, University of\nLeiden.\n\n\nRoss, J., and N. Cliff. 1964. “A\nGeneralization of the Interpoint Distance Model.”\nPsychometrika 29 (167-176).\n\n\nRothkopf, E. Z. 1957. “A Measure of Stimulus\nSimilarity and Errors in some Paired-associate Learning.”\nJournal of Experimental Psychology 53: 94–101.\n\n\nSchoenberg, I. J. 1935. “Remarks to Maurice\nFrechet’s article: Sur la Definition Axiomatique d’une Classe d’Espaces\nVectoriels Distancies Applicables Vectoriellement sur l’Espace de\nHllbert.” Annals of Mathematics 36: 724–32.\n\n\n———. 1937. “On Certain Metric Spaces Arising\nFrom Euclidean Spaces by a Change of Metric and Their Imbedding in\nHilbert Space.” Annals of Mathematics 38 (4):\n787–93.\n\n\nSchönemann, P. H. 1970. “On Metric Multidimensional\nUnfolding.” Psychometrika 35 (3): 349–66.\n\n\nSchumaker, L. 2007. Spline Functions: Basic\nTheory. Third Edition. Cambridge University Press.\n\n\nShepard, R. N. 1962a. “The Analysis of\nProximities: Multidimensional Scaling with an Unknown Distance Function.\nI.” Psychometrika 27: 125–40.\n\n\n———. 1962b. “The Analysis of Proximities:\nMultidimensional Scaling with an Unknown Distance Function.\nII.” Psychometrika 27: 219–46.\n\n\nShepard, R. N., and J. D. Carroll. 1966. “Parametric\nRepresentation of Nonlinear Data Structures.” In\nMulktivariate Analysis, edited by P. R. Krishnaiah, 561–92.\nAcademic Press.\n\n\nSidi, A. 2017. Vector Extrapolation Methods with Applicatons.\nSIAM.\n\n\nSpang, H. A. 1962. “A Review of Minimization\nTechniques for Nonlinear Functions.” SIAM Review\n4 (4): 343–65.\n\n\nSpence, I. 1983. “Monte Carlo Studies.” Applied\nPsychological Measurement 7 (4): 405–25.\n\n\nStephenson, W. 1953. The Study of Behavior:\nQ-technique and its Methodology. University of Chicago\nPress.\n\n\nTakane, Y., F. W. Young, and J. De Leeuw. 1977. “Nonmetric\nIndividual Differences in Multidimensional Scaling: An Alternating Least\nSquares Method with Optimal Scaling Features.”\nPsychometrika 42: 7–67.\n\n\nTaussky, O. 1949. “A Recurring Theorem on\nDeterminants.” American Mathematical Monthly 56:\n672–76.\n\n\nTisseur, F., and K. Meerbergen. 2001. “The Quadratic\nEigenvalue Problem.” SIAM Review 43\n(2): 235–86.\n\n\nTorgerson, W. S. 1958. Theory and Methods of\nScaling. New York: Wiley.\n\n\nTorgerson, W. S. 1952. “Multidimensional\nScaling: I. Theory and Method.” Psychometrika 17\n(4): 401–19.\n\n\n———. 1965. “Multidimensional Scaling of\nSimilarity.” Psychometrika 30 (4): 379–93.\n\n\nTrosset, M. W., and R. Mathar. 1997. “On the\nExistence on Nonglobal Minimizers of the STRESS Criterion for Metric\nMultidimensional Scaling.” In Proceedings of the\nStatistical Computing Section, 158–62. Alexandria, VA: American\nStatistical Association.\n\n\nVan de Geer, J. P. 1967. Inleiding in de\nMultivariate Analyse. Van Loghum Slaterus.\n\n\n———. 1971. Introduction to Multivariate\nAnalysis for the Social Sciences. San Francisco, CA:\nFreeman.\n\n\nVan de Geer, J. P., W. J. M. Levelt, and R. Plomp. 1962. “The Connotation of Musical Consonance.”\nActa Psychologica 20: 308–19.\n\n\nWang, Y., C. L. Lawson, and R. J. Hanson. 2015. lsei: Solving Least Squares Problems under\nEquality/Inequality Constraints.\n\n\nYoung, F. W. 1981. “Quantitative Analysis of\nQualitative Data.” Psychometrika 46: 357–88.\n\n\nYoung, F. W., Y. Takane, and R. Lewyckyj. 1978a. “ALSCAL: A Nonmetric Multidimensional Scaling Program with\nSeveral Individual-Differences Options.” Behavior\nResearch Methods & Instrumentation 10 (3): 451–53.\n\n\n———. 1978b. “Three Notes on\nALSCAL.” Psychometrika 43 (3): 433–35.\n\n\nYoung, G., and A. S. Householder. 1938. “Discussion of a Set of Points in Terms of Their Mutual\nDistances.” Psychometrika 3 (19-22).\n\n\nZilinskas, A., and A. Poslipskyte. 2003. “On\nMultimodality of the SSTRESS Criterion for Metric Multidimensional\nScaling.” Informatica 14 (1): 121–30.",
    "crumbs": [
      "References"
    ]
  }
]