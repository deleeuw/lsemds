<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Minimization of Basic Stress – Least Squares Euclidean Multidimensional Scaling</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./acceleration.html" rel="next">
<link href="./classical.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
</head><body class="nav-sidebar floating">.greybox {
  padding: 1em;
  background: white;
  color: black;
  border: 2px solid orange;
  border-radius: 10px;
}
.center {
  text-align: center;
}

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>





<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./minimization.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Minimization of Basic Stress</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Least Squares Euclidean Multidimensional Scaling</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Note</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notation and Reserved Symbols</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./properties.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Properties of Stress</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./spaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Stress Spaces</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Multidimensional Scaling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./minimization.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Minimization of Basic Stress</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Acceleration of Convergence</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nonmetric.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Nonmetric MDS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./interval.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Interval MDS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./polynomial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Polynomial MDS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ordinal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Ordinal MDS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./splinical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Splinical MDS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unidimensional.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Unidimensional Scaling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./full.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Full-dimensional Scaling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unfolding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Unfolding</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./constrained.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Constrained Multidimensional Scaling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./individual.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Individual Differences</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymmetry.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Asymmetry in MDS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nominal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Nominal MDS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nonordinal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Nonmonotonic MDS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nonpairs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Compound Objects</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sstress.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">sstress</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rstress.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">fstress and rstress</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./altls.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Alternative Least Squares Loss</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./inverse.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Inverse Multidimensional Scaling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Stability of MDS Solutions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./global.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">In Search of Global Minima</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Software</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./backmatter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">(APPENDIX) Appendices</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#gradient-methods" id="toc-gradient-methods" class="nav-link active" data-scroll-target="#gradient-methods"><span class="header-section-number">5.1</span> Gradient Methods</a>
  <ul class="collapse">
  <li><a href="#step-size" id="toc-step-size" class="nav-link" data-scroll-target="#step-size"><span class="header-section-number">5.1.1</span> Step Size</a></li>
  </ul></li>
  <li><a href="#initial-configurations" id="toc-initial-configurations" class="nav-link" data-scroll-target="#initial-configurations"><span class="header-section-number">5.2</span> Initial Configurations</a></li>
  <li><a href="#apmajmin" id="toc-apmajmin" class="nav-link" data-scroll-target="#apmajmin"><span class="header-section-number">5.3</span> On MM Algorithms</a></li>
  <li><a href="#smacof-algorithm" id="toc-smacof-algorithm" class="nav-link" data-scroll-target="#smacof-algorithm"><span class="header-section-number">5.4</span> Smacof Algorithm</a>
  <ul class="collapse">
  <li><a href="#propguttman" id="toc-propguttman" class="nav-link" data-scroll-target="#propguttman"><span class="header-section-number">5.4.1</span> Guttman Transform</a></li>
  <li><a href="#derivative" id="toc-derivative" class="nav-link" data-scroll-target="#derivative"><span class="header-section-number">5.4.2</span> Derivative</a></li>
  <li><a href="#global-convergence" id="toc-global-convergence" class="nav-link" data-scroll-target="#global-convergence"><span class="header-section-number">5.4.3</span> Global Convergence</a></li>
  <li><a href="#mincomprot" id="toc-mincomprot" class="nav-link" data-scroll-target="#mincomprot"><span class="header-section-number">5.4.4</span> Component Rotated Smacof</a></li>
  <li><a href="#minlocconv" id="toc-minlocconv" class="nav-link" data-scroll-target="#minlocconv"><span class="header-section-number">5.4.5</span> Local Convergence</a></li>
  <li><a href="#datasym" id="toc-datasym" class="nav-link" data-scroll-target="#datasym"><span class="header-section-number">5.4.6</span> Data Asymmetry</a></li>
  <li><a href="#minrepl" id="toc-minrepl" class="nav-link" data-scroll-target="#minrepl"><span class="header-section-number">5.4.7</span> Replications</a></li>
  <li><a href="#negative-dissimilarities" id="toc-negative-dissimilarities" class="nav-link" data-scroll-target="#negative-dissimilarities"><span class="header-section-number">5.4.8</span> Negative Dissimilarities</a></li>
  <li><a href="#normalization" id="toc-normalization" class="nav-link" data-scroll-target="#normalization"><span class="header-section-number">5.4.9</span> Normalization</a></li>
  <li><a href="#minunweight" id="toc-minunweight" class="nav-link" data-scroll-target="#minunweight"><span class="header-section-number">5.4.10</span> Unweighting</a></li>
  </ul></li>
  <li><a href="#propenvelopes" id="toc-propenvelopes" class="nav-link" data-scroll-target="#propenvelopes"><span class="header-section-number">5.5</span> Stress Envelopes</a>
  <ul class="collapse">
  <li><a href="#propcsmaj" id="toc-propcsmaj" class="nav-link" data-scroll-target="#propcsmaj"><span class="header-section-number">5.5.1</span> CS Majorization</a></li>
  <li><a href="#propamgmmin" id="toc-propamgmmin" class="nav-link" data-scroll-target="#propamgmmin"><span class="header-section-number">5.5.2</span> AM/GM Minorization</a></li>
  <li><a href="#dualities" id="toc-dualities" class="nav-link" data-scroll-target="#dualities"><span class="header-section-number">5.5.3</span> Dualities</a></li>
  </ul></li>
  <li><a href="#smacofcoef" id="toc-smacofcoef" class="nav-link" data-scroll-target="#smacofcoef"><span class="header-section-number">5.6</span> Smacof in Coefficient Space</a></li>
  <li><a href="#smacofnewton" id="toc-smacofnewton" class="nav-link" data-scroll-target="#smacofnewton"><span class="header-section-number">5.7</span> Newton in MDS</a>
  <ul class="collapse">
  <li><a href="#attraction" id="toc-attraction" class="nav-link" data-scroll-target="#attraction"><span class="header-section-number">5.7.1</span> Regions of Attraction</a></li>
  </ul></li>
  <li><a href="#propdistsmo" id="toc-propdistsmo" class="nav-link" data-scroll-target="#propdistsmo"><span class="header-section-number">5.8</span> Distance Smoothing</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="minstr" class="quarto-section-identifier"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Minimization of Basic Stress</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="gradient-methods" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="gradient-methods"><span class="header-section-number">5.1</span> Gradient Methods</h2>
<p>The initial algorithms for nonmetric MDS <span class="citation" data-cites="kruskal_64b">Kruskal (<a href="references.html#ref-kruskal_64b" role="doc-biblioref">1964</a>)</span> and <span class="citation" data-cites="guttman_68">Guttman (<a href="references.html#ref-guttman_68" role="doc-biblioref">1968</a>)</span> were gradient methods. Thus the gradient, or vector of partial derivatives, was computed in each iteration step, and a step was taken in the direction of the negative gradient (which is also known as the direction of steepest descent).</p>
<p>Informally, if <span class="math inline">\(f\)</span> is differentiable at <span class="math inline">\(x\)</span> then <span class="math inline">\(f(x+h)\approx f(x)+h'\mathcal{D}f(x)\)</span> and the direction <span class="math inline">\(h\)</span> that minimizes the diferential (the second term) is <span class="math inline">\(-\mathcal{D}f(x)/\|\mathcal{D}f(x)\|\)</span>, the normalized negative gradient. Although psychometricians had been in the business of minimizing least squares loss functions in the linear and bilinear case, this result for general nonlinear functions was new to them. And I, and probably many others, hungrily devoured the main optimization reference in <span class="citation" data-cites="kruskal_64b">Kruskal (<a href="references.html#ref-kruskal_64b" role="doc-biblioref">1964</a>)</span>, which was the excellent early review by <span class="citation" data-cites="spang_62">Spang (<a href="references.html#ref-spang_62" role="doc-biblioref">1962</a>)</span>.</p>
<p>Kruskal’s paper also presents an elaborate step-size procedure, to determine how far we go in the negative gradient direction. In the long and convoluted paper <span class="citation" data-cites="guttman_68">Guttman (<a href="references.html#ref-guttman_68" role="doc-biblioref">1968</a>)</span> there is an important contribution to gradient methods in basic MDS. Let’s ignore the complications arising from zero distances, which is after all what both Kruskal and Guttman do as well, and assume all distances at configuration <span class="math inline">\(X\)</span> are positive. Then stress is differentiable at <span class="math inline">\(X\)</span>, with gradient</p>
<p><span class="math display">\[
\nabla\sigma(X)=
-\sum_{i=1}^n\sum_{j=1}^nw_{ij}(\delta_{ij}-d_{ij}(X))\frac{1}{d_{ij}(X)}
(e_i-e_j)(x_i-x_j)'
\]</span></p>
<p>Geometrical interpretation - Gleason, Borg-Groenen p 20</p>
<p>Guttman C-matrix</p>
<p>Ramsay C-matrix</p>
<section id="step-size" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="step-size"><span class="header-section-number">5.1.1</span> Step Size</h3>
<section id="kruskal-step-size" class="level4" data-number="5.1.1.1">
<h4 data-number="5.1.1.1" class="anchored" data-anchor-id="kruskal-step-size"><span class="header-section-number">5.1.1.1</span> Kruskal Step Size</h4>
<p>elaborate</p>
</section>
<section id="guttman-step-size" class="level4" data-number="5.1.1.2">
<h4 data-number="5.1.1.2" class="anchored" data-anchor-id="guttman-step-size"><span class="header-section-number">5.1.1.2</span> Guttman Step Size</h4>
<p>constant</p>
</section>
<section id="cauchy-step-size" class="level4" data-number="5.1.1.3">
<h4 data-number="5.1.1.3" class="anchored" data-anchor-id="cauchy-step-size"><span class="header-section-number">5.1.1.3</span> Cauchy Step Size</h4>
<p>In the classical version of the steepest descent method we choose the step-size <span class="math inline">\(\alpha\)</span> by minimizing <span class="math inline">\(h(\alpha)=f(x+\alpha y)\)</span> over <span class="math inline">\(\alpha\)</span></p>
<p><span class="math display">\[
d_+h(\alpha;\beta)=\lim_{\epsilon\downarrow 0}\frac{f(x+(\alpha+\epsilon\beta)y)-f(x+\alpha y)}{\epsilon}=\beta\ d_+f(x+\alpha y;y)
\]</span> or local minimum closest to zero</p>
<p>Newton version</p>
<p><span class="math display">\[
d_+^2h(\alpha;\beta,\gamma)=\beta\gamma d_+^2f(x+\alpha y;y,y)
\]</span></p>
</section>
<section id="majorization-step-size" class="level4" data-number="5.1.1.4">
<h4 data-number="5.1.1.4" class="anchored" data-anchor-id="majorization-step-size"><span class="header-section-number">5.1.1.4</span> Majorization Step Size</h4>
<p>Lagrange form of the remainder</p>
<p><span class="math inline">\(e(\epsilon)=\eta^2(X+\epsilon Y)\)</span> <span class="math inline">\(r(\epsilon)=\rho(X+\epsilon Y)\)</span> <span class="math inline">\(s(\epsilon)=\sigma(X+\epsilon Y)=1-r(\epsilon)+\frac12e(\epsilon)\)</span></p>
<p><span class="math display">\[
r(\epsilon)\geq r(0)+r'(0)\epsilon
\]</span> <span class="math display">\[
e(\epsilon)=e(0)+e'(0)\epsilon+\frac12 e''(0)\epsilon^2
\]</span> <span class="math display">\[
s(\epsilon)\leq s(0)+s'(0)\epsilon+\frac12 e''(0)\epsilon^2
\]</span> <span class="math display">\[
\hat\epsilon=\frac{s'(0)}{e''(0)}
\]</span> underestimates newton step</p>
<p><span class="math display">\[
r(\epsilon)\geq r(0)+\epsilon\ r'(0)+\frac12\epsilon^2\min_{0\leq \xi\leq\epsilon}r''(\xi)
\]</span> <span class="math display">\[
\hat\epsilon=\frac{s'(0)}{e''(0)-\min_{0\leq \xi\leq\epsilon}r''(\xi)}
\]</span></p>
<p><span class="math display">\[
r''(\xi)=\mathcal{D}^2\sigma(X+\xi Y;Y,Y)
\]</span></p>
</section>
</section>
</section>
<section id="initial-configurations" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="initial-configurations"><span class="header-section-number">5.2</span> Initial Configurations</h2>
<p>Random</p>
<p>L</p>
<p>Torgerson</p>
<p>Guttman</p>
</section>
<section id="apmajmin" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="apmajmin"><span class="header-section-number">5.3</span> On MM Algorithms</h2>
<p>The term <em>majorization</em> is used in mathematics in many different ways. In this book we use it as a general technique for the construction of <em>stable</em> iterative minimization algorithms. An iterative minimization algorithm is stable if it decreases the objective function in each iteration.</p>
<p>Ortega, Rheinboldt Weber Bohning, Lindsay Vosz, Eckart</p>
<p>Special cases of majorization had been around earlier, most notably the smacof algorithm for MDS and the EM algorithm for maximum likelihood with missing data, but in full generality majorization was first discussed in <span class="citation" data-cites="deleeuw_C_94c">De Leeuw (<a href="references.html#ref-deleeuw_C_94c" role="doc-biblioref">1994</a>)</span> and <span class="citation" data-cites="heiser_95">Heiser (<a href="references.html#ref-heiser_95" role="doc-biblioref">1995</a>)</span>.</p>
<p>Majorization can be used to construct minimization methods, while minorization can construct maximization methods. This cleverly suggests to use the acronym MM algorithms for this class of techniques. An excellent comprehensive account of MM algorithms is <span class="citation" data-cites="lange_16">Lange (<a href="references.html#ref-lange_16" role="doc-biblioref">2016</a>)</span>. Another such account is slowly growing in one of the companion volumes in this series of personal research histories (<span class="citation" data-cites="deleeuw_B_21b">(<a href="references.html#ref-deleeuw_B_21b" role="doc-biblioref"><strong>deleeuw_B_21b?</strong></a>)</span>).</p>
<p>Here we just give a quick introduction to majorization. Suppose <span class="math inline">\(f\)</span> is a real-valued function on <span class="math inline">\(X\subseteq\mathbb{R}^n\)</span>. Then a real-valued function <span class="math inline">\(g\)</span> on <span class="math inline">\(X\otimes X\)</span> is said to majorize <span class="math inline">\(f\)</span> on <span class="math inline">\(X\)</span> if</p>
<p><span class="math display">\[\begin{equation}
g(x,y)\geq f(x)\qquad\forall (x,y)\in X\otimes X,
(\#eq:majorineq)
\end{equation}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{equation}
g(y,y)=f(y)\qquad\forall y\in X.
(\#eq:majoreq)
\end{equation}\]</span></p>
<p>Thus for each <span class="math inline">\(y\in X\)</span> the function <span class="math inline">\(g(\bullet,y)\)</span> lies above <span class="math inline">\(f\)</span>, and it touches <span class="math inline">\(f\)</span> from above at <span class="math inline">\(x=y\)</span>. Majorization is <em>strict</em> if <span class="math inline">\(g(x,y)=f(x)\)</span> if and only if <span class="math inline">\(x=y\)</span>, i.e.&nbsp;if <span class="math inline">\(y\)</span> is the only point in <span class="math inline">\(X\)</span> where <span class="math inline">\(g(\bullet,y)\)</span> and <span class="math inline">\(f\)</span> touch.</p>
<p>A <em>majorization algorithm</em> to minimize <span class="math inline">\(f\)</span> on <span class="math inline">\(X\)</span> starts with an initial estimate, and then updates the estimate in iteration <span class="math inline">\(k\)</span> by</p>
<p><span class="math display">\[\begin{equation}
x^{(k+1)}\in\mathop{\text{argmin}}_{x\in X}g(x,x^{(k)}),
(\#eq:majoralg)
\end{equation}\]</span></p>
<p>with the understanding that the algorithms stops when <span class="math inline">\(x^{(k)}\in\mathop{\text{argmin}}_{x\in X}g(x,x^{(k)})\)</span>.</p>
<p>If we do not stop we have an infinite sequence satisfying the <em>sandwich inequality</em></p>
<p><span class="math display">\[\begin{equation}
f(x^{(k+1)})\leq g(x^{(k+1)},x^{(k)})\leq g(x^{(k)},x^{(k)})=f(x^{(k)}).
(\#eq:sandwich)
\end{equation}\]</span></p>
<p>The first inequality in this chain comes from @ref(eq:majorineq). It is strict when majorization is strict. The second inequality comes from @ref(eq:majoralg), and it is strict if <span class="math inline">\(g(\bullet,y)\)</span> has a unique minimum on <span class="math inline">\(X\)</span> for each <span class="math inline">\(y\)</span>.</p>
<p>Necessary conditions through MM</p>
</section>
<section id="smacof-algorithm" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="smacof-algorithm"><span class="header-section-number">5.4</span> Smacof Algorithm</h2>
<section id="propguttman" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="propguttman"><span class="header-section-number">5.4.1</span> Guttman Transform</h3>
<p>The Guttman Transform is named to honor the contribution of Louis Guttman to non-metric MDS (mainly, but by no means exclusively, in <span class="citation" data-cites="guttman_68">Guttman (<a href="references.html#ref-guttman_68" role="doc-biblioref">1968</a>)</span>). Guttman introduced the transform in a slightly different way, and partly for different reasons. In chapter @ref(minstr) we shall see that the Guttman Transform plays a major role in defining and understanding the <span class="math inline">\(\textrm{smacof}\)</span> algorithm.</p>
<p>The <em>Guttman Transform</em> of a configuration <span class="math inline">\(X\)</span> is defined as <span class="math display">\[\begin{equation}
\Gamma(X):=V^+B(X)X,
(\#eq:guttrans)
\end{equation}\]</span> which is simply equal to <span class="math inline">\(\Gamma(X)=n^{-1}B(X)X\)</span> if all weights are equal. For some purposes it is useful to think of <span class="math inline">\(V^+B(X)X\)</span> as a function of the weights, the dissimilarities, and the configuration (see, for exam0ple, chapter @ref(chinverse)), but we reserve the name Guttman transform for a map from <span class="math inline">\(\mathbb{R}^{n\times p}\)</span> into <span class="math inline">\(\mathbb{R}^{n\times p}\)</span> .</p>
<p>What we have called <span class="math inline">\(B(X)\)</span> is what Guttman calls the <em>correction matrix</em> or <em>C-matrix</em> (see <span class="citation" data-cites="deleeuw_heiser_C_77">De Leeuw and Heiser (<a href="references.html#ref-deleeuw_heiser_C_77" role="doc-biblioref">1977</a>)</span> for a comparison).</p>
<p>Completing the square in equation @ref(eq:propmatexp) gives <span class="math display">\[\begin{equation}
\sigma(X)=1+\eta^2(X-\Gamma(X))-\eta^2(\Gamma(X)),
(\#eq:gutsquare)
\end{equation}\]</span> which shows that <span class="math display">\[\begin{equation}
1-\eta^2(\Gamma(X))\leq\sigma(X)\leq 1+\eta^2(X-\Gamma(X)).
(\#eq:propsbounds)
\end{equation}\]</span></p>
<p>Note that it follows from @ref(eq:propsbounds) that <span class="math inline">\(\sigma(X)\geq 1-\eta^2(\Gamma(X))\)</span>, with equality if and only if <span class="math inline">\(X=\Gamma(X)\)</span>.</p>
<div class="theorem">
<p>The Guttman transform is</p>
<ul>
<li><p><em>self-scaling</em> (a.k.a. homogeneous of degree zero) because <span class="math inline">\(\Gamma(\alpha X)=\Gamma(X)\)</span> for all <span class="math inline">\(-\infty&lt;\alpha&lt;+\infty\)</span>. With our definition @ref(eq:bdef) of <span class="math inline">\(B(X)\)</span> we also have <span class="math inline">\(\Gamma(0)=0\)</span>.</p></li>
<li><p><em>self-centering</em>, because <span class="math inline">\(\Gamma(X+e\mu')=\Gamma(X)\)</span> for all <span class="math inline">\(\mu\in\mathbb{R}^p\)</span>.</p></li>
<li><p>Bounded</p></li>
<li><p>Lipschitz</p></li>
</ul>
</div>
<p>::: { ,proof} We already know, from the CS inequality, that <span class="math display">\[\begin{equation}
\rho(X)\leq\eta(X).
(\#eq:csagain)
\end{equation}\]</span></p>
<p>With the Guttman transform in hand we can apply CS once more, and find</p>
<p><span class="math display">\[\begin{equation}
\rho(X)=\text{tr}\ X'B(X)X=\text{tr}\ X'V\Gamma(X)\leq\eta(X)\eta(\Gamma(X))
(\#eq:csagainagain)
\end{equation}\]</span></p>
<p>Note that the Guttman transform is bounded. In fact, using the Euclidean norm throughout, <span class="math display">\[
\Gamma(X)\leq\|V^+\|\|B(X)X\|
\]</span> Now <span class="math display">\[
B(X)X=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\delta_{ij}\frac{x_i-x_j}{d_{ij}(X)}(e_i-e_j),
\]</span> and thus</p>
<p><span class="math display">\[
\|B(X)X\|\leq\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\delta_{ij}\left\|\frac{x_i-x_j}{d_{ij}(X)}\right\|\|e_i-e_j\|=\sqrt{2}\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\delta_{ij},
\]</span> and <span class="math display">\[
\|\Gamma(X)\|\leq\sqrt{2}\|V^+\|\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\delta_{ij}.
\]</span> In fact <span class="math display">\[
B(X)X-B(Y)Y=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\delta_{ij}\left\{\frac{x_i-x_j}{d_{ij}(X)}-\frac{y_i-y_j}{d_{ij}(Y)}\right\}(e_i-e_j),
\]</span> and thus <span class="math display">\[
\|\Gamma(X)-\Gamma(Y)\|\leq 2\|V^+\|
\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\delta_{ij},
\]</span> and thus the Guttman transform is Lipschitz. ::: ;’’’’ytyh ### Subdifferentials</p>
</section>
<section id="derivative" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2" class="anchored" data-anchor-id="derivative"><span class="header-section-number">5.4.2</span> Derivative</h3>
<p>The basic smacof algorithm, which is the main building block for most of the MDS techniques discussed in this book, updates the configuration <span class="math inline">\(X^{(k)}\)</span> in iteration <span class="math inline">\(k\)</span> by</p>
<p><span class="math display">\[\begin{equation}
X^{(k+1)}=\Gamma(X^{(k)})=V^+B(X^{(k)})X^{(k)}.
(\#eq:smacofupdate)
\end{equation}\]</span></p>
<p>so that first <span class="math inline">\(X^{(1)}=\Gamma(X^{(0)})\)</span>, then <span class="math inline">\(X^{(2)}=\Gamma(X^{(1)})=\Gamma(\Gamma(X^{(0)}))\)</span>, and generally <span class="math inline">\(X^{(k)}=\Gamma^k(X^{0}),\)</span> where <span class="math inline">\(\Gamma^k\)</span> is the k-times composition (or iteration) of <span class="math inline">\(\Gamma.\)</span></p>
<p>We shall show in this chapter that as <span class="math inline">\(k\rightarrow\infty\)</span> both<br>
<span class="math inline">\(\sigma(X^{(k+1)})-\sigma(X^{(k)})\rightarrow 0\)</span>, and <span class="math inline">\(\eta^2(X^{(k)}-X^{(k+1)})=(X^{(k+1)}-X^{(k)})'V(X^{(k+1)}-X^{(k)})\rightarrow 0\)</span>. The iterations stop either if <span class="math inline">\(\sigma(X^{(k)})-\sigma(X^{(k+1)})&lt;\epsilon\)</span> or if <span class="math inline">\(\eta^2(X^{(k)}-X^{(k+1)})&lt;\epsilon\)</span>, where the <span class="math inline">\(\epsilon\)</span> are small cut-off numbers chosen by the user, or if we reach a user-defined maximum number of iterations, and we give up on convergence. The user also chooses if the stop criterion is based on function value changes or configuration changes.</p>
<p>Some quick remarks on implementation. We only have to compute <span class="math inline">\(V^+\)</span> once, but premultiplying by a full symmetric matrix in each iteration does add quite a few multiplications to the algorithm. If all <span class="math inline">\(w_{ij}\)</span> are one, then <span class="math inline">\(V^+=\frac{1}{n}J\)</span> and thus <span class="math inline">\(\Gamma(X^{(k)})=\frac{1}{n}B(X^{(k)})X^{(k)}\)</span>. In fact we do not even have to carry out this division by <span class="math inline">\(n\)</span>, because the basic algorithm is <em>self scaling</em>. which means in this context that <span class="math inline">\(\Gamma(\alpha X)=\Gamma(X)\)</span> for all <span class="math inline">\(\alpha\geq 0\)</span>.</p>
</section>
<section id="global-convergence" class="level3" data-number="5.4.3">
<h3 data-number="5.4.3" class="anchored" data-anchor-id="global-convergence"><span class="header-section-number">5.4.3</span> Global Convergence</h3>
<p>The iterations in @ref(eq:smacofupdate) start at some <span class="math inline">\(X^{(0)}\)</span> and then generate five sequences of non-negative numbers. Define <span class="math inline">\(\lambda(X):=\rho(X)/\eta(X)\)</span> and <span class="math inline">\(\Gamma(X):=\eta^2(X-\Gamma(X))\)</span>. The five sequences we will look at are</p>
<p><span class="math display">\[\begin{align}
\begin{split}
\sigma_k&amp;:=\sigma(X^{(k)}),\\
\rho_k&amp;:=\rho(X^{(k)}),\\
\eta_k&amp;:=\eta(X^{(k)}),\\
\lambda_k&amp;:=\lambda(X^{(k)}),\\
\Gamma_k&amp;:=\Gamma(X^{(k)}),
\end{split}
(\#eq:smacofseq)
\end{align}\]</span></p>
<p>Depend on <span class="math inline">\(X^{(0)}\)</span></p>
<p>Zangwill</p>
<p>Argyros</p>
<section id="from-the-cs-inequality" class="level4" data-number="5.4.3.1">
<h4 data-number="5.4.3.1" class="anchored" data-anchor-id="from-the-cs-inequality"><span class="header-section-number">5.4.3.1</span> From the CS Inequality</h4>
<div id="smacoffunc" class="theorem">
<p>&nbsp;</p>
<ol type="1">
<li><p><span class="math inline">\(\sigma_k\)</span> is a decreasing sequence, bounded below by 0.</p></li>
<li><p><span class="math inline">\(\rho_k\)</span>, <span class="math inline">\(\eta_k\)</span>, amd <span class="math inline">\(\lambda_k\)</span> are increasing sequences, bounded above by 1.</p></li>
<li><p><span class="math inline">\(\Gamma_k\)</span> is a null-sequence.</p></li>
</ol>
</div>
<p>To prove convergence of these sequences we slightly modify and extend the proofs in <span class="citation" data-cites="deleeuw_C_77">De Leeuw (<a href="references.html#ref-deleeuw_C_77" role="doc-biblioref">1977</a>)</span> and <span class="citation" data-cites="deleeuw_heiser_C_77">De Leeuw and Heiser (<a href="references.html#ref-deleeuw_heiser_C_77" role="doc-biblioref">1977</a>)</span> (while I say to myself: that’s 44 years ago).</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span></p>
<ol type="1">
<li><p>For each <span class="math inline">\(X\in\mathbb{R}^{n\times p}\)</span> we have <span class="math inline">\(\rho(X)\leq\eta(X)\)</span> and thus <span class="math inline">\(\lambda(X)\leq 1\)</span>.</p></li>
<li><p>For each <span class="math inline">\(X,Y\in\mathbb{R}^{n\times p}\)</span> we have <span class="math inline">\(\rho(X)\geq\text{tr}\ X'B(Y)Y\)</span> and thus <span class="math inline">\(\rho(X)\geq\text{tr}\ X'V\Gamma(Y)\)</span>. Taking <span class="math inline">\(X=\Gamma(Y)\)</span> we see that <span class="math inline">\(\rho(X)\geq\eta^2(\Gamma(Y))\)</span>. Now <span class="math inline">\(\sigma(\Gamma(Y))=1-2\rho(\Gamma(Y))+\eta^2(\Gamma(Y))\leq 1-\eta^2(\Gamma(Y))\)</span> and thus for all <span class="math inline">\(X\)</span> <span class="math inline">\(\eta^2(\Gamma(X)) \leq 1\)</span>.</p></li>
<li><p>For each <span class="math inline">\(X\in\mathbb{R}^{n\times p}\)</span> we have <span class="math inline">\(\rho(X)=\text{tr}\ X'B(X)X\)</span> and thus <span class="math inline">\(\rho(X)\leq\eta(X)\eta(\Gamma(X))\)</span> and thus <span class="math inline">\(\lambda(X)\leq\eta(\Gamma(X))\)</span></p></li>
</ol>
<p><span class="math display">\[
\rho(X^{(k)})=\text{tr}\ \{X^{(k)}\}'VX^{(k+1)}\leq\eta(X^{(k)})\eta(X^{(k+1)}),
\]</span></p>
<p><span class="math display">\[
\rho(X^{(k+1)})\geq\text{tr}\ \{X^{(k+1)}\}'B(X^{(k)})X^{(k)}=\eta^{2}(X^{(k+1)}),
\]</span></p>
<p><span class="math display">\[
\eta(X^{(k)})\leq\lambda(X^{(k)})\leq\eta(X^{(k+1)})
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\rho(X^{(k)})\leq\frac{\eta(X^{(k)}}{X^{(k+1)}}\rho(X^{(k+1)})\leq\rho(X^{(k+1)})
\]</span></p>
</div>
</section>
<section id="from-majorization" class="level4" data-number="5.4.3.2">
<h4 data-number="5.4.3.2" class="anchored" data-anchor-id="from-majorization"><span class="header-section-number">5.4.3.2</span> From Majorization</h4>
<p>Smacof is based on the majorization, valid for all configurations <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>,</p>
<p><span class="math display">\[\begin{equation}
\sigma(X)\leq 1+\eta^2(X-\Gamma(Y))-\eta^2(\Gamma(Y)),
(\#eq:upbmajineq)
\end{equation}\]</span></p>
<p>with equality if and only if <span class="math inline">\(X\propto Y\)</span>. If <span class="math inline">\(Y=\alpha X\)</span> for some <span class="math inline">\(\alpha\)</span> then <span class="math display">\[\begin{equation}
\sigma(X)=1+\eta^2(X-\Gamma(Y))-\eta^2(\Gamma(Y)),
(\#eq:upbmajeq)
\end{equation}\]</span> and specifically we have @ref(eq:upbmajeq) if <span class="math inline">\(Y=X\)</span>.</p>
<p>Now suppose we have an <span class="math inline">\(Y\)</span> with <span class="math inline">\(Y\not=\Gamma(Y)\)</span>. If <span class="math inline">\(\eta^2(X-\Gamma(Y))\leq\eta^2(Y-\Gamma(Y))\)</span> then</p>
<p><span class="math display">\[\begin{align}
\begin{split}
\sigma(X)%\leq 1+\eta^2(X-\Gamma(Y))-\eta^2(\Gamma(Y))\leq\\
&amp;\leq 1+\eta^2(Y-\Gamma(Y))-\eta^2(\Gamma(Y))=\sigma(Y)
\end{split}
(\#eq:upbmajimp)
\end{align}\]</span></p>
<p>The obvious choice for <span class="math inline">\(X\)</span> is <span class="math inline">\(X=\Gamma(Y)\)</span>, which makes <span class="math inline">\(\eta^2(X-\Gamma(Y))=0\)</span>, and thus</p>
<p><span class="math display">\[\begin{equation}
\sigma(X)\leq 1-\eta^2(\Gamma(Y))&lt;
1+\eta^2(Y-\Gamma(Y))-\eta^2(\Gamma(Y))=\sigma(Y)
(\#eq:upbmajmin)
\end{equation}\]</span></p>
</section>
<section id="from-ratio-of-norms" class="level4" data-number="5.4.3.3">
<h4 data-number="5.4.3.3" class="anchored" data-anchor-id="from-ratio-of-norms"><span class="header-section-number">5.4.3.3</span> From Ratio of Norms</h4>
<p><span class="citation" data-cites="deleeuw_C_77">De Leeuw (<a href="references.html#ref-deleeuw_C_77" role="doc-biblioref">1977</a>)</span></p>
<p>DC Algorithm</p>
<p>Robert</p>
</section>
</section>
<section id="mincomprot" class="level3" data-number="5.4.4">
<h3 data-number="5.4.4" class="anchored" data-anchor-id="mincomprot"><span class="header-section-number">5.4.4</span> Component Rotated Smacof</h3>
<p>Consider the modified smacof iterations <span class="math inline">\(\tilde X^{(k+1)}=X^{(k+1)}L^{(k+1)}\)</span>, where <span class="math inline">\(L^{(k+1)}\)</span> are the normalized eigenvectors of <span class="math inline">\(\{X^{(k+1)}\}^TVX^{(k+1)}\)</span>. Then</p>
<p><span class="math display">\[
\sigma(\tilde X^{(k)})=\sigma(X^{(k)})
\]</span> Thus the modified update produces the same sequence of stress values as the basic update. Also <span class="math display">\[
\Gamma(\tilde X^{(k)})=\Gamma(X^{(k)})L^{(k)}
\]</span> Thus <span class="math inline">\(\tilde X^{(k)}\)</span> and <span class="math inline">\(X^{(k)}\)</span> differ by a rotation for each <span class="math inline">\(k\)</span>. It follows that we can actually compute <span class="math inline">\(\tilde X^{(k)}\)</span> from the basic sequence <span class="math inline">\(X^{(k)}\)</span> by rotating the <span class="math inline">\(X^{(k)}\)</span> to principal components. Specifically if <span class="math inline">\(X_\infty\)</span> is a subsequential limit of <span class="math inline">\(X^{(k)}\)</span> then the corresponding limit of <span class="math inline">\(\tilde X^{(k)}\)</span> is <span class="math inline">\(X_\infty\)</span> rotated to principal components. Modifying the intermediate updates is just a waste of time, we can simply rotate the final smacof solution. And we should, as we explain in the next section, @ref(minlocconv).</p>
</section>
<section id="minlocconv" class="level3" data-number="5.4.5">
<h3 data-number="5.4.5" class="anchored" data-anchor-id="minlocconv"><span class="header-section-number">5.4.5</span> Local Convergence</h3>
<p><span class="math display">\[
\mathcal{D}\Gamma(X)(Y)=V^+\left\{B(X)Y-\mathop{\sum\sum}_{1\leq i&lt;j\leq n} w_{ij}\frac{\delta_{ij}}{d_{ij}(X)}\left(\frac{\text{tr}\ Y'A_{ij}X}{d_{ij}^2(X)}\right)A_{ij}\right\}
\]</span></p>
<p>For any X: one zero eigenvalue <span class="math inline">\(\mathcal{D}\Gamma(X)(X)=0\)</span></p>
<p>If on <span class="math inline">\(\mathbb{R}^{n\times p}\)</span> then an additional <span class="math inline">\(p\)</span> zero eigenvalues <span class="math inline">\(\mathcal{D}\Gamma(X)(e\mu^T)=0\)</span></p>
<p>For <span class="math inline">\(X=\Gamma(X)\)</span> and <span class="math inline">\(M\)</span> anti-symmetric: <span class="math inline">\(\frac12 p(p-1)E\)</span> unit eigenvalues <span class="math inline">\(\mathcal{D}\Gamma(X)(XM)=\Gamma(X)M=XM\)</span></p>
<section id="cross-product-iterations" class="level4" data-number="5.4.5.1">
<h4 data-number="5.4.5.1" class="anchored" data-anchor-id="cross-product-iterations"><span class="header-section-number">5.4.5.1</span> Cross Product Iterations</h4>
<p>Map of C into C. No rotational indeterminacy. Same stress sequence.</p>
<p><span class="math display">\[
C^{(k+1)}=V^+B(C^{(k)})C^{(k)}B(C^{(k)})V^+
\]</span> Map of <span class="math inline">\(D\)</span> into <span class="math inline">\(D\)</span>. Guttman transform as function of distances. Not very nice.</p>
<p><span class="math display">\[
D^{(k+1)}:=D(X^{(k+1)})=D(\Gamma(X^{(k)}))
\]</span></p>
</section>
<section id="rotation-to-pc" class="level4" data-number="5.4.5.2">
<h4 data-number="5.4.5.2" class="anchored" data-anchor-id="rotation-to-pc"><span class="header-section-number">5.4.5.2</span> Rotation to PC</h4>
<p>We suppose the configfuration <span class="math inline">\(X\)</span> is <span class="math inline">\(n\times p\)</span>, with rank <span class="math inline">\(p\)</span>. If the singular value decomposition is <span class="math inline">\(X=K\Lambda L'\)</span> then the rotation to principle components is <span class="math inline">\(\Gamma(X):=K\Lambda=XL\)</span>. Thus <span class="math inline">\(\mathcal{D}\Gamma(X)(Y)=YL+X\mathcal{D}L(X)(Y)\)</span>. So we need to compute <span class="math inline">\(\mathcal{D}L(X)\)</span>, with <span class="math inline">\(L\)</span> the right singular vectors of <span class="math inline">\(X\)</span>, i.e.&nbsp;the eigenvectors of <span class="math inline">\(X^TX\)</span>. We use the methods and results from <span class="citation" data-cites="deleeuw_R_07c">De Leeuw (<a href="references.html#ref-deleeuw_R_07c" role="doc-biblioref">2007</a>)</span>, which were applied to similar problems in <span class="citation" data-cites="deleeuw_R_08b">De Leeuw (<a href="references.html#ref-deleeuw_R_08b" role="doc-biblioref">2008</a>)</span>, <span class="citation" data-cites="deleeuw_sorenson_U_12b">De Leeuw and Sorenson (<a href="references.html#ref-deleeuw_sorenson_U_12b" role="doc-biblioref">2012</a>)</span>, and <span class="citation" data-cites="deleeuw_E_16p">De Leeuw (<a href="references.html#ref-deleeuw_E_16p" role="doc-biblioref">2016</a>)</span>.</p>
<div id="minrotpc" class="theorem">
<p>If the <span class="math inline">\(n\times p\)</span> matrix has rank <span class="math inline">\(p\)</span>, singular value decomposition <span class="math inline">\(X=K\Lambda L^T\)</span>, with all singular values different, then <span class="math inline">\(\Gamma(X+\Delta)=\Gamma(X)+\Delta L+XLM+o(\|\Delta\|)\)</span>, where <span class="math inline">\(M\)</span> is antisymmetric with off-diagonal elements</p>
<p><span class="math display">\[\begin{equation}
m_{ij}=\frac{\lambda_is_{ij}+\lambda_js_{ji}}{\lambda_i^2-\lambda_j^2}.
(\#eq:minsvdmsolve)
\end{equation}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof involves computing the derivatives of the singular value decomposition of <span class="math inline">\(X\)</span>, which is defined by the equations</p>
<p><span class="math display">\[\begin{align}
XL&amp;=K\Lambda,(\#eq:minsvd1)\\
X^TK&amp;=L\Lambda,(\#eq:minsvd2)\\
K^TK&amp;=I,(\#eq:minsvd3)\\
L^TL&amp;=LL^T=I.(\#eq:minsvd4)
\end{align}\]</span></p>
<p>We now perturb <span class="math inline">\(X\)</span> to <span class="math inline">\(X+\Delta\)</span>, which perturbs <span class="math inline">\(L\)</span> to <span class="math inline">\(L+L_\Delta+o(\|\Delta\|)\)</span>, <span class="math inline">\(K\)</span> to <span class="math inline">\(K+K_\Delta+o(\|\Delta\|)\)</span>, and <span class="math inline">\(\Lambda\)</span> to <span class="math inline">\(\Lambda+\Lambda_\Delta+o(\|\Delta\|)\)</span>. Substutute this into the four SVD equations for <span class="math inline">\(X+\Delta\)</span> and keep the linear terms.</p>
<p><span class="math display">\[\begin{align}
XL_\Delta+\Delta L&amp;=K_\Delta\Lambda+K\Lambda_\Delta,(\#eq:minsvdperb1)\\
X^TK_\Delta+\Delta^TK&amp;=L_\Delta\Lambda+L\Lambda_\Delta,(\#eq:minsvdperb2)\\
L_\Delta^TL+L^TL_\Delta&amp;=0,(\#eq:minsvdperb3)\\
K_\Delta^TK+K^TK_\Delta&amp;=0.(\#eq:minsvdperb4)
\end{align}\]</span></p>
<p>Define <span class="math inline">\(M:=L^TL_\Delta\)</span> and <span class="math inline">\(N:=K^TK_\Delta\)</span>. Then equations @ref(eq:minsvdperb3) and @ref(eq:minsvdperb4) say that <span class="math inline">\(M\)</span> and <span class="math inline">\(N\)</span> are antisymmetric. Also define <span class="math inline">\(S:=K^T\Delta L\)</span>. Premultiplying equation @ref(eq:minsvdperb1) by <span class="math inline">\(K^T\)</span> and @ref(eq:minsvdperb2) by <span class="math inline">\(L^T\)</span> gives</p>
<p><span class="math display">\[\begin{align}
\Lambda M+S&amp;=N\Lambda+\Lambda_\Delta,(\#eq:minsvdred1)\\
\Lambda N+S^T&amp;=M\Lambda+\Lambda_\Delta.(\#eq:minsvdred2)
\end{align}\]</span></p>
<p>Either of these two equations gives, using the antisymmetry, and thus hollowness, of <span class="math inline">\(M\)</span> and <span class="math inline">\(N\)</span>, that <span class="math inline">\(\Lambda_\Delta=\text{diag}(S)\)</span>. Define the hollow matrix <span class="math inline">\(U:=S-\text{diag}(S)\)</span>. Then</p>
<p><span class="math display">\[\begin{align}
\Lambda M-N\Lambda&amp;=U,(\#eq:minsvdu1)\\
\Lambda N-M\Lambda&amp;=U^T.(\#eq:minsvdu2)
\end{align}\]</span></p>
<p>Premultiply @ref(eq:minsvdu1) and postmultiply @ref(eq:minsvdu2) by <span class="math inline">\(\Lambda\)</span>.</p>
<p><span class="math display">\[\begin{align}
\Lambda^2 M-\Lambda N\Lambda&amp;=\Lambda U,(\#eq:minsvdu3)\\
\Lambda N\Lambda-M\Lambda^2&amp;=U^T\Lambda.(\#eq:minsvdu4)
\end{align}\]</span></p>
<p>If we add these two equations we can solve for the off-diagonal elements of <span class="math inline">\(M\)</span> and find the expression in the theorem. Since <span class="math inline">\(L_\Delta=LM\)</span> this completes the proof.</p>
</div>
</section>
</section>
<section id="datasym" class="level3" data-number="5.4.6">
<h3 data-number="5.4.6" class="anchored" data-anchor-id="datasym"><span class="header-section-number">5.4.6</span> Data Asymmetry</h3>
<p>The non-basic situation in which there are asymmetric weights and/or asymmetric dissimilarities in basic MDS is analyzed in <span class="citation" data-cites="deleeuw_C_77">De Leeuw (<a href="references.html#ref-deleeuw_C_77" role="doc-biblioref">1977</a>)</span>, although it is just standard linear least squares projection theory. We give a slightly different partitionng ofd the sum of squares here. Note that it is not even necessary that the weights and dissimilarities are hollow and/or non-negative.</p>
<p>We decompose the weights and dissimilarities additively into a symmetric and anti-symmetric part. Thus <span class="math inline">\(w_{ij}=w_{ij}^S+w_{ij}^A\)</span> and <span class="math inline">\(\delta_{ij}=\delta_{ij}^S+\delta_{ij}^A\)</span>. Now in general if <span class="math inline">\(A\)</span> is anti-symmetric and <span class="math inline">\(B\)</span> is symmetric, then <span class="math inline">\(\text{tr}\ AB=0\)</span>. Also their Hadamard (element-wise) product <span class="math inline">\(A*B\)</span> is anti-symmetric, and the Hadamard product of two anti-symmetric matrices is symmetric. Using these rules gives after some calculation <span class="math display">\[\begin{align}
\begin{split}
\sigma(X)&amp;=\frac14\sum_{i=1}^n\sum_{j=1}^nw_{ij}(\delta_{ij}-d_{ij}(X))^2=\\&amp;=\frac14\sum_{i=1}^n w_{ii}^{\ }\delta_{ii}^2+
\frac12\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}^S\left\{\underline{\delta}_{ij}-d_{ij}(X)\right\}^2+\frac12\mathop{\sum\sum}_{1\leq i&lt;j\leq n}\frac{w_{ij}w_{ji}}{w_{ij}^S}\{\delta_{ij}^A\}^2,
\end{split}(\#eq:partiunsym)
\end{align}\]</span> where <span class="math display">\[\begin{equation}
\underline{\delta}_{ij}:=\delta_{ij}^S+\frac{w_{ij}^A\delta_{ij}^A}{w_{ij}^S}.
(\#eq:defofunddelta)
\end{equation}\]</span> Thus minimizing stress in the case of asymmetric weights and dissimilarities, which even can be non-hollow and non-positive, reduces to a symmmetric basic MDS problem for adjusted dissimilarities defined by equation @ref(eq:defofunddelta). If the original weights and dissimilarities are non-negative, then so are the weights <span class="math inline">\(w_{ij}^S\)</span> and the dissimilarities <span class="math inline">\(\underline{\delta}_{ij}\)</span>.</p>
</section>
<section id="minrepl" class="level3" data-number="5.4.7">
<h3 data-number="5.4.7" class="anchored" data-anchor-id="minrepl"><span class="header-section-number">5.4.7</span> Replications</h3>
<p>If there are replications in basic MDS we can use a simple partitioning of stress to reduce the problem to standard form. We start with <span class="math display">\[\begin{equation}
\sigma(X)=\frac12\sum_{k=1}^m\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ijk}(\delta_{ijk}-d_{ij}(X))^2.
\#(eq:inddiffstress)
\end{equation}\]</span> Let <span class="math display">\[\begin{align}
w_{ij\bullet}&amp;=\sum_{k=1}^m w_{ijk}(\#eq:wbul1)\\
\delta_{ij\bullet}&amp;=\frac{\sum_{k=1}^m w_{ijk}\delta_{ijk}}{w_{ij\bullet}}(\#eq:wbul2).
\end{align}\]</span> Then <span class="math display">\[\begin{equation}
\sigma(X)=\frac12\mathop{\sum\sum}_{1\leq i&lt;j\leq n} w_{ij\bullet}(\delta_{ij\bullet}-d_{ij}(X))^2+\frac12\sum_{k=1}^m\mathop{\sum\sum}_{1\leq i&lt;j\leq n} w_{ijk}(\delta_{ijk}-\delta_{ij\bullet})^2,
(\#eq:partinddiff)
\end{equation}\]</span> and it suffices to minimize the first term, which is a standard basic MDS problem.</p>
<p>In the nonmetric case, in which in principle each of the <span class="math inline">\(\Delta_k\)</span> can be transformed, we must alternate minimization of #ref(eq:inddiffstress) over the <span class="math inline">\(\Delta_k\)</span> and minimization of @ref(eq:partinddiff) over <span class="math inline">\(X\)</span>. In the case in which <span class="math inline">\(X_k\)</span> is different pover replications we use the methods of chapter @ref(chindif).</p>
</section>
<section id="negative-dissimilarities" class="level3" data-number="5.4.8">
<h3 data-number="5.4.8" class="anchored" data-anchor-id="negative-dissimilarities"><span class="header-section-number">5.4.8</span> Negative Dissimilarities</h3>
<p><span class="math display">\[\begin{equation}
\sigma(X)=1-\sum_{k\in\mathcal{K}_{1+}} w_k\delta_kd_k(X)
+\sum_{k\in\mathcal{K}_{1-}} w_k|\delta_k|d_k(X)+\frac12\sum_{k\in\mathcal{K}} w_kd_k^2(X)).
(\#eq:disneg)
\end{equation}\]</span></p>
<p>Split rho</p>
<p><span class="citation" data-cites="heiser_91">Heiser (<a href="references.html#ref-heiser_91" role="doc-biblioref">1991</a>)</span></p>
</section>
<section id="normalization" class="level3" data-number="5.4.9">
<h3 data-number="5.4.9" class="anchored" data-anchor-id="normalization"><span class="header-section-number">5.4.9</span> Normalization</h3>
<p>In actual computer output using the scaling in formula @ref(eq:scaldiss1) and @ref(eq:scaldiss1) has some disadvantages. There are, say, <span class="math inline">\(M\)</span> non-zero weights. The summation in #ref(eq:stressall) is really over <span class="math inline">\(M\)</span> terms only. If <span class="math inline">\(n\)</span> is at all large the scaled dissimilarities, and consequently the distances and the configuration, will become very small. Thus, in actual computation, or at least in the computer output, we scale our dissimilarities as <span class="math inline">\(\frac12\mathop{\sum\sum}_{1\leq j&lt;i\leq n} w_{ij}^{\ }\delta_{ij}^2=M\)</span>. So, we scale our dissimilarities to one in formulas and to <span class="math inline">\(M\)</span> in computations. Thus the computed stress will b</p>
<p>In fact, we do not even use it in our computer programs, except at the very last moment when we return the final stress after the algorithm has completed.</p>
</section>
<section id="minunweight" class="level3" data-number="5.4.10">
<h3 data-number="5.4.10" class="anchored" data-anchor-id="minunweight"><span class="header-section-number">5.4.10</span> Unweighting</h3>
<p>Consider the general problem of minimizing a least squares loss function, defined as <span class="math inline">\(f(x):=(x-y)'W(x-y)\)</span> over <span class="math inline">\(x\)</span> in some set <span class="math inline">\(X\)</span>, where <span class="math inline">\(W\)</span> is a symmetric weight matrix. Sometimes <span class="math inline">\(W\)</span> complicates the problem, maybe because it is too big, too full, too singular, or even indefinite. We will use iterative majorization to give <span class="math inline">\(W\)</span> a more subordinate role. See also <span class="citation" data-cites="kiers_97">Kiers (<a href="references.html#ref-kiers_97" role="doc-biblioref">1997</a>)</span> and <span class="citation" data-cites="groenen_giaquinto_kiers_03">Groenen, Giaquinto, and Kiers (<a href="references.html#ref-groenen_giaquinto_kiers_03" role="doc-biblioref">2003</a>)</span>.</p>
<p>Suppose <span class="math inline">\(z\)</span> is another element of <span class="math inline">\(X\)</span>. Think of it as the current best approximation to <span class="math inline">\(y\)</span> that we have, which we want to improve. Then</p>
<p><span class="math display">\[\begin{align}
\begin{split}
f(x)&amp;=(x-y)'W(x-y)\\
&amp;=((x-z)+(z-y))'W((x-z)+(z-y))\\
&amp;=f(z)+2(x-z)'W(z-y)+(x-z)'W(x-z)
\end{split}
(\#eq:unwgth)
\end{align}\]</span></p>
<p>Now choose a non-singular <span class="math inline">\(V\)</span> such that <span class="math inline">\(W\lesssim V\)</span> and define <span class="math inline">\(u:=V^{-1}W(z-y)\)</span>. Then we have the majorization</p>
<p><span class="math display">\[\begin{equation}
f(x)\leq f(z)+2(x-z)'W(z-y)+(x-z)'V(x-z)=\\
f(z)+2(x-z)'Vu+(x-z)'V(x-z)=\\
f(z)+(x-(z-u))'V(x-(z-u))-u'Vu.
(\#eq:compsq)
\end{equation}\]</span></p>
<p>Here are some ways to choose <span class="math inline">\(V\)</span>. We use <span class="math inline">\(\lambda_{\text{max}}(W)\)</span> and <span class="math inline">\(\lambda_{\text{min}}(W)\)</span> for the largest and smallest eigenvalues of the symmetric matrix <span class="math inline">\(W\)</span>.</p>
<p>For any <span class="math inline">\(W\)</span> we can choose <span class="math inline">\(V=\lambda_{\text{max}}(W)I\)</span>. Or, more generally, <span class="math inline">\(V=\lambda_{\text{max}}(D^{-1}W)D\)</span> for any positive definite <span class="math inline">\(D\)</span>. If <span class="math inline">\(W\)</span> is singular we can choose <span class="math inline">\(V=W+\epsilon D\)</span> for any positive definite <span class="math inline">\(D\)</span>. And in the unlikely case that <span class="math inline">\(W\)</span> is indefinite we can choose <span class="math inline">\(V=W+(\epsilon-\lambda_{\text{min}}(W))I\)</span>. But if <span class="math inline">\(W\)</span> is indefinite we have more serious problems.</p>
<p>In appendix @ref(apcodemathadd) the R function lsuw(), implements the iterative majorization algorithm minimizing <span class="math inline">\((x-y)'W(x-y)\)</span> over <span class="math inline">\(x\)</span> in some set <span class="math inline">\(X\)</span>. One of the parameters of lsuw() is a function proj(), which projects a vector on <span class="math inline">\(X\)</span> in the metric define by <span class="math inline">\(V\)</span>. The projection could be on the positive orthant, on a cone with isotone vectors, on a linear subspace, on a sphere, on a set of low-rank matrices, and so on.</p>
<p>As an example choose <span class="math inline">\(W\)</span> as a banded matrix of order 10 with <span class="math inline">\(w_{ij}=1\)</span> if <span class="math inline">\(|i-j|\leq 3\)</span> and <span class="math inline">\(i\not= j\)</span>, <span class="math inline">\(w_{ij}=i\)</span> if <span class="math inline">\(i=j\)</span>, and <span class="math inline">\(w_{ij}=0\)</span> otherwise. We require all 10 elements of <span class="math inline">\(x\)</span> to be the same, and we use <span class="math inline">\(V=\lambda_{\text{max}}(W)I\)</span> (the default).</p>
<p>The iterations are</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>w<span class="ot">&lt;-</span><span class="fu">ifelse</span>(<span class="fu">outer</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,<span class="cf">function</span>(x,y) <span class="fu">abs</span>(x<span class="sc">-</span>y) <span class="sc">&lt;=</span> <span class="dv">3</span>),<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>w <span class="ot">&lt;-</span> w <span class="sc">+</span> <span class="fu">diag</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">9</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>h1 <span class="ot">&lt;-</span> <span class="fu">lsuw</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, w, projeq)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If we use <span class="math inline">\(\lambda_{\text{max}}(D^{-1}W)D\)</span> with <span class="math inline">\(D=\text{diag}(W)\)</span> for <span class="math inline">\(V\)</span> we see the following majorization iterations.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">diag</span>(w)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>v <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="fu">eigen</span>((<span class="dv">1</span> <span class="sc">/</span> d) <span class="sc">*</span> w)<span class="sc">$</span>values) <span class="sc">*</span> <span class="fu">diag</span>(d)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>h2 <span class="ot">&lt;-</span> <span class="fu">lsuw</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, w, <span class="at">v =</span> v, projeq)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>So the second method of choosing <span class="math inline">\(V\)</span> is a tiny bit less efficient in this case, but it really does not make much of a difference. In both cases <span class="math inline">\(x\)</span> is 6.3009558, 6.3009558, 6.3009558, 6.3009558, 6.3009558, 6.3009558, 6.3009558, 6.3009558, 6.3009558, 6.3009558 with function value 595.6699029.</p>
<p>Apply to stress and to</p>
<p>Inner iterations, use one.</p>
<p><span class="math display">\[
\sigma_c(X):=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}\mathop{\sum\sum}_{1\leq k&lt;l\leq n}w_{ijkl}(\delta_{ij}-d_{ij}(X))(\delta_{kl}-d_{kl}(X))
\]</span> If <span class="math inline">\(A\leq B\)</span> (elementwise) then <span class="math inline">\(\sum\sum(b_{ij}-a_{ij})(x_i-x_j)^2\geq 0\)</span> and thus <span class="math inline">\(V(A)\lesssim V(B)\)</span>.</p>
<section id="symnmf" class="level4" data-number="5.4.10.1">
<h4 data-number="5.4.10.1" class="anchored" data-anchor-id="symnmf"><span class="header-section-number">5.4.10.1</span> Symmetric non-negative matrix factorization</h4>
<p><span class="math inline">\(w_{ij}=\sum_{s=1}^rv_{is}^2v_{js}^2\)</span> for all <span class="math inline">\(i\not= j\)</span>. Then <span class="math display">\[
\sigma(X)=\sum_{s=1}^p\mathop{\sum\sum}_{1\leq i&lt;j\leq n}(
\delta_{ijs}-d_{ijs}(X))^2
\]</span> with <span class="math inline">\(\delta_{ijs}:=v_{is}v_{js}\delta_{ij}\)</span> and <span class="math inline">\(d_{ijs}(X):=v_{is}v_{js}d_{ij}(X)\)</span>.</p>
</section>
</section>
</section>
<section id="propenvelopes" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="propenvelopes"><span class="header-section-number">5.5</span> Stress Envelopes</h2>
<p>intro</p>
<section id="propcsmaj" class="level3" data-number="5.5.1">
<h3 data-number="5.5.1" class="anchored" data-anchor-id="propcsmaj"><span class="header-section-number">5.5.1</span> CS Majorization</h3>
<div id="proplowenv" class="theorem">
<p><span class="math inline">\(\sigma\)</span> is the lower envelop of an infinite number of convex quadratics.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>By the CS inequality</p>
<p><span class="math display">\[\begin{equation}
d_{ij}(X)=\max_Y \frac{\text{tr}\ X^TA_{ij}Y}{d_{ij}(Y)},
(\#eq:dasmax)
\end{equation}\]</span></p>
<p>which implies</p>
<p><span class="math display">\[\begin{equation}
\sigma(X)=\min_Y\left(1-\text{tr}\ X^TB(Y)Y+\frac12\text{tr}\ X^TVX\right),
(\#eq:sigasmin)
\end{equation}\]</span></p>
<p>which is what we set out to prove.</p>
</div>
<p>We can use the lower envelop of a finite number of the quadratics from theorem @ref(thm:proplowenv) to approximate stress. This is illustrated graphically, using a small example in which the configuration is a convex combination of two fixed configurations. Thus in the example stress is a function of the single parameter <span class="math inline">\(0\leq\lambda\leq 1\)</span> defining the convex combination. In figure @ref(fig:upperfig) stress is in red, and we have used the three quadratics corresponding with <span class="math inline">\(\lambda\)</span> equal to 0.25, 0.5, 0.75. The maximum of the three quadratics is in blue, and the approximation is really good, in fact almost perfect in the areas where the blue is not even visible. As an aside, we also see three points in the figure where stress is not differentiable. The minimum of the three quadratics is also not differentiable at a point, but that point is different from the points where stress is non-smooth.</p>
<p>Note that by definition stress and the lower envelop of the quadratics are equal at the three points where <span class="math inline">\(\lambda\)</span> is 0.25, 0.5, 0.75, i.e at the three vertical lines in the plot.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="minimization_files/figure-html/upperfig-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Piecewise Quadratic Upper Approximation</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="propamgmmin" class="level3" data-number="5.5.2">
<h3 data-number="5.5.2" class="anchored" data-anchor-id="propamgmmin"><span class="header-section-number">5.5.2</span> AM/GM Minorization</h3>
<p>Instead of approximating stress from above, we can also approximate it from below.</p>
<div id="uplowenv" class="theorem">
<p><span class="math inline">\(\sigma\)</span> is the upper envelop of an infinite number of quadratics.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>By AM/GM</p>
<p><span class="math display">\[\begin{equation}
d_{ij}(X)\leq\min
\frac12\frac{1}{d_{ij}(Y)}\{d_{ij}^2(X)+d_{ij}^2(Y)\}
(\#eq:dasmin)
\end{equation}\]</span></p>
<p>Thus</p>
<p><span class="math display">\[\begin{equation}
\sigma(X)=\max_Y \left(1-\frac12\rho(Y)+\frac12\text{tr}\ X'(V-B(Y))X\right)
(\#eq:sigmax)
\end{equation}\]</span></p>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="minimization_files/figure-html/lowerfig-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Piecewise Quadratic Lower Approximation</figcaption>
</figure>
</div>
</div>
</div>
<p>Again we illustrate this result using a finite number of quadratics. In figure @ref(fig:lowerfig) we choose <span class="math inline">\(\lambda\)</span> equal to 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. Although we now use 11 quadratics, and thus force the envelop to be equal to the function at the 11 points on the vertical lines in the plot, the approximation is poor. This seems to be mainly because the convex-like function stress must be approximated from below by quadratics which are often concave.</p>
</section>
<section id="dualities" class="level3" data-number="5.5.3">
<h3 data-number="5.5.3" class="anchored" data-anchor-id="dualities"><span class="header-section-number">5.5.3</span> Dualities</h3>
<p><span class="math display">\[\begin{multline}
\min_X\sigma(X)=\min_Y \left(1 - \frac12\text{tr}\ Y'B(Y)V^+B(Y)Y\right)=\\1-\frac12\max_Y\text{tr}\ Y'B(Y)V^+B(Y)Y.
\end{multline}\]</span></p>
<p>Thus minimizing stress is equivalent to maximizing <span class="math inline">\(\eta^2(V^+B(X)X)\)</span>.</p>
<p><span class="math display">\[
\min_X\sigma(X)\geq\max_{B(Y)\lesssim V}(1-\rho(Y))
\]</span></p>
<p>By the minimax inequality <span class="math inline">\(\min_X\sigma(X)=\min_X\max_Y\theta(X,Y)\geq\max_Y\min_X\theta(X,Y).\)</span> Now <span class="math inline">\(\min_X\theta(X,Y)\)</span> is <span class="math inline">\(-\infty\)</span>, unless <span class="math inline">\(B(Y)\lesssim V\)</span>, in which case <span class="math inline">\(\min_X\theta(X,Y)=0\)</span>. Thus <span class="math display">\[
\max_Y\min_X\theta(X,Y)=\max_{B(Y)\lesssim V}(1-\rho(Y))=1-\min_{B(Y)\lesssim V}\ \rho(Y)
\]</span></p>
</section>
</section>
<section id="smacofcoef" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="smacofcoef"><span class="header-section-number">5.6</span> Smacof in Coefficient Space</h2>
</section>
<section id="smacofnewton" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="smacofnewton"><span class="header-section-number">5.7</span> Newton in MDS</h2>
<section id="attraction" class="level3" data-number="5.7.1">
<h3 data-number="5.7.1" class="anchored" data-anchor-id="attraction"><span class="header-section-number">5.7.1</span> Regions of Attraction</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>delta <span class="ot">&lt;-</span> <span class="fu">as.matrix</span> (<span class="fu">dist</span> (<span class="fu">diag</span> (<span class="dv">4</span>)))</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>delta <span class="ot">&lt;-</span> delta <span class="sc">*</span> <span class="fu">sqrt</span> (<span class="dv">2</span> <span class="sc">/</span> <span class="fu">sum</span> (delta <span class="sc">^</span> <span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="attractsmacof" class="level4" data-number="5.7.1.1">
<h4 data-number="5.7.1.1" class="anchored" data-anchor-id="attractsmacof"><span class="header-section-number">5.7.1.1</span> Smacof</h4>
<p>We use the smacof() function from the code in the appendix with 100 different starting points of <span class="math inline">\(\theta\)</span>, equally spaced on the circle. Figure @ref(fig:histsmacof) is a histogram of the number of smacof iterations to convergence within 1e-15. In all cases smacof converges to a local minimum in coefficient space, never to a saddle point. Figure @ref(fig:pathsmacof) shows which local minima are reached from the different starting points. This shows, more or less contrary to what <span class="citation" data-cites="trosset_mathar_97">Trosset and Mathar (<a href="references.html#ref-trosset_mathar_97" role="doc-biblioref">1997</a>)</span> suggests, that non-global minima can indeed be points of attraction for smacof iterations.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="minimization_files/figure-html/histsmacof-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Histogram Number of Smacof Iterations</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="minimization_files/figure-html/pathsmacof-1.png" class="img-fluid figure-img" width="960"></p>
<figcaption>Path Endpoints of Smacof Iterations</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="attractnewton" class="level4" data-number="5.7.1.2">
<h4 data-number="5.7.1.2" class="anchored" data-anchor-id="attractnewton"><span class="header-section-number">5.7.1.2</span> Newton</h4>
<p>We repeat the same exercise with Newton’s method, which also converges from all 100 starting points in our example. In higher dimensions we may not be so lucky.</p>
<p>The histogram of iteration counts is in figure @ref(fig:histnewton). It shows in this example that <code>smacof</code> needs about 10 times the number of iterations that Newton needs. Because <code>smacof</code> iterations are much less expensive than Newton ones, this does not really say much about computing times. If we look at figure @ref(fig:pathnewton) we see the problem with non-safeguarded Newton. Although we have fast convergence from all 100 starting points, Newton converges to a saddle point in 45 cases.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="minimization_files/figure-html/histnewton-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Histogram Number of Newton Iterations</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="minimization_files/figure-html/pathnewton-1.png" class="img-fluid figure-img" width="960"></p>
<figcaption>Path Endpoints of Newton Iterations</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="propdistsmo" class="level2" data-number="5.8">
<h2 data-number="5.8" class="anchored" data-anchor-id="propdistsmo"><span class="header-section-number">5.8</span> Distance Smoothing</h2>
<p>In sections @ref(propconvex) and @ref(propstationary) we show the lack of differentiability in basic MDS is not a serious problem in the actual computation of local minima.</p>
<p>There is another rather straightforward way to circumvent the differentiabily issue, which actually may have additional benefits. The idea is to use an approximation of the Euclidean distance that is as close as possible on the positive real axis, but smooth at zero. This was first applied in unidimensional MDS by Pliner (<span class="citation" data-cites="pliner_86">Pliner (<a href="references.html#ref-pliner_86" role="doc-biblioref">1986</a>)</span>, <span class="citation" data-cites="pliner_96">Pliner (<a href="references.html#ref-pliner_96" role="doc-biblioref">1996</a>)</span>) and later taken up and generalized to pMDS for arbitrary <span class="math inline">\(p\)</span>, and even for arbitrary Minkovski metrics, by <span class="citation" data-cites="groenen_heiser_meulman_98">Groenen, Heiser, and Meulman (<a href="references.html#ref-groenen_heiser_meulman_98" role="doc-biblioref">1998</a>)</span> and <span class="citation" data-cites="groenen_heiser_meulman_99">Groenen, Heiser, and Meulman (<a href="references.html#ref-groenen_heiser_meulman_99" role="doc-biblioref">1999</a>)</span>. They coined the term <em>distance smoothing</em> for this variation of the <span class="math inline">\(\textrm{smacof}\)</span> framework for MDS.</p>
<p><span class="citation" data-cites="pliner_86">Pliner (<a href="references.html#ref-pliner_86" role="doc-biblioref">1986</a>)</span> uses a smooth approximation of the sign function, while <span class="citation" data-cites="groenen_heiser_meulman_98">Groenen, Heiser, and Meulman (<a href="references.html#ref-groenen_heiser_meulman_98" role="doc-biblioref">1998</a>)</span> borrow the smooth Huber approximation of the absolute value function from robust regression. We use another classical and efficient approximation <span class="math inline">\(|x|\approx\sqrt{x^2+\epsilon^2}\)</span> to the absolute value function, used in image analysis, location analysis, and computational geometry (<span class="citation" data-cites="deleeuw_E_18f">De Leeuw (<a href="references.html#ref-deleeuw_E_18f" role="doc-biblioref">2018</a>)</span>, <span class="citation" data-cites="ramirez_sanchez_kreinovich_argaez_14">Ramirez et al. (<a href="references.html#ref-ramirez_sanchez_kreinovich_argaez_14" role="doc-biblioref">2014</a>)</span>). In our context that becomes <span class="math inline">\(d_{ij}(X)\approx d_{ij}(X,\epsilon):=\sqrt{d_{ij}^2(X)+\epsilon^2}\)</span>. Note that on the non-negative reals <span class="math display">\[\begin{equation}
\max(\epsilon,d_{ij}(X))\leq d_{ij}(X,\epsilon)\leq d_{ij}(X)+\epsilon.
(\#eq:smoothineq)
\end{equation}\]</span> Figures @ref(fig:dfsmoother) and @ref(fig:ddsmoother) show the absolute value function and its derivative are approximated for <span class="math inline">\(\epsilon\)</span> equal to 0, 0.01, 0.05, 0.1, 0.5.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="minimization_files/figure-html/dfsmoother-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Function for Various Epsilon</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="minimization_files/figure-html/ddsmoother-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Derivative for Various Epsilon</figcaption>
</figure>
</div>
</div>
</div>
<p>The distance smoother we use fits nicely into <span class="math inline">\(\textrm{smacof}\)</span>. Define <span class="math inline">\(X_\epsilon:=\begin{bmatrix}X&amp;\mid&amp;\epsilon I\end{bmatrix}\)</span>. Then <span class="math inline">\(d_{ij}(X_\epsilon)=\sqrt{d_{ij}^2(X)+\epsilon^2}\)</span>. Thus we can define <span class="math display">\[\begin{equation}
\sigma_\epsilon(X):=\sigma(X_\epsilon)=\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}(\delta_{ij}- d_{ij}(X_\epsilon))^2,
(\#eq:sigmaepsilon)
\end{equation}\]</span> with <span class="math inline">\(\rho_\epsilon\)</span> and <span class="math inline">\(\eta^2_\epsilon\)</span> defined in the same way.</p>
<p>For a fixed <span class="math inline">\(\epsilon&gt;0\)</span> now <span class="math inline">\(d_{ij}(X_\epsilon)\)</span>, and thus stress, is (infinitely many times) differentiable on all of <span class="math inline">\(\mathbb{R}^{n\times p}\)</span>. Moreover <span class="math inline">\(d_{ij}(X,\epsilon)\)</span> is convex in <span class="math inline">\(X\)</span> for fixed <span class="math inline">\(\epsilon\)</span> and jointly convex in <span class="math inline">\(X\)</span> and <span class="math inline">\(\epsilon\)</span>, and as a consequence so are <span class="math inline">\(\rho_\epsilon\)</span> and <span class="math inline">\(\eta^2_\epsilon\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="graphics/calcutta_12_85.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Jan de Leeuw, Gilbert Saporta, Yutaka Kanaka in Kolkata, December 1985</figcaption>
</figure>
</div>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-deleeuw_C_77" class="csl-entry" role="listitem">
De Leeuw, J. 1977. <span>“Applications of Convex Analysis to Multidimensional Scaling.”</span> In <em>Recent Developments in Statistics</em>, edited by J. R. Barra, F. Brodeau, G. Romier, and B. Van Cutsem, 133–45. Amsterdam, The Netherlands: North Holland Publishing Company.
</div>
<div id="ref-deleeuw_C_94c" class="csl-entry" role="listitem">
———. 1994. <span>“<span class="nocase">Block Relaxation Algorithms in Statistics</span>.”</span> In <em>Information Systems and Data Analysis</em>, edited by H. H. Bock, W. Lenski, and M. M. Richter, 308–24. Berlin: Springer Verlag. <a href="https://jansweb.netlify.app/publication/deleeuw-c-94-c/deleeuw-c-94-c.pdf">https://jansweb.netlify.app/publication/deleeuw-c-94-c/deleeuw-c-94-c.pdf</a>.
</div>
<div id="ref-deleeuw_R_07c" class="csl-entry" role="listitem">
———. 2007. <span>“Derivatives of Generalized Eigen Systems with Applications.”</span> Preprint Series 528. Los Angeles, CA: UCLA Department of Statistics.
</div>
<div id="ref-deleeuw_R_08b" class="csl-entry" role="listitem">
———. 2008. <span>“<span class="nocase">Derivatives of Fixed-Rank Approximations</span>.”</span> Preprint Series 547. Los Angeles, CA: UCLA Department of Statistics.
</div>
<div id="ref-deleeuw_E_16p" class="csl-entry" role="listitem">
———. 2016. <span>“<span class="nocase">Derivatives of Low Rank PSD Approximation</span>.”</span> 2016.
</div>
<div id="ref-deleeuw_E_18f" class="csl-entry" role="listitem">
———. 2018. <span>“<span class="nocase">MM Algorithms for Smoothed Absolute Values</span>.”</span> 2018.
</div>
<div id="ref-deleeuw_heiser_C_77" class="csl-entry" role="listitem">
De Leeuw, J., and W. J. Heiser. 1977. <span>“Convergence of Correction Matrix Algorithms for Multidimensional Scaling.”</span> In <em>Geometric Representations of Relational Data</em>, edited by J. C. Lingoes, 735–53. Ann Arbor, Michigan: Mathesis Press.
</div>
<div id="ref-deleeuw_sorenson_U_12b" class="csl-entry" role="listitem">
De Leeuw, J., and K. Sorenson. 2012. <span>“<span class="nocase">Derivatives of the Procrustus Transformation with Applications</span>.”</span>
</div>
<div id="ref-groenen_giaquinto_kiers_03" class="csl-entry" role="listitem">
Groenen, P. J. F., P. Giaquinto, and H. A. L Kiers. 2003. <span>“<span class="nocase">Weighted Majorization Algorithms for Weighted Least Squares Decomposition Models</span>.”</span> Econometric Institute Report EI 2003-09. Econometric Institute, Erasmus University Rotterdam. <a href="https://repub.eur.nl/pub/1700">https://repub.eur.nl/pub/1700</a>.
</div>
<div id="ref-groenen_heiser_meulman_98" class="csl-entry" role="listitem">
Groenen, P. J. F., W. J. Heiser, and J. J. Meulman. 1998. <span>“<span class="nocase">City-Block Scaling: Smoothing Strategies for Avoiding Local Minima</span>.”</span> In <em>Classification, Data Analysis, and Data Highways</em>, edited by I. Balderjahn, R. Mathar, and M. Schader. Springer.
</div>
<div id="ref-groenen_heiser_meulman_99" class="csl-entry" role="listitem">
———. 1999. <span>“<span class="nocase">Global Optimization in Least-Squares Multidimensional Scaling by Distance Smoothing</span>.”</span> <em>Journal of Classification</em> 16: 225–54.
</div>
<div id="ref-guttman_68" class="csl-entry" role="listitem">
Guttman, L. 1968. <span>“<span class="nocase">A General Nonmetric Technique for Fitting the Smallest Coordinate Space for a Configuration of Points</span>.”</span> <em>Psychometrika</em> 33: 469–506.
</div>
<div id="ref-heiser_91" class="csl-entry" role="listitem">
Heiser, W. J. 1991. <span>“<span class="nocase">A Generalized Majorization Method for Least Squares Multidimensional Scaling of Pseudodistances that May Be Negative</span>.”</span> <em>Psychometrika</em> 56 (1): 7–27.
</div>
<div id="ref-heiser_95" class="csl-entry" role="listitem">
———. 1995. <span>“<span class="nocase">Convergent Computing by Iterative Majorization: Theory and Applications in Multidimensional Data Analysis</span>.”</span> In <em>Recent Advantages in Descriptive Multivariate Analysis</em>, edited by W. J. Krzanowski, 157–89. Oxford: Clarendon Press.
</div>
<div id="ref-kiers_97" class="csl-entry" role="listitem">
Kiers, H. A. L. 1997. <span>“<span>Weighted Least Squares Fitting Using Iterative Ordinary Least Squares Algorithms</span>.”</span> <em>Psychometrika</em> 62: 251–66.
</div>
<div id="ref-kruskal_64b" class="csl-entry" role="listitem">
Kruskal, J. B. 1964. <span>“<span class="nocase">Nonmetric Multidimensional Scaling: a Numerical Method</span>.”</span> <em>Psychometrika</em> 29: 115–29.
</div>
<div id="ref-lange_16" class="csl-entry" role="listitem">
Lange, K. 2016. <em>MM Optimization Algorithms</em>. SIAM.
</div>
<div id="ref-pliner_86" class="csl-entry" role="listitem">
Pliner, V. 1986. <span>“<span class="nocase">The Problem of Multidimensional Metric Scaling</span>.”</span> <em>Automation and Remote Control</em> 47: 560–67.
</div>
<div id="ref-pliner_96" class="csl-entry" role="listitem">
———. 1996. <span>“<span class="nocase">Metric Unidimensional Scaling and Global Optimization</span>.”</span> <em>Journal of Classification</em> 13: 3–18.
</div>
<div id="ref-ramirez_sanchez_kreinovich_argaez_14" class="csl-entry" role="listitem">
Ramirez, C., R. Sanchez, V. Kreinovich, and M. Argaez. 2014. <span>“<span class="nocase"><span class="math inline">\(\sqrt{x^2+\mu}\)</span> is the Most Computationally Efficient Smooth Approximation to x</span>.”</span> <em>Journal of Uncertain Systems</em> 8: 205–10.
</div>
<div id="ref-spang_62" class="csl-entry" role="listitem">
Spang, H. A. 1962. <span>“<span class="nocase">A Review of Minimization Techniques for Nonlinear Functions</span>.”</span> <em>SIAM Review</em> 4 (4): 343–65.
</div>
<div id="ref-trosset_mathar_97" class="csl-entry" role="listitem">
Trosset, M. W., and R. Mathar. 1997. <span>“<span class="nocase">On the Existence on Nonglobal Minimizers of the STRESS Criterion for Metric Multidimensional Scaling</span>.”</span> In <em>Proceedings of the Statistical Computing Section</em>, 158–62. Alexandria, VA: American Statistical Association.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./classical.html" class="pagination-link" aria-label="Classical Multidimensional Scaling">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Multidimensional Scaling</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./acceleration.html" class="pagination-link" aria-label="Acceleration of Convergence">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Acceleration of Convergence</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>